{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.992261033256641,
  "eval_steps": 500,
  "global_step": 11950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020916126333403055,
      "grad_norm": 67739.703125,
      "learning_rate": 3.451882845188285e-07,
      "loss": 7287.0037,
      "step": 25
    },
    {
      "epoch": 0.04183225266680611,
      "grad_norm": 72765.8125,
      "learning_rate": 1.1297071129707113e-06,
      "loss": 6615.4131,
      "step": 50
    },
    {
      "epoch": 0.06274837900020916,
      "grad_norm": 49777.01953125,
      "learning_rate": 1.9142259414225943e-06,
      "loss": 5165.5938,
      "step": 75
    },
    {
      "epoch": 0.08366450533361222,
      "grad_norm": 26180.333984375,
      "learning_rate": 2.698744769874477e-06,
      "loss": 3156.7575,
      "step": 100
    },
    {
      "epoch": 0.10458063166701527,
      "grad_norm": 9235.36328125,
      "learning_rate": 3.4832635983263597e-06,
      "loss": 1752.6778,
      "step": 125
    },
    {
      "epoch": 0.12549675800041832,
      "grad_norm": 5154.06396484375,
      "learning_rate": 4.267782426778243e-06,
      "loss": 922.3782,
      "step": 150
    },
    {
      "epoch": 0.14641288433382138,
      "grad_norm": 2695.83447265625,
      "learning_rate": 5.052301255230126e-06,
      "loss": 468.4045,
      "step": 175
    },
    {
      "epoch": 0.16732901066722444,
      "grad_norm": 1978.790283203125,
      "learning_rate": 5.836820083682009e-06,
      "loss": 252.5346,
      "step": 200
    },
    {
      "epoch": 0.18824513700062748,
      "grad_norm": 1467.5531005859375,
      "learning_rate": 6.6213389121338915e-06,
      "loss": 144.4355,
      "step": 225
    },
    {
      "epoch": 0.20916126333403054,
      "grad_norm": 2490.191650390625,
      "learning_rate": 7.405857740585774e-06,
      "loss": 83.1875,
      "step": 250
    },
    {
      "epoch": 0.2300773896674336,
      "grad_norm": 862.4862060546875,
      "learning_rate": 8.190376569037657e-06,
      "loss": 55.4107,
      "step": 275
    },
    {
      "epoch": 0.25099351600083664,
      "grad_norm": 951.5653076171875,
      "learning_rate": 8.974895397489539e-06,
      "loss": 42.8861,
      "step": 300
    },
    {
      "epoch": 0.2719096423342397,
      "grad_norm": 1001.077880859375,
      "learning_rate": 9.759414225941422e-06,
      "loss": 34.3134,
      "step": 325
    },
    {
      "epoch": 0.29282576866764276,
      "grad_norm": 505.2092590332031,
      "learning_rate": 1.0543933054393304e-05,
      "loss": 30.5689,
      "step": 350
    },
    {
      "epoch": 0.3137418950010458,
      "grad_norm": 699.9384765625,
      "learning_rate": 1.132845188284519e-05,
      "loss": 26.081,
      "step": 375
    },
    {
      "epoch": 0.3346580213344489,
      "grad_norm": 374.21832275390625,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 22.7558,
      "step": 400
    },
    {
      "epoch": 0.3555741476678519,
      "grad_norm": 335.92242431640625,
      "learning_rate": 1.2897489539748955e-05,
      "loss": 20.0955,
      "step": 425
    },
    {
      "epoch": 0.37649027400125495,
      "grad_norm": 423.0104064941406,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 18.8336,
      "step": 450
    },
    {
      "epoch": 0.397406400334658,
      "grad_norm": 289.7948303222656,
      "learning_rate": 1.446652719665272e-05,
      "loss": 17.2085,
      "step": 475
    },
    {
      "epoch": 0.4183225266680611,
      "grad_norm": 203.27639770507812,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 15.4589,
      "step": 500
    },
    {
      "epoch": 0.43923865300146414,
      "grad_norm": 305.3823547363281,
      "learning_rate": 1.6035564853556484e-05,
      "loss": 14.1082,
      "step": 525
    },
    {
      "epoch": 0.4601547793348672,
      "grad_norm": 178.1066436767578,
      "learning_rate": 1.682008368200837e-05,
      "loss": 13.1408,
      "step": 550
    },
    {
      "epoch": 0.4810709056682702,
      "grad_norm": 213.33688354492188,
      "learning_rate": 1.760460251046025e-05,
      "loss": 12.5381,
      "step": 575
    },
    {
      "epoch": 0.5019870320016733,
      "grad_norm": 186.55694580078125,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 12.1148,
      "step": 600
    },
    {
      "epoch": 0.5229031583350764,
      "grad_norm": 170.68447875976562,
      "learning_rate": 1.9173640167364015e-05,
      "loss": 11.9643,
      "step": 625
    },
    {
      "epoch": 0.5438192846684794,
      "grad_norm": 178.8546905517578,
      "learning_rate": 1.99581589958159e-05,
      "loss": 11.4346,
      "step": 650
    },
    {
      "epoch": 0.5647354110018824,
      "grad_norm": 142.96109008789062,
      "learning_rate": 2.0742677824267782e-05,
      "loss": 11.036,
      "step": 675
    },
    {
      "epoch": 0.5856515373352855,
      "grad_norm": 129.5043182373047,
      "learning_rate": 2.1527196652719668e-05,
      "loss": 10.6892,
      "step": 700
    },
    {
      "epoch": 0.6065676636686885,
      "grad_norm": 149.344482421875,
      "learning_rate": 2.2311715481171546e-05,
      "loss": 10.5678,
      "step": 725
    },
    {
      "epoch": 0.6274837900020916,
      "grad_norm": 146.15878295898438,
      "learning_rate": 2.309623430962343e-05,
      "loss": 10.1855,
      "step": 750
    },
    {
      "epoch": 0.6483999163354947,
      "grad_norm": 153.01402282714844,
      "learning_rate": 2.3880753138075313e-05,
      "loss": 9.7644,
      "step": 775
    },
    {
      "epoch": 0.6693160426688978,
      "grad_norm": 133.17457580566406,
      "learning_rate": 2.46652719665272e-05,
      "loss": 9.2872,
      "step": 800
    },
    {
      "epoch": 0.6902321690023008,
      "grad_norm": 137.90269470214844,
      "learning_rate": 2.5449790794979077e-05,
      "loss": 9.0353,
      "step": 825
    },
    {
      "epoch": 0.7111482953357038,
      "grad_norm": 140.9998779296875,
      "learning_rate": 2.6234309623430962e-05,
      "loss": 8.4803,
      "step": 850
    },
    {
      "epoch": 0.7320644216691069,
      "grad_norm": 123.49948120117188,
      "learning_rate": 2.7018828451882844e-05,
      "loss": 10.9496,
      "step": 875
    },
    {
      "epoch": 0.7529805480025099,
      "grad_norm": 130.37692260742188,
      "learning_rate": 2.780334728033473e-05,
      "loss": 7.9815,
      "step": 900
    },
    {
      "epoch": 0.773896674335913,
      "grad_norm": 208.55609130859375,
      "learning_rate": 2.858786610878661e-05,
      "loss": 7.7468,
      "step": 925
    },
    {
      "epoch": 0.794812800669316,
      "grad_norm": 109.66746520996094,
      "learning_rate": 2.9372384937238493e-05,
      "loss": 7.3679,
      "step": 950
    },
    {
      "epoch": 0.8157289270027192,
      "grad_norm": 125.1940689086914,
      "learning_rate": 2.9986356194287794e-05,
      "loss": 7.0003,
      "step": 975
    },
    {
      "epoch": 0.8366450533361222,
      "grad_norm": 111.22805786132812,
      "learning_rate": 2.991813716572676e-05,
      "loss": 6.9098,
      "step": 1000
    },
    {
      "epoch": 0.8575611796695252,
      "grad_norm": 124.72747802734375,
      "learning_rate": 2.9849918137165727e-05,
      "loss": 6.9099,
      "step": 1025
    },
    {
      "epoch": 0.8784773060029283,
      "grad_norm": 120.16636657714844,
      "learning_rate": 2.9781699108604695e-05,
      "loss": 6.4205,
      "step": 1050
    },
    {
      "epoch": 0.8993934323363313,
      "grad_norm": 122.77552795410156,
      "learning_rate": 2.9713480080043663e-05,
      "loss": 6.5306,
      "step": 1075
    },
    {
      "epoch": 0.9203095586697344,
      "grad_norm": 115.97383117675781,
      "learning_rate": 2.9645261051482628e-05,
      "loss": 6.0135,
      "step": 1100
    },
    {
      "epoch": 0.9412256850031374,
      "grad_norm": 115.93516540527344,
      "learning_rate": 2.9577042022921592e-05,
      "loss": 6.285,
      "step": 1125
    },
    {
      "epoch": 0.9621418113365404,
      "grad_norm": 101.31525421142578,
      "learning_rate": 2.9508822994360564e-05,
      "loss": 6.0358,
      "step": 1150
    },
    {
      "epoch": 0.9830579376699435,
      "grad_norm": 99.14838409423828,
      "learning_rate": 2.944060396579953e-05,
      "loss": 5.5468,
      "step": 1175
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.151129126548767,
      "eval_runtime": 7.8778,
      "eval_samples_per_second": 2157.951,
      "eval_steps_per_second": 67.531,
      "step": 1196
    },
    {
      "epoch": 1.0033465802133446,
      "grad_norm": 118.87664031982422,
      "learning_rate": 2.9372384937238493e-05,
      "loss": 5.3224,
      "step": 1200
    },
    {
      "epoch": 1.0242627065467476,
      "grad_norm": 116.55988311767578,
      "learning_rate": 2.930416590867746e-05,
      "loss": 5.2385,
      "step": 1225
    },
    {
      "epoch": 1.0451788328801506,
      "grad_norm": 99.35057830810547,
      "learning_rate": 2.9235946880116426e-05,
      "loss": 5.2977,
      "step": 1250
    },
    {
      "epoch": 1.0660949592135536,
      "grad_norm": 97.72042846679688,
      "learning_rate": 2.9167727851555394e-05,
      "loss": 4.9996,
      "step": 1275
    },
    {
      "epoch": 1.0870110855469568,
      "grad_norm": 107.85177612304688,
      "learning_rate": 2.9099508822994362e-05,
      "loss": 5.0568,
      "step": 1300
    },
    {
      "epoch": 1.1079272118803598,
      "grad_norm": 100.03577423095703,
      "learning_rate": 2.9031289794433327e-05,
      "loss": 4.8159,
      "step": 1325
    },
    {
      "epoch": 1.1288433382137628,
      "grad_norm": 91.98090362548828,
      "learning_rate": 2.8963070765872295e-05,
      "loss": 4.978,
      "step": 1350
    },
    {
      "epoch": 1.1497594645471658,
      "grad_norm": 97.00345611572266,
      "learning_rate": 2.8894851737311263e-05,
      "loss": 4.647,
      "step": 1375
    },
    {
      "epoch": 1.1706755908805688,
      "grad_norm": 91.64454650878906,
      "learning_rate": 2.8826632708750228e-05,
      "loss": 4.5914,
      "step": 1400
    },
    {
      "epoch": 1.191591717213972,
      "grad_norm": 85.3683853149414,
      "learning_rate": 2.8758413680189196e-05,
      "loss": 4.7713,
      "step": 1425
    },
    {
      "epoch": 1.212507843547375,
      "grad_norm": 107.71749877929688,
      "learning_rate": 2.869019465162816e-05,
      "loss": 4.5983,
      "step": 1450
    },
    {
      "epoch": 1.233423969880778,
      "grad_norm": 89.33069610595703,
      "learning_rate": 2.8621975623067126e-05,
      "loss": 4.4464,
      "step": 1475
    },
    {
      "epoch": 1.254340096214181,
      "grad_norm": 99.31279754638672,
      "learning_rate": 2.8553756594506097e-05,
      "loss": 4.5525,
      "step": 1500
    },
    {
      "epoch": 1.275256222547584,
      "grad_norm": 96.603759765625,
      "learning_rate": 2.8485537565945062e-05,
      "loss": 4.443,
      "step": 1525
    },
    {
      "epoch": 1.2961723488809873,
      "grad_norm": 87.56232452392578,
      "learning_rate": 2.841731853738403e-05,
      "loss": 4.4114,
      "step": 1550
    },
    {
      "epoch": 1.3170884752143903,
      "grad_norm": 78.50248718261719,
      "learning_rate": 2.8349099508822995e-05,
      "loss": 4.0618,
      "step": 1575
    },
    {
      "epoch": 1.3380046015477933,
      "grad_norm": 68.52643585205078,
      "learning_rate": 2.828088048026196e-05,
      "loss": 4.0393,
      "step": 1600
    },
    {
      "epoch": 1.3589207278811963,
      "grad_norm": 82.97811126708984,
      "learning_rate": 2.821266145170093e-05,
      "loss": 4.0492,
      "step": 1625
    },
    {
      "epoch": 1.3798368542145996,
      "grad_norm": 105.36497497558594,
      "learning_rate": 2.8144442423139896e-05,
      "loss": 4.1664,
      "step": 1650
    },
    {
      "epoch": 1.4007529805480026,
      "grad_norm": 88.03392791748047,
      "learning_rate": 2.807622339457886e-05,
      "loss": 4.2287,
      "step": 1675
    },
    {
      "epoch": 1.4216691068814056,
      "grad_norm": 88.258544921875,
      "learning_rate": 2.800800436601783e-05,
      "loss": 4.1111,
      "step": 1700
    },
    {
      "epoch": 1.4425852332148086,
      "grad_norm": 134.1741485595703,
      "learning_rate": 2.7939785337456797e-05,
      "loss": 3.7994,
      "step": 1725
    },
    {
      "epoch": 1.4635013595482116,
      "grad_norm": 94.7978744506836,
      "learning_rate": 2.787156630889576e-05,
      "loss": 3.8316,
      "step": 1750
    },
    {
      "epoch": 1.4844174858816146,
      "grad_norm": 180.31298828125,
      "learning_rate": 2.780334728033473e-05,
      "loss": 3.6228,
      "step": 1775
    },
    {
      "epoch": 1.5053336122150178,
      "grad_norm": 80.24591064453125,
      "learning_rate": 2.7735128251773694e-05,
      "loss": 3.9557,
      "step": 1800
    },
    {
      "epoch": 1.5262497385484208,
      "grad_norm": 78.62939453125,
      "learning_rate": 2.7666909223212662e-05,
      "loss": 3.964,
      "step": 1825
    },
    {
      "epoch": 1.5471658648818238,
      "grad_norm": 91.49930572509766,
      "learning_rate": 2.759869019465163e-05,
      "loss": 3.8689,
      "step": 1850
    },
    {
      "epoch": 1.568081991215227,
      "grad_norm": 92.42330169677734,
      "learning_rate": 2.7530471166090595e-05,
      "loss": 4.2717,
      "step": 1875
    },
    {
      "epoch": 1.58899811754863,
      "grad_norm": 75.57794952392578,
      "learning_rate": 2.7462252137529563e-05,
      "loss": 3.8881,
      "step": 1900
    },
    {
      "epoch": 1.609914243882033,
      "grad_norm": 82.08036804199219,
      "learning_rate": 2.7394033108968528e-05,
      "loss": 3.6009,
      "step": 1925
    },
    {
      "epoch": 1.630830370215436,
      "grad_norm": 94.0594711303711,
      "learning_rate": 2.7325814080407493e-05,
      "loss": 3.6595,
      "step": 1950
    },
    {
      "epoch": 1.651746496548839,
      "grad_norm": 83.09349060058594,
      "learning_rate": 2.7257595051846464e-05,
      "loss": 4.7243,
      "step": 1975
    },
    {
      "epoch": 1.672662622882242,
      "grad_norm": 77.61454010009766,
      "learning_rate": 2.718937602328543e-05,
      "loss": 3.6909,
      "step": 2000
    },
    {
      "epoch": 1.693578749215645,
      "grad_norm": 73.56149291992188,
      "learning_rate": 2.7121156994724394e-05,
      "loss": 3.5145,
      "step": 2025
    },
    {
      "epoch": 1.7144948755490483,
      "grad_norm": 85.70193481445312,
      "learning_rate": 2.7052937966163362e-05,
      "loss": 3.6053,
      "step": 2050
    },
    {
      "epoch": 1.7354110018824513,
      "grad_norm": 70.05289459228516,
      "learning_rate": 2.698471893760233e-05,
      "loss": 3.9313,
      "step": 2075
    },
    {
      "epoch": 1.7563271282158546,
      "grad_norm": 68.8621597290039,
      "learning_rate": 2.6916499909041298e-05,
      "loss": 3.5291,
      "step": 2100
    },
    {
      "epoch": 1.7772432545492576,
      "grad_norm": 84.72420501708984,
      "learning_rate": 2.6848280880480263e-05,
      "loss": 3.5007,
      "step": 2125
    },
    {
      "epoch": 1.7981593808826606,
      "grad_norm": 87.5242919921875,
      "learning_rate": 2.6780061851919228e-05,
      "loss": 3.564,
      "step": 2150
    },
    {
      "epoch": 1.8190755072160636,
      "grad_norm": 84.94788360595703,
      "learning_rate": 2.6711842823358196e-05,
      "loss": 3.4317,
      "step": 2175
    },
    {
      "epoch": 1.8399916335494666,
      "grad_norm": 81.98171997070312,
      "learning_rate": 2.6643623794797164e-05,
      "loss": 3.5558,
      "step": 2200
    },
    {
      "epoch": 1.8609077598828696,
      "grad_norm": 75.99791717529297,
      "learning_rate": 2.657540476623613e-05,
      "loss": 3.3724,
      "step": 2225
    },
    {
      "epoch": 1.8818238862162726,
      "grad_norm": 65.72471618652344,
      "learning_rate": 2.6507185737675097e-05,
      "loss": 3.5086,
      "step": 2250
    },
    {
      "epoch": 1.9027400125496758,
      "grad_norm": 74.33403015136719,
      "learning_rate": 2.643896670911406e-05,
      "loss": 3.498,
      "step": 2275
    },
    {
      "epoch": 1.9236561388830788,
      "grad_norm": 74.06954193115234,
      "learning_rate": 2.6370747680553033e-05,
      "loss": 3.261,
      "step": 2300
    },
    {
      "epoch": 1.9445722652164819,
      "grad_norm": 72.7291488647461,
      "learning_rate": 2.6302528651991998e-05,
      "loss": 3.3137,
      "step": 2325
    },
    {
      "epoch": 1.965488391549885,
      "grad_norm": 78.95838928222656,
      "learning_rate": 2.6234309623430962e-05,
      "loss": 3.3259,
      "step": 2350
    },
    {
      "epoch": 1.986404517883288,
      "grad_norm": 79.6773681640625,
      "learning_rate": 2.616609059486993e-05,
      "loss": 3.4449,
      "step": 2375
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.0195362567901611,
      "eval_runtime": 8.1099,
      "eval_samples_per_second": 2096.212,
      "eval_steps_per_second": 65.599,
      "step": 2392
    },
    {
      "epoch": 2.006693160426689,
      "grad_norm": 75.50452423095703,
      "learning_rate": 2.6097871566308895e-05,
      "loss": 3.0721,
      "step": 2400
    },
    {
      "epoch": 2.027609286760092,
      "grad_norm": 86.47798919677734,
      "learning_rate": 2.6029652537747863e-05,
      "loss": 2.9587,
      "step": 2425
    },
    {
      "epoch": 2.048525413093495,
      "grad_norm": 70.87667846679688,
      "learning_rate": 2.596143350918683e-05,
      "loss": 2.9173,
      "step": 2450
    },
    {
      "epoch": 2.069441539426898,
      "grad_norm": 74.27769470214844,
      "learning_rate": 2.5893214480625796e-05,
      "loss": 3.0636,
      "step": 2475
    },
    {
      "epoch": 2.090357665760301,
      "grad_norm": 80.50829315185547,
      "learning_rate": 2.582499545206476e-05,
      "loss": 3.0425,
      "step": 2500
    },
    {
      "epoch": 2.111273792093704,
      "grad_norm": 70.89522552490234,
      "learning_rate": 2.575677642350373e-05,
      "loss": 3.3841,
      "step": 2525
    },
    {
      "epoch": 2.132189918427107,
      "grad_norm": 66.09414672851562,
      "learning_rate": 2.5688557394942697e-05,
      "loss": 3.0018,
      "step": 2550
    },
    {
      "epoch": 2.15310604476051,
      "grad_norm": 75.17288970947266,
      "learning_rate": 2.5620338366381665e-05,
      "loss": 3.1322,
      "step": 2575
    },
    {
      "epoch": 2.1740221710939136,
      "grad_norm": 80.322998046875,
      "learning_rate": 2.555211933782063e-05,
      "loss": 2.8917,
      "step": 2600
    },
    {
      "epoch": 2.1949382974273166,
      "grad_norm": 72.09761810302734,
      "learning_rate": 2.5483900309259595e-05,
      "loss": 2.8191,
      "step": 2625
    },
    {
      "epoch": 2.2158544237607196,
      "grad_norm": 77.60404968261719,
      "learning_rate": 2.5415681280698566e-05,
      "loss": 2.9329,
      "step": 2650
    },
    {
      "epoch": 2.2367705500941226,
      "grad_norm": 71.53092193603516,
      "learning_rate": 2.534746225213753e-05,
      "loss": 3.3096,
      "step": 2675
    },
    {
      "epoch": 2.2576866764275256,
      "grad_norm": 75.45820617675781,
      "learning_rate": 2.5279243223576496e-05,
      "loss": 2.7466,
      "step": 2700
    },
    {
      "epoch": 2.2786028027609286,
      "grad_norm": 64.1966781616211,
      "learning_rate": 2.5211024195015464e-05,
      "loss": 2.8564,
      "step": 2725
    },
    {
      "epoch": 2.2995189290943316,
      "grad_norm": 77.32536315917969,
      "learning_rate": 2.514280516645443e-05,
      "loss": 2.705,
      "step": 2750
    },
    {
      "epoch": 2.3204350554277346,
      "grad_norm": 76.02467346191406,
      "learning_rate": 2.5074586137893397e-05,
      "loss": 2.776,
      "step": 2775
    },
    {
      "epoch": 2.3413511817611377,
      "grad_norm": 59.43437576293945,
      "learning_rate": 2.5006367109332365e-05,
      "loss": 2.7662,
      "step": 2800
    },
    {
      "epoch": 2.362267308094541,
      "grad_norm": 87.95166015625,
      "learning_rate": 2.493814808077133e-05,
      "loss": 2.7267,
      "step": 2825
    },
    {
      "epoch": 2.383183434427944,
      "grad_norm": 65.01778411865234,
      "learning_rate": 2.4869929052210298e-05,
      "loss": 2.807,
      "step": 2850
    },
    {
      "epoch": 2.404099560761347,
      "grad_norm": 77.91682434082031,
      "learning_rate": 2.4801710023649262e-05,
      "loss": 2.6256,
      "step": 2875
    },
    {
      "epoch": 2.42501568709475,
      "grad_norm": 80.16232299804688,
      "learning_rate": 2.473349099508823e-05,
      "loss": 2.8112,
      "step": 2900
    },
    {
      "epoch": 2.445931813428153,
      "grad_norm": 62.10005187988281,
      "learning_rate": 2.46652719665272e-05,
      "loss": 2.7375,
      "step": 2925
    },
    {
      "epoch": 2.466847939761556,
      "grad_norm": 68.67879486083984,
      "learning_rate": 2.4597052937966163e-05,
      "loss": 2.7668,
      "step": 2950
    },
    {
      "epoch": 2.487764066094959,
      "grad_norm": 61.65019607543945,
      "learning_rate": 2.4528833909405128e-05,
      "loss": 2.6371,
      "step": 2975
    },
    {
      "epoch": 2.508680192428362,
      "grad_norm": 74.04094696044922,
      "learning_rate": 2.44606148808441e-05,
      "loss": 2.8185,
      "step": 3000
    },
    {
      "epoch": 2.529596318761765,
      "grad_norm": 60.14577865600586,
      "learning_rate": 2.4392395852283064e-05,
      "loss": 2.7186,
      "step": 3025
    },
    {
      "epoch": 2.550512445095168,
      "grad_norm": 67.41131591796875,
      "learning_rate": 2.4324176823722032e-05,
      "loss": 2.7581,
      "step": 3050
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 67.56135559082031,
      "learning_rate": 2.4255957795160997e-05,
      "loss": 2.638,
      "step": 3075
    },
    {
      "epoch": 2.5923446977619746,
      "grad_norm": 69.31661224365234,
      "learning_rate": 2.4187738766599962e-05,
      "loss": 2.6773,
      "step": 3100
    },
    {
      "epoch": 2.6132608240953776,
      "grad_norm": 71.10139465332031,
      "learning_rate": 2.4119519738038933e-05,
      "loss": 2.8809,
      "step": 3125
    },
    {
      "epoch": 2.6341769504287806,
      "grad_norm": 67.22647857666016,
      "learning_rate": 2.4051300709477898e-05,
      "loss": 2.6301,
      "step": 3150
    },
    {
      "epoch": 2.6550930767621836,
      "grad_norm": 82.80781555175781,
      "learning_rate": 2.3983081680916863e-05,
      "loss": 2.5227,
      "step": 3175
    },
    {
      "epoch": 2.6760092030955867,
      "grad_norm": 73.92268371582031,
      "learning_rate": 2.391486265235583e-05,
      "loss": 2.5075,
      "step": 3200
    },
    {
      "epoch": 2.6969253294289897,
      "grad_norm": 64.6511001586914,
      "learning_rate": 2.38466436237948e-05,
      "loss": 2.7651,
      "step": 3225
    },
    {
      "epoch": 2.7178414557623927,
      "grad_norm": 67.0396499633789,
      "learning_rate": 2.3778424595233764e-05,
      "loss": 2.6704,
      "step": 3250
    },
    {
      "epoch": 2.738757582095796,
      "grad_norm": 65.61665344238281,
      "learning_rate": 2.3710205566672732e-05,
      "loss": 2.5489,
      "step": 3275
    },
    {
      "epoch": 2.759673708429199,
      "grad_norm": 64.91505432128906,
      "learning_rate": 2.3641986538111697e-05,
      "loss": 2.6637,
      "step": 3300
    },
    {
      "epoch": 2.780589834762602,
      "grad_norm": 67.80037689208984,
      "learning_rate": 2.3573767509550665e-05,
      "loss": 2.4705,
      "step": 3325
    },
    {
      "epoch": 2.801505961096005,
      "grad_norm": 64.2939224243164,
      "learning_rate": 2.3505548480989633e-05,
      "loss": 2.606,
      "step": 3350
    },
    {
      "epoch": 2.822422087429408,
      "grad_norm": 63.69418716430664,
      "learning_rate": 2.3437329452428598e-05,
      "loss": 2.4866,
      "step": 3375
    },
    {
      "epoch": 2.843338213762811,
      "grad_norm": 62.09162521362305,
      "learning_rate": 2.3369110423867566e-05,
      "loss": 2.6283,
      "step": 3400
    },
    {
      "epoch": 2.864254340096214,
      "grad_norm": 62.16901397705078,
      "learning_rate": 2.330089139530653e-05,
      "loss": 2.4637,
      "step": 3425
    },
    {
      "epoch": 2.885170466429617,
      "grad_norm": 68.51805877685547,
      "learning_rate": 2.3232672366745495e-05,
      "loss": 2.4305,
      "step": 3450
    },
    {
      "epoch": 2.90608659276302,
      "grad_norm": 71.63359832763672,
      "learning_rate": 2.3164453338184467e-05,
      "loss": 2.6206,
      "step": 3475
    },
    {
      "epoch": 2.927002719096423,
      "grad_norm": 68.31275177001953,
      "learning_rate": 2.309623430962343e-05,
      "loss": 2.9854,
      "step": 3500
    },
    {
      "epoch": 2.947918845429826,
      "grad_norm": 54.66984558105469,
      "learning_rate": 2.30280152810624e-05,
      "loss": 2.5005,
      "step": 3525
    },
    {
      "epoch": 2.968834971763229,
      "grad_norm": 67.18350219726562,
      "learning_rate": 2.2959796252501364e-05,
      "loss": 2.5375,
      "step": 3550
    },
    {
      "epoch": 2.9897510980966326,
      "grad_norm": 59.73198699951172,
      "learning_rate": 2.2891577223940332e-05,
      "loss": 2.4142,
      "step": 3575
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.7693282961845398,
      "eval_runtime": 8.0645,
      "eval_samples_per_second": 2107.995,
      "eval_steps_per_second": 65.968,
      "step": 3588
    },
    {
      "epoch": 3.0100397406400337,
      "grad_norm": 63.95540237426758,
      "learning_rate": 2.28233581953793e-05,
      "loss": 2.1219,
      "step": 3600
    },
    {
      "epoch": 3.0309558669734367,
      "grad_norm": 54.8077507019043,
      "learning_rate": 2.2755139166818265e-05,
      "loss": 2.1833,
      "step": 3625
    },
    {
      "epoch": 3.0518719933068397,
      "grad_norm": 59.530147552490234,
      "learning_rate": 2.268692013825723e-05,
      "loss": 2.3227,
      "step": 3650
    },
    {
      "epoch": 3.0727881196402427,
      "grad_norm": 73.87374114990234,
      "learning_rate": 2.2618701109696198e-05,
      "loss": 2.3195,
      "step": 3675
    },
    {
      "epoch": 3.0937042459736457,
      "grad_norm": 67.79176330566406,
      "learning_rate": 2.2550482081135166e-05,
      "loss": 2.3971,
      "step": 3700
    },
    {
      "epoch": 3.1146203723070487,
      "grad_norm": 61.790245056152344,
      "learning_rate": 2.248226305257413e-05,
      "loss": 2.2775,
      "step": 3725
    },
    {
      "epoch": 3.1355364986404517,
      "grad_norm": 62.19757080078125,
      "learning_rate": 2.24140440240131e-05,
      "loss": 2.2007,
      "step": 3750
    },
    {
      "epoch": 3.1564526249738547,
      "grad_norm": 57.61259078979492,
      "learning_rate": 2.2345824995452064e-05,
      "loss": 2.0727,
      "step": 3775
    },
    {
      "epoch": 3.1773687513072577,
      "grad_norm": 64.25800323486328,
      "learning_rate": 2.2277605966891035e-05,
      "loss": 2.1226,
      "step": 3800
    },
    {
      "epoch": 3.1982848776406607,
      "grad_norm": 57.10171890258789,
      "learning_rate": 2.220938693833e-05,
      "loss": 2.2022,
      "step": 3825
    },
    {
      "epoch": 3.219201003974064,
      "grad_norm": 61.96831512451172,
      "learning_rate": 2.2141167909768965e-05,
      "loss": 2.3737,
      "step": 3850
    },
    {
      "epoch": 3.240117130307467,
      "grad_norm": 65.36203002929688,
      "learning_rate": 2.2072948881207933e-05,
      "loss": 2.4097,
      "step": 3875
    },
    {
      "epoch": 3.26103325664087,
      "grad_norm": 62.90299987792969,
      "learning_rate": 2.2004729852646898e-05,
      "loss": 2.3255,
      "step": 3900
    },
    {
      "epoch": 3.281949382974273,
      "grad_norm": 55.533199310302734,
      "learning_rate": 2.1936510824085866e-05,
      "loss": 2.3473,
      "step": 3925
    },
    {
      "epoch": 3.302865509307676,
      "grad_norm": 64.47765350341797,
      "learning_rate": 2.1868291795524834e-05,
      "loss": 2.2528,
      "step": 3950
    },
    {
      "epoch": 3.323781635641079,
      "grad_norm": 52.89434814453125,
      "learning_rate": 2.18000727669638e-05,
      "loss": 2.2227,
      "step": 3975
    },
    {
      "epoch": 3.344697761974482,
      "grad_norm": 61.0213737487793,
      "learning_rate": 2.1731853738402763e-05,
      "loss": 2.3276,
      "step": 4000
    },
    {
      "epoch": 3.365613888307885,
      "grad_norm": 55.7202033996582,
      "learning_rate": 2.166363470984173e-05,
      "loss": 2.1734,
      "step": 4025
    },
    {
      "epoch": 3.3865300146412887,
      "grad_norm": 60.511905670166016,
      "learning_rate": 2.15954156812807e-05,
      "loss": 2.1559,
      "step": 4050
    },
    {
      "epoch": 3.4074461409746917,
      "grad_norm": 69.76045989990234,
      "learning_rate": 2.1527196652719668e-05,
      "loss": 2.1909,
      "step": 4075
    },
    {
      "epoch": 3.4283622673080947,
      "grad_norm": 47.6801643371582,
      "learning_rate": 2.1458977624158632e-05,
      "loss": 2.1331,
      "step": 4100
    },
    {
      "epoch": 3.4492783936414977,
      "grad_norm": 64.83487701416016,
      "learning_rate": 2.1390758595597597e-05,
      "loss": 2.1528,
      "step": 4125
    },
    {
      "epoch": 3.4701945199749007,
      "grad_norm": 59.30024337768555,
      "learning_rate": 2.132253956703657e-05,
      "loss": 2.0085,
      "step": 4150
    },
    {
      "epoch": 3.4911106463083037,
      "grad_norm": 56.3996467590332,
      "learning_rate": 2.1254320538475533e-05,
      "loss": 2.1103,
      "step": 4175
    },
    {
      "epoch": 3.5120267726417067,
      "grad_norm": 52.13330841064453,
      "learning_rate": 2.1186101509914498e-05,
      "loss": 2.1811,
      "step": 4200
    },
    {
      "epoch": 3.5329428989751097,
      "grad_norm": 64.9131088256836,
      "learning_rate": 2.1117882481353466e-05,
      "loss": 2.0621,
      "step": 4225
    },
    {
      "epoch": 3.5538590253085127,
      "grad_norm": 65.45631408691406,
      "learning_rate": 2.104966345279243e-05,
      "loss": 2.1706,
      "step": 4250
    },
    {
      "epoch": 3.5747751516419157,
      "grad_norm": 61.7546501159668,
      "learning_rate": 2.0981444424231402e-05,
      "loss": 2.1279,
      "step": 4275
    },
    {
      "epoch": 3.5956912779753187,
      "grad_norm": 54.131568908691406,
      "learning_rate": 2.0913225395670367e-05,
      "loss": 2.0157,
      "step": 4300
    },
    {
      "epoch": 3.616607404308722,
      "grad_norm": 58.48411178588867,
      "learning_rate": 2.0845006367109332e-05,
      "loss": 2.0705,
      "step": 4325
    },
    {
      "epoch": 3.637523530642125,
      "grad_norm": 60.22759246826172,
      "learning_rate": 2.07767873385483e-05,
      "loss": 2.0638,
      "step": 4350
    },
    {
      "epoch": 3.658439656975528,
      "grad_norm": 62.123600006103516,
      "learning_rate": 2.0708568309987265e-05,
      "loss": 2.0952,
      "step": 4375
    },
    {
      "epoch": 3.679355783308931,
      "grad_norm": 58.02349853515625,
      "learning_rate": 2.0640349281426233e-05,
      "loss": 2.2106,
      "step": 4400
    },
    {
      "epoch": 3.700271909642334,
      "grad_norm": 56.541500091552734,
      "learning_rate": 2.05721302528652e-05,
      "loss": 2.0929,
      "step": 4425
    },
    {
      "epoch": 3.721188035975737,
      "grad_norm": 77.9372787475586,
      "learning_rate": 2.0503911224304166e-05,
      "loss": 2.1225,
      "step": 4450
    },
    {
      "epoch": 3.7421041623091402,
      "grad_norm": 63.702919006347656,
      "learning_rate": 2.043569219574313e-05,
      "loss": 2.01,
      "step": 4475
    },
    {
      "epoch": 3.7630202886425437,
      "grad_norm": 59.25242233276367,
      "learning_rate": 2.0367473167182102e-05,
      "loss": 1.9513,
      "step": 4500
    },
    {
      "epoch": 3.7839364149759467,
      "grad_norm": 57.63154220581055,
      "learning_rate": 2.0299254138621067e-05,
      "loss": 2.0939,
      "step": 4525
    },
    {
      "epoch": 3.8048525413093497,
      "grad_norm": 56.25257110595703,
      "learning_rate": 2.0231035110060035e-05,
      "loss": 2.1791,
      "step": 4550
    },
    {
      "epoch": 3.8257686676427527,
      "grad_norm": 59.36117172241211,
      "learning_rate": 2.0162816081499e-05,
      "loss": 2.0137,
      "step": 4575
    },
    {
      "epoch": 3.8466847939761557,
      "grad_norm": 68.57710266113281,
      "learning_rate": 2.0094597052937964e-05,
      "loss": 1.9789,
      "step": 4600
    },
    {
      "epoch": 3.8676009203095587,
      "grad_norm": 62.31576919555664,
      "learning_rate": 2.0026378024376936e-05,
      "loss": 1.9638,
      "step": 4625
    },
    {
      "epoch": 3.8885170466429617,
      "grad_norm": 65.7546615600586,
      "learning_rate": 1.99581589958159e-05,
      "loss": 1.9747,
      "step": 4650
    },
    {
      "epoch": 3.9094331729763647,
      "grad_norm": 92.22772216796875,
      "learning_rate": 1.9889939967254865e-05,
      "loss": 2.0563,
      "step": 4675
    },
    {
      "epoch": 3.9303492993097677,
      "grad_norm": 93.01242065429688,
      "learning_rate": 1.9821720938693833e-05,
      "loss": 1.99,
      "step": 4700
    },
    {
      "epoch": 3.9512654256431707,
      "grad_norm": 61.45549774169922,
      "learning_rate": 1.97535019101328e-05,
      "loss": 2.1325,
      "step": 4725
    },
    {
      "epoch": 3.9721815519765737,
      "grad_norm": 60.62841796875,
      "learning_rate": 1.9685282881571766e-05,
      "loss": 2.0793,
      "step": 4750
    },
    {
      "epoch": 3.9930976783099768,
      "grad_norm": 67.39749908447266,
      "learning_rate": 1.9617063853010734e-05,
      "loss": 1.9708,
      "step": 4775
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.6912441253662109,
      "eval_runtime": 7.989,
      "eval_samples_per_second": 2127.936,
      "eval_steps_per_second": 66.592,
      "step": 4784
    },
    {
      "epoch": 4.013386320853378,
      "grad_norm": 59.28174591064453,
      "learning_rate": 1.95488448244497e-05,
      "loss": 1.8964,
      "step": 4800
    },
    {
      "epoch": 4.034302447186781,
      "grad_norm": 65.83511352539062,
      "learning_rate": 1.9480625795888667e-05,
      "loss": 1.7733,
      "step": 4825
    },
    {
      "epoch": 4.055218573520184,
      "grad_norm": 74.54533386230469,
      "learning_rate": 1.9412406767327635e-05,
      "loss": 2.0228,
      "step": 4850
    },
    {
      "epoch": 4.076134699853587,
      "grad_norm": 61.68505859375,
      "learning_rate": 1.93441877387666e-05,
      "loss": 1.8613,
      "step": 4875
    },
    {
      "epoch": 4.09705082618699,
      "grad_norm": 58.718894958496094,
      "learning_rate": 1.9275968710205568e-05,
      "loss": 1.9426,
      "step": 4900
    },
    {
      "epoch": 4.117966952520393,
      "grad_norm": 74.48822784423828,
      "learning_rate": 1.9207749681644533e-05,
      "loss": 1.8675,
      "step": 4925
    },
    {
      "epoch": 4.138883078853796,
      "grad_norm": 53.87174987792969,
      "learning_rate": 1.9139530653083498e-05,
      "loss": 1.7717,
      "step": 4950
    },
    {
      "epoch": 4.159799205187199,
      "grad_norm": 61.83536148071289,
      "learning_rate": 1.907131162452247e-05,
      "loss": 1.9079,
      "step": 4975
    },
    {
      "epoch": 4.180715331520602,
      "grad_norm": 59.113311767578125,
      "learning_rate": 1.9003092595961434e-05,
      "loss": 1.9474,
      "step": 5000
    },
    {
      "epoch": 4.201631457854005,
      "grad_norm": 54.98815155029297,
      "learning_rate": 1.8934873567400402e-05,
      "loss": 1.6996,
      "step": 5025
    },
    {
      "epoch": 4.222547584187408,
      "grad_norm": 61.64276885986328,
      "learning_rate": 1.8866654538839367e-05,
      "loss": 1.8244,
      "step": 5050
    },
    {
      "epoch": 4.243463710520811,
      "grad_norm": 60.826210021972656,
      "learning_rate": 1.8798435510278335e-05,
      "loss": 1.8897,
      "step": 5075
    },
    {
      "epoch": 4.264379836854214,
      "grad_norm": 60.251380920410156,
      "learning_rate": 1.8730216481717303e-05,
      "loss": 1.8312,
      "step": 5100
    },
    {
      "epoch": 4.285295963187617,
      "grad_norm": 53.003929138183594,
      "learning_rate": 1.8661997453156268e-05,
      "loss": 1.784,
      "step": 5125
    },
    {
      "epoch": 4.30621208952102,
      "grad_norm": 46.79014587402344,
      "learning_rate": 1.8593778424595232e-05,
      "loss": 1.9437,
      "step": 5150
    },
    {
      "epoch": 4.327128215854424,
      "grad_norm": 92.34153747558594,
      "learning_rate": 1.85255593960342e-05,
      "loss": 1.9276,
      "step": 5175
    },
    {
      "epoch": 4.348044342187827,
      "grad_norm": 176.15060424804688,
      "learning_rate": 1.845734036747317e-05,
      "loss": 2.2377,
      "step": 5200
    },
    {
      "epoch": 4.36896046852123,
      "grad_norm": 85.94109344482422,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 1.9203,
      "step": 5225
    },
    {
      "epoch": 4.389876594854633,
      "grad_norm": 52.48459243774414,
      "learning_rate": 1.83209023103511e-05,
      "loss": 1.7298,
      "step": 5250
    },
    {
      "epoch": 4.410792721188036,
      "grad_norm": 52.14617156982422,
      "learning_rate": 1.8252683281790066e-05,
      "loss": 1.8156,
      "step": 5275
    },
    {
      "epoch": 4.431708847521439,
      "grad_norm": 57.306705474853516,
      "learning_rate": 1.8184464253229034e-05,
      "loss": 1.7559,
      "step": 5300
    },
    {
      "epoch": 4.452624973854842,
      "grad_norm": 64.59849548339844,
      "learning_rate": 1.8116245224668002e-05,
      "loss": 1.9351,
      "step": 5325
    },
    {
      "epoch": 4.473541100188245,
      "grad_norm": 70.91484069824219,
      "learning_rate": 1.8048026196106967e-05,
      "loss": 1.7261,
      "step": 5350
    },
    {
      "epoch": 4.494457226521648,
      "grad_norm": 59.895633697509766,
      "learning_rate": 1.7979807167545935e-05,
      "loss": 1.6836,
      "step": 5375
    },
    {
      "epoch": 4.515373352855051,
      "grad_norm": 75.50821685791016,
      "learning_rate": 1.79115881389849e-05,
      "loss": 1.8694,
      "step": 5400
    },
    {
      "epoch": 4.536289479188454,
      "grad_norm": 49.77582550048828,
      "learning_rate": 1.7843369110423868e-05,
      "loss": 1.8241,
      "step": 5425
    },
    {
      "epoch": 4.557205605521857,
      "grad_norm": 78.72937774658203,
      "learning_rate": 1.7775150081862836e-05,
      "loss": 1.9027,
      "step": 5450
    },
    {
      "epoch": 4.57812173185526,
      "grad_norm": 47.85055923461914,
      "learning_rate": 1.77069310533018e-05,
      "loss": 1.7204,
      "step": 5475
    },
    {
      "epoch": 4.599037858188663,
      "grad_norm": 52.45658493041992,
      "learning_rate": 1.763871202474077e-05,
      "loss": 1.7944,
      "step": 5500
    },
    {
      "epoch": 4.619953984522066,
      "grad_norm": 62.09748458862305,
      "learning_rate": 1.7570492996179734e-05,
      "loss": 1.7942,
      "step": 5525
    },
    {
      "epoch": 4.640870110855469,
      "grad_norm": 55.27763366699219,
      "learning_rate": 1.7502273967618702e-05,
      "loss": 1.746,
      "step": 5550
    },
    {
      "epoch": 4.661786237188872,
      "grad_norm": 63.81172180175781,
      "learning_rate": 1.743405493905767e-05,
      "loss": 1.7907,
      "step": 5575
    },
    {
      "epoch": 4.682702363522275,
      "grad_norm": 56.15145492553711,
      "learning_rate": 1.7365835910496635e-05,
      "loss": 1.7963,
      "step": 5600
    },
    {
      "epoch": 4.703618489855678,
      "grad_norm": 54.529117584228516,
      "learning_rate": 1.72976168819356e-05,
      "loss": 1.5814,
      "step": 5625
    },
    {
      "epoch": 4.724534616189082,
      "grad_norm": 69.58109283447266,
      "learning_rate": 1.722939785337457e-05,
      "loss": 1.8949,
      "step": 5650
    },
    {
      "epoch": 4.745450742522485,
      "grad_norm": 55.460636138916016,
      "learning_rate": 1.7161178824813536e-05,
      "loss": 1.7438,
      "step": 5675
    },
    {
      "epoch": 4.766366868855888,
      "grad_norm": 60.84114456176758,
      "learning_rate": 1.70929597962525e-05,
      "loss": 1.7925,
      "step": 5700
    },
    {
      "epoch": 4.787282995189291,
      "grad_norm": 55.980533599853516,
      "learning_rate": 1.702474076769147e-05,
      "loss": 1.8806,
      "step": 5725
    },
    {
      "epoch": 4.808199121522694,
      "grad_norm": 63.782955169677734,
      "learning_rate": 1.6956521739130433e-05,
      "loss": 1.7472,
      "step": 5750
    },
    {
      "epoch": 4.829115247856097,
      "grad_norm": 76.7124252319336,
      "learning_rate": 1.6888302710569405e-05,
      "loss": 1.6966,
      "step": 5775
    },
    {
      "epoch": 4.8500313741895,
      "grad_norm": 56.1375617980957,
      "learning_rate": 1.682008368200837e-05,
      "loss": 1.7364,
      "step": 5800
    },
    {
      "epoch": 4.870947500522903,
      "grad_norm": 108.56002044677734,
      "learning_rate": 1.6751864653447334e-05,
      "loss": 1.7778,
      "step": 5825
    },
    {
      "epoch": 4.891863626856306,
      "grad_norm": 51.78474807739258,
      "learning_rate": 1.6683645624886302e-05,
      "loss": 1.7092,
      "step": 5850
    },
    {
      "epoch": 4.912779753189709,
      "grad_norm": 59.998416900634766,
      "learning_rate": 1.6615426596325267e-05,
      "loss": 1.6966,
      "step": 5875
    },
    {
      "epoch": 4.933695879523112,
      "grad_norm": 51.83110809326172,
      "learning_rate": 1.6547207567764235e-05,
      "loss": 1.7479,
      "step": 5900
    },
    {
      "epoch": 4.954612005856515,
      "grad_norm": 86.43026733398438,
      "learning_rate": 1.6478988539203203e-05,
      "loss": 1.7206,
      "step": 5925
    },
    {
      "epoch": 4.975528132189918,
      "grad_norm": 203.0537567138672,
      "learning_rate": 1.6410769510642168e-05,
      "loss": 3.0665,
      "step": 5950
    },
    {
      "epoch": 4.996444258523321,
      "grad_norm": 99.6737060546875,
      "learning_rate": 1.6342550482081133e-05,
      "loss": 1.7979,
      "step": 5975
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5596317052841187,
      "eval_runtime": 7.9952,
      "eval_samples_per_second": 2126.286,
      "eval_steps_per_second": 66.54,
      "step": 5980
    },
    {
      "epoch": 5.016732901066723,
      "grad_norm": 66.1754379272461,
      "learning_rate": 1.6274331453520104e-05,
      "loss": 1.5502,
      "step": 6000
    },
    {
      "epoch": 5.037649027400126,
      "grad_norm": 76.33758544921875,
      "learning_rate": 1.620611242495907e-05,
      "loss": 1.6591,
      "step": 6025
    },
    {
      "epoch": 5.058565153733529,
      "grad_norm": 81.91728973388672,
      "learning_rate": 1.6137893396398037e-05,
      "loss": 1.5392,
      "step": 6050
    },
    {
      "epoch": 5.079481280066932,
      "grad_norm": 109.0200424194336,
      "learning_rate": 1.6069674367837002e-05,
      "loss": 1.6809,
      "step": 6075
    },
    {
      "epoch": 5.100397406400335,
      "grad_norm": 106.1747817993164,
      "learning_rate": 1.6001455339275967e-05,
      "loss": 1.7549,
      "step": 6100
    },
    {
      "epoch": 5.121313532733738,
      "grad_norm": 55.3903923034668,
      "learning_rate": 1.5933236310714938e-05,
      "loss": 1.6717,
      "step": 6125
    },
    {
      "epoch": 5.142229659067141,
      "grad_norm": 48.27347183227539,
      "learning_rate": 1.5865017282153903e-05,
      "loss": 1.5319,
      "step": 6150
    },
    {
      "epoch": 5.163145785400544,
      "grad_norm": 57.13055419921875,
      "learning_rate": 1.5796798253592868e-05,
      "loss": 1.5763,
      "step": 6175
    },
    {
      "epoch": 5.184061911733947,
      "grad_norm": 60.25787353515625,
      "learning_rate": 1.5728579225031836e-05,
      "loss": 1.6248,
      "step": 6200
    },
    {
      "epoch": 5.20497803806735,
      "grad_norm": 53.08587646484375,
      "learning_rate": 1.5660360196470804e-05,
      "loss": 1.624,
      "step": 6225
    },
    {
      "epoch": 5.225894164400753,
      "grad_norm": 55.206111907958984,
      "learning_rate": 1.5592141167909772e-05,
      "loss": 1.793,
      "step": 6250
    },
    {
      "epoch": 5.246810290734156,
      "grad_norm": 50.87397384643555,
      "learning_rate": 1.5523922139348737e-05,
      "loss": 1.5836,
      "step": 6275
    },
    {
      "epoch": 5.267726417067559,
      "grad_norm": 60.9964714050293,
      "learning_rate": 1.54557031107877e-05,
      "loss": 1.6908,
      "step": 6300
    },
    {
      "epoch": 5.288642543400962,
      "grad_norm": 76.36341094970703,
      "learning_rate": 1.538748408222667e-05,
      "loss": 1.6538,
      "step": 6325
    },
    {
      "epoch": 5.309558669734365,
      "grad_norm": 61.12174987792969,
      "learning_rate": 1.5319265053665638e-05,
      "loss": 1.6085,
      "step": 6350
    },
    {
      "epoch": 5.330474796067768,
      "grad_norm": 69.33353424072266,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 1.5635,
      "step": 6375
    },
    {
      "epoch": 5.351390922401171,
      "grad_norm": 56.782676696777344,
      "learning_rate": 1.5182826996543569e-05,
      "loss": 1.6836,
      "step": 6400
    },
    {
      "epoch": 5.372307048734575,
      "grad_norm": 57.366539001464844,
      "learning_rate": 1.5114607967982535e-05,
      "loss": 1.4843,
      "step": 6425
    },
    {
      "epoch": 5.393223175067978,
      "grad_norm": 86.61720275878906,
      "learning_rate": 1.5046388939421502e-05,
      "loss": 1.576,
      "step": 6450
    },
    {
      "epoch": 5.414139301401381,
      "grad_norm": 76.5871810913086,
      "learning_rate": 1.497816991086047e-05,
      "loss": 1.5629,
      "step": 6475
    },
    {
      "epoch": 5.435055427734784,
      "grad_norm": 90.34391021728516,
      "learning_rate": 1.4909950882299436e-05,
      "loss": 1.5441,
      "step": 6500
    },
    {
      "epoch": 5.455971554068187,
      "grad_norm": 39.23140335083008,
      "learning_rate": 1.4841731853738403e-05,
      "loss": 1.5024,
      "step": 6525
    },
    {
      "epoch": 5.47688768040159,
      "grad_norm": 67.31940460205078,
      "learning_rate": 1.477351282517737e-05,
      "loss": 1.6217,
      "step": 6550
    },
    {
      "epoch": 5.497803806734993,
      "grad_norm": 65.58402252197266,
      "learning_rate": 1.4705293796616337e-05,
      "loss": 1.5497,
      "step": 6575
    },
    {
      "epoch": 5.518719933068396,
      "grad_norm": 51.23017883300781,
      "learning_rate": 1.4637074768055304e-05,
      "loss": 1.5781,
      "step": 6600
    },
    {
      "epoch": 5.539636059401799,
      "grad_norm": 60.29945373535156,
      "learning_rate": 1.456885573949427e-05,
      "loss": 1.691,
      "step": 6625
    },
    {
      "epoch": 5.560552185735202,
      "grad_norm": 56.34239196777344,
      "learning_rate": 1.4500636710933236e-05,
      "loss": 1.6745,
      "step": 6650
    },
    {
      "epoch": 5.581468312068605,
      "grad_norm": 54.36130905151367,
      "learning_rate": 1.4432417682372205e-05,
      "loss": 1.4831,
      "step": 6675
    },
    {
      "epoch": 5.602384438402008,
      "grad_norm": 50.254329681396484,
      "learning_rate": 1.436419865381117e-05,
      "loss": 1.4973,
      "step": 6700
    },
    {
      "epoch": 5.623300564735411,
      "grad_norm": 47.566558837890625,
      "learning_rate": 1.4295979625250137e-05,
      "loss": 1.4531,
      "step": 6725
    },
    {
      "epoch": 5.644216691068814,
      "grad_norm": 80.8124008178711,
      "learning_rate": 1.4227760596689104e-05,
      "loss": 1.4839,
      "step": 6750
    },
    {
      "epoch": 5.665132817402217,
      "grad_norm": 59.84382247924805,
      "learning_rate": 1.415954156812807e-05,
      "loss": 1.6858,
      "step": 6775
    },
    {
      "epoch": 5.68604894373562,
      "grad_norm": 59.90185546875,
      "learning_rate": 1.4091322539567037e-05,
      "loss": 1.5338,
      "step": 6800
    },
    {
      "epoch": 5.706965070069023,
      "grad_norm": 54.37534713745117,
      "learning_rate": 1.4023103511006003e-05,
      "loss": 1.4586,
      "step": 6825
    },
    {
      "epoch": 5.727881196402427,
      "grad_norm": 53.944419860839844,
      "learning_rate": 1.3954884482444971e-05,
      "loss": 1.4829,
      "step": 6850
    },
    {
      "epoch": 5.748797322735829,
      "grad_norm": 52.27125549316406,
      "learning_rate": 1.3886665453883936e-05,
      "loss": 1.6737,
      "step": 6875
    },
    {
      "epoch": 5.769713449069233,
      "grad_norm": 59.21403503417969,
      "learning_rate": 1.3818446425322904e-05,
      "loss": 1.5708,
      "step": 6900
    },
    {
      "epoch": 5.790629575402636,
      "grad_norm": 55.153316497802734,
      "learning_rate": 1.375022739676187e-05,
      "loss": 1.605,
      "step": 6925
    },
    {
      "epoch": 5.811545701736039,
      "grad_norm": 65.61193084716797,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 1.6209,
      "step": 6950
    },
    {
      "epoch": 5.832461828069442,
      "grad_norm": 59.969749450683594,
      "learning_rate": 1.3613789339639803e-05,
      "loss": 1.7195,
      "step": 6975
    },
    {
      "epoch": 5.853377954402845,
      "grad_norm": 43.00251770019531,
      "learning_rate": 1.354557031107877e-05,
      "loss": 1.5269,
      "step": 7000
    },
    {
      "epoch": 5.874294080736248,
      "grad_norm": 51.31962585449219,
      "learning_rate": 1.3477351282517738e-05,
      "loss": 1.5767,
      "step": 7025
    },
    {
      "epoch": 5.895210207069651,
      "grad_norm": 50.66520309448242,
      "learning_rate": 1.3409132253956704e-05,
      "loss": 1.7042,
      "step": 7050
    },
    {
      "epoch": 5.916126333403054,
      "grad_norm": 63.540992736816406,
      "learning_rate": 1.334091322539567e-05,
      "loss": 1.4981,
      "step": 7075
    },
    {
      "epoch": 5.937042459736457,
      "grad_norm": 55.200469970703125,
      "learning_rate": 1.3272694196834637e-05,
      "loss": 1.5898,
      "step": 7100
    },
    {
      "epoch": 5.95795858606986,
      "grad_norm": 67.30253601074219,
      "learning_rate": 1.3204475168273605e-05,
      "loss": 1.5837,
      "step": 7125
    },
    {
      "epoch": 5.978874712403263,
      "grad_norm": 88.4648208618164,
      "learning_rate": 1.313625613971257e-05,
      "loss": 1.6348,
      "step": 7150
    },
    {
      "epoch": 5.999790838736666,
      "grad_norm": 130.41116333007812,
      "learning_rate": 1.3068037111151536e-05,
      "loss": 1.4435,
      "step": 7175
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5450243949890137,
      "eval_runtime": 7.9375,
      "eval_samples_per_second": 2141.725,
      "eval_steps_per_second": 67.023,
      "step": 7176
    },
    {
      "epoch": 6.020079481280067,
      "grad_norm": 131.3970489501953,
      "learning_rate": 1.2999818082590505e-05,
      "loss": 1.303,
      "step": 7200
    },
    {
      "epoch": 6.04099560761347,
      "grad_norm": 42.95185852050781,
      "learning_rate": 1.2931599054029471e-05,
      "loss": 1.3333,
      "step": 7225
    },
    {
      "epoch": 6.061911733946873,
      "grad_norm": 51.97904968261719,
      "learning_rate": 1.2863380025468437e-05,
      "loss": 1.4236,
      "step": 7250
    },
    {
      "epoch": 6.082827860280276,
      "grad_norm": 69.56243896484375,
      "learning_rate": 1.2795160996907404e-05,
      "loss": 1.355,
      "step": 7275
    },
    {
      "epoch": 6.103743986613679,
      "grad_norm": 90.10336303710938,
      "learning_rate": 1.2726941968346372e-05,
      "loss": 1.4204,
      "step": 7300
    },
    {
      "epoch": 6.124660112947082,
      "grad_norm": 38.88650894165039,
      "learning_rate": 1.2658722939785338e-05,
      "loss": 1.4935,
      "step": 7325
    },
    {
      "epoch": 6.145576239280485,
      "grad_norm": 49.78894805908203,
      "learning_rate": 1.2590503911224303e-05,
      "loss": 1.4483,
      "step": 7350
    },
    {
      "epoch": 6.166492365613888,
      "grad_norm": 65.41981506347656,
      "learning_rate": 1.2522284882663271e-05,
      "loss": 1.4814,
      "step": 7375
    },
    {
      "epoch": 6.187408491947291,
      "grad_norm": 56.37100601196289,
      "learning_rate": 1.2454065854102238e-05,
      "loss": 1.4663,
      "step": 7400
    },
    {
      "epoch": 6.208324618280694,
      "grad_norm": 65.75453186035156,
      "learning_rate": 1.2385846825541206e-05,
      "loss": 1.423,
      "step": 7425
    },
    {
      "epoch": 6.229240744614097,
      "grad_norm": 97.24888610839844,
      "learning_rate": 1.231762779698017e-05,
      "loss": 1.4879,
      "step": 7450
    },
    {
      "epoch": 6.2501568709475,
      "grad_norm": 54.09173583984375,
      "learning_rate": 1.2249408768419139e-05,
      "loss": 1.4656,
      "step": 7475
    },
    {
      "epoch": 6.271072997280903,
      "grad_norm": 54.45741271972656,
      "learning_rate": 1.2181189739858105e-05,
      "loss": 1.5062,
      "step": 7500
    },
    {
      "epoch": 6.291989123614306,
      "grad_norm": 63.89504623413086,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 1.3841,
      "step": 7525
    },
    {
      "epoch": 6.312905249947709,
      "grad_norm": 66.90827178955078,
      "learning_rate": 1.2044751682736038e-05,
      "loss": 1.4056,
      "step": 7550
    },
    {
      "epoch": 6.333821376281112,
      "grad_norm": 63.94700241088867,
      "learning_rate": 1.1976532654175004e-05,
      "loss": 1.4394,
      "step": 7575
    },
    {
      "epoch": 6.354737502614515,
      "grad_norm": 47.27705764770508,
      "learning_rate": 1.1908313625613972e-05,
      "loss": 1.3588,
      "step": 7600
    },
    {
      "epoch": 6.375653628947919,
      "grad_norm": 184.77001953125,
      "learning_rate": 1.1840094597052937e-05,
      "loss": 1.3317,
      "step": 7625
    },
    {
      "epoch": 6.396569755281321,
      "grad_norm": 39.11935043334961,
      "learning_rate": 1.1771875568491905e-05,
      "loss": 1.2351,
      "step": 7650
    },
    {
      "epoch": 6.417485881614725,
      "grad_norm": 68.44929504394531,
      "learning_rate": 1.1703656539930872e-05,
      "loss": 1.4927,
      "step": 7675
    },
    {
      "epoch": 6.438402007948128,
      "grad_norm": 139.7034912109375,
      "learning_rate": 1.163543751136984e-05,
      "loss": 1.4045,
      "step": 7700
    },
    {
      "epoch": 6.459318134281531,
      "grad_norm": 49.72844696044922,
      "learning_rate": 1.1567218482808805e-05,
      "loss": 2.1578,
      "step": 7725
    },
    {
      "epoch": 6.480234260614934,
      "grad_norm": 52.110843658447266,
      "learning_rate": 1.1498999454247771e-05,
      "loss": 1.3068,
      "step": 7750
    },
    {
      "epoch": 6.501150386948337,
      "grad_norm": 60.2198486328125,
      "learning_rate": 1.1430780425686739e-05,
      "loss": 1.4378,
      "step": 7775
    },
    {
      "epoch": 6.52206651328174,
      "grad_norm": 76.40383911132812,
      "learning_rate": 1.1362561397125706e-05,
      "loss": 1.4498,
      "step": 7800
    },
    {
      "epoch": 6.542982639615143,
      "grad_norm": 72.37965393066406,
      "learning_rate": 1.1294342368564672e-05,
      "loss": 1.4703,
      "step": 7825
    },
    {
      "epoch": 6.563898765948546,
      "grad_norm": 113.93900299072266,
      "learning_rate": 1.1226123340003638e-05,
      "loss": 1.4237,
      "step": 7850
    },
    {
      "epoch": 6.584814892281949,
      "grad_norm": 60.5257453918457,
      "learning_rate": 1.1157904311442606e-05,
      "loss": 1.405,
      "step": 7875
    },
    {
      "epoch": 6.605731018615352,
      "grad_norm": 51.80965805053711,
      "learning_rate": 1.1089685282881571e-05,
      "loss": 1.4271,
      "step": 7900
    },
    {
      "epoch": 6.626647144948755,
      "grad_norm": 115.85269165039062,
      "learning_rate": 1.1021466254320538e-05,
      "loss": 1.4564,
      "step": 7925
    },
    {
      "epoch": 6.647563271282158,
      "grad_norm": 127.51425170898438,
      "learning_rate": 1.0953247225759506e-05,
      "loss": 1.3717,
      "step": 7950
    },
    {
      "epoch": 6.668479397615561,
      "grad_norm": 47.24102020263672,
      "learning_rate": 1.0885028197198472e-05,
      "loss": 1.3911,
      "step": 7975
    },
    {
      "epoch": 6.689395523948964,
      "grad_norm": 52.16204071044922,
      "learning_rate": 1.0816809168637439e-05,
      "loss": 1.4441,
      "step": 8000
    },
    {
      "epoch": 6.710311650282367,
      "grad_norm": 90.10038757324219,
      "learning_rate": 1.0748590140076405e-05,
      "loss": 1.5035,
      "step": 8025
    },
    {
      "epoch": 6.73122777661577,
      "grad_norm": 63.14326095581055,
      "learning_rate": 1.0680371111515373e-05,
      "loss": 1.3734,
      "step": 8050
    },
    {
      "epoch": 6.752143902949173,
      "grad_norm": 59.41158676147461,
      "learning_rate": 1.061215208295434e-05,
      "loss": 1.3301,
      "step": 8075
    },
    {
      "epoch": 6.773060029282577,
      "grad_norm": 58.17959213256836,
      "learning_rate": 1.0543933054393304e-05,
      "loss": 1.4058,
      "step": 8100
    },
    {
      "epoch": 6.7939761556159795,
      "grad_norm": 48.29291534423828,
      "learning_rate": 1.0475714025832272e-05,
      "loss": 1.4477,
      "step": 8125
    },
    {
      "epoch": 6.814892281949383,
      "grad_norm": 55.67224884033203,
      "learning_rate": 1.0407494997271239e-05,
      "loss": 1.4175,
      "step": 8150
    },
    {
      "epoch": 6.835808408282786,
      "grad_norm": 72.7037124633789,
      "learning_rate": 1.0339275968710207e-05,
      "loss": 1.3346,
      "step": 8175
    },
    {
      "epoch": 6.856724534616189,
      "grad_norm": 63.66419982910156,
      "learning_rate": 1.0271056940149172e-05,
      "loss": 1.4478,
      "step": 8200
    },
    {
      "epoch": 6.877640660949592,
      "grad_norm": 54.86393737792969,
      "learning_rate": 1.020283791158814e-05,
      "loss": 1.6391,
      "step": 8225
    },
    {
      "epoch": 6.898556787282995,
      "grad_norm": 77.51091766357422,
      "learning_rate": 1.0134618883027106e-05,
      "loss": 1.3905,
      "step": 8250
    },
    {
      "epoch": 6.919472913616398,
      "grad_norm": 55.32304000854492,
      "learning_rate": 1.0066399854466073e-05,
      "loss": 1.3644,
      "step": 8275
    },
    {
      "epoch": 6.940389039949801,
      "grad_norm": 75.04690551757812,
      "learning_rate": 9.998180825905039e-06,
      "loss": 1.4779,
      "step": 8300
    },
    {
      "epoch": 6.961305166283204,
      "grad_norm": 53.65996551513672,
      "learning_rate": 9.929961797344006e-06,
      "loss": 1.2928,
      "step": 8325
    },
    {
      "epoch": 6.982221292616607,
      "grad_norm": 59.100460052490234,
      "learning_rate": 9.861742768782974e-06,
      "loss": 1.3961,
      "step": 8350
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5205527544021606,
      "eval_runtime": 8.1956,
      "eval_samples_per_second": 2074.286,
      "eval_steps_per_second": 64.913,
      "step": 8372
    },
    {
      "epoch": 7.002509935160008,
      "grad_norm": 57.903011322021484,
      "learning_rate": 9.793523740221938e-06,
      "loss": 1.4219,
      "step": 8375
    },
    {
      "epoch": 7.023426061493412,
      "grad_norm": 57.979095458984375,
      "learning_rate": 9.725304711660906e-06,
      "loss": 1.3406,
      "step": 8400
    },
    {
      "epoch": 7.044342187826815,
      "grad_norm": 63.26646041870117,
      "learning_rate": 9.657085683099873e-06,
      "loss": 1.3372,
      "step": 8425
    },
    {
      "epoch": 7.065258314160218,
      "grad_norm": 57.18966293334961,
      "learning_rate": 9.588866654538841e-06,
      "loss": 1.4108,
      "step": 8450
    },
    {
      "epoch": 7.086174440493621,
      "grad_norm": 73.67167663574219,
      "learning_rate": 9.520647625977806e-06,
      "loss": 1.2952,
      "step": 8475
    },
    {
      "epoch": 7.107090566827024,
      "grad_norm": 102.93199157714844,
      "learning_rate": 9.452428597416772e-06,
      "loss": 1.2675,
      "step": 8500
    },
    {
      "epoch": 7.128006693160427,
      "grad_norm": 62.319122314453125,
      "learning_rate": 9.38420956885574e-06,
      "loss": 1.3569,
      "step": 8525
    },
    {
      "epoch": 7.14892281949383,
      "grad_norm": 54.942626953125,
      "learning_rate": 9.315990540294707e-06,
      "loss": 1.3471,
      "step": 8550
    },
    {
      "epoch": 7.169838945827233,
      "grad_norm": 60.32692337036133,
      "learning_rate": 9.247771511733673e-06,
      "loss": 1.4294,
      "step": 8575
    },
    {
      "epoch": 7.190755072160636,
      "grad_norm": 62.73239517211914,
      "learning_rate": 9.17955248317264e-06,
      "loss": 1.2304,
      "step": 8600
    },
    {
      "epoch": 7.211671198494039,
      "grad_norm": 60.305023193359375,
      "learning_rate": 9.111333454611608e-06,
      "loss": 1.4733,
      "step": 8625
    },
    {
      "epoch": 7.232587324827442,
      "grad_norm": 50.8016357421875,
      "learning_rate": 9.043114426050572e-06,
      "loss": 1.2949,
      "step": 8650
    },
    {
      "epoch": 7.253503451160845,
      "grad_norm": 44.9687614440918,
      "learning_rate": 8.974895397489539e-06,
      "loss": 1.248,
      "step": 8675
    },
    {
      "epoch": 7.274419577494248,
      "grad_norm": 174.74386596679688,
      "learning_rate": 8.906676368928507e-06,
      "loss": 1.3229,
      "step": 8700
    },
    {
      "epoch": 7.295335703827651,
      "grad_norm": 50.345970153808594,
      "learning_rate": 8.838457340367473e-06,
      "loss": 1.2472,
      "step": 8725
    },
    {
      "epoch": 7.316251830161054,
      "grad_norm": 48.48869323730469,
      "learning_rate": 8.77023831180644e-06,
      "loss": 1.1973,
      "step": 8750
    },
    {
      "epoch": 7.337167956494457,
      "grad_norm": 59.005943298339844,
      "learning_rate": 8.702019283245406e-06,
      "loss": 1.3863,
      "step": 8775
    },
    {
      "epoch": 7.35808408282786,
      "grad_norm": 59.93585968017578,
      "learning_rate": 8.633800254684374e-06,
      "loss": 1.3483,
      "step": 8800
    },
    {
      "epoch": 7.379000209161263,
      "grad_norm": 56.633392333984375,
      "learning_rate": 8.56558122612334e-06,
      "loss": 1.2368,
      "step": 8825
    },
    {
      "epoch": 7.399916335494666,
      "grad_norm": 60.210243225097656,
      "learning_rate": 8.497362197562306e-06,
      "loss": 1.3537,
      "step": 8850
    },
    {
      "epoch": 7.42083246182807,
      "grad_norm": 60.20065689086914,
      "learning_rate": 8.429143169001274e-06,
      "loss": 1.3836,
      "step": 8875
    },
    {
      "epoch": 7.441748588161473,
      "grad_norm": 56.342437744140625,
      "learning_rate": 8.36092414044024e-06,
      "loss": 1.2606,
      "step": 8900
    },
    {
      "epoch": 7.462664714494876,
      "grad_norm": 48.375064849853516,
      "learning_rate": 8.292705111879208e-06,
      "loss": 1.3183,
      "step": 8925
    },
    {
      "epoch": 7.483580840828279,
      "grad_norm": 64.86119079589844,
      "learning_rate": 8.224486083318173e-06,
      "loss": 1.3057,
      "step": 8950
    },
    {
      "epoch": 7.504496967161682,
      "grad_norm": 54.20290756225586,
      "learning_rate": 8.156267054757141e-06,
      "loss": 1.3114,
      "step": 8975
    },
    {
      "epoch": 7.525413093495085,
      "grad_norm": 53.88773727416992,
      "learning_rate": 8.088048026196107e-06,
      "loss": 1.3575,
      "step": 9000
    },
    {
      "epoch": 7.546329219828488,
      "grad_norm": 61.105926513671875,
      "learning_rate": 8.019828997635074e-06,
      "loss": 1.352,
      "step": 9025
    },
    {
      "epoch": 7.567245346161891,
      "grad_norm": 42.08253860473633,
      "learning_rate": 7.95160996907404e-06,
      "loss": 1.3745,
      "step": 9050
    },
    {
      "epoch": 7.588161472495294,
      "grad_norm": 55.287559509277344,
      "learning_rate": 7.883390940513007e-06,
      "loss": 1.2205,
      "step": 9075
    },
    {
      "epoch": 7.609077598828697,
      "grad_norm": 35.75313949584961,
      "learning_rate": 7.815171911951975e-06,
      "loss": 1.1614,
      "step": 9100
    },
    {
      "epoch": 7.6299937251621,
      "grad_norm": 79.88298034667969,
      "learning_rate": 7.74695288339094e-06,
      "loss": 1.2377,
      "step": 9125
    },
    {
      "epoch": 7.650909851495503,
      "grad_norm": 48.99446487426758,
      "learning_rate": 7.678733854829908e-06,
      "loss": 1.2143,
      "step": 9150
    },
    {
      "epoch": 7.671825977828906,
      "grad_norm": 55.444175720214844,
      "learning_rate": 7.610514826268874e-06,
      "loss": 1.2606,
      "step": 9175
    },
    {
      "epoch": 7.692742104162309,
      "grad_norm": 70.11286926269531,
      "learning_rate": 7.542295797707841e-06,
      "loss": 1.2849,
      "step": 9200
    },
    {
      "epoch": 7.713658230495712,
      "grad_norm": 48.57691955566406,
      "learning_rate": 7.474076769146808e-06,
      "loss": 1.3321,
      "step": 9225
    },
    {
      "epoch": 7.734574356829115,
      "grad_norm": 74.2833023071289,
      "learning_rate": 7.405857740585774e-06,
      "loss": 1.2426,
      "step": 9250
    },
    {
      "epoch": 7.755490483162518,
      "grad_norm": 50.139678955078125,
      "learning_rate": 7.3376387120247415e-06,
      "loss": 1.2089,
      "step": 9275
    },
    {
      "epoch": 7.776406609495921,
      "grad_norm": 39.64876174926758,
      "learning_rate": 7.269419683463708e-06,
      "loss": 1.2699,
      "step": 9300
    },
    {
      "epoch": 7.797322735829324,
      "grad_norm": 43.96186828613281,
      "learning_rate": 7.201200654902674e-06,
      "loss": 1.3348,
      "step": 9325
    },
    {
      "epoch": 7.818238862162728,
      "grad_norm": 63.2742919921875,
      "learning_rate": 7.132981626341641e-06,
      "loss": 1.3324,
      "step": 9350
    },
    {
      "epoch": 7.839154988496131,
      "grad_norm": 72.11660766601562,
      "learning_rate": 7.064762597780608e-06,
      "loss": 1.2922,
      "step": 9375
    },
    {
      "epoch": 7.860071114829534,
      "grad_norm": 61.473609924316406,
      "learning_rate": 6.9965435692195745e-06,
      "loss": 1.3035,
      "step": 9400
    },
    {
      "epoch": 7.880987241162937,
      "grad_norm": 58.378334045410156,
      "learning_rate": 6.928324540658542e-06,
      "loss": 1.257,
      "step": 9425
    },
    {
      "epoch": 7.90190336749634,
      "grad_norm": 43.729957580566406,
      "learning_rate": 6.860105512097508e-06,
      "loss": 1.2461,
      "step": 9450
    },
    {
      "epoch": 7.922819493829743,
      "grad_norm": 43.52495193481445,
      "learning_rate": 6.791886483536475e-06,
      "loss": 1.2263,
      "step": 9475
    },
    {
      "epoch": 7.943735620163146,
      "grad_norm": 52.94915008544922,
      "learning_rate": 6.723667454975441e-06,
      "loss": 1.3314,
      "step": 9500
    },
    {
      "epoch": 7.964651746496549,
      "grad_norm": 55.261653900146484,
      "learning_rate": 6.6554484264144074e-06,
      "loss": 1.3288,
      "step": 9525
    },
    {
      "epoch": 7.985567872829952,
      "grad_norm": 52.167869567871094,
      "learning_rate": 6.587229397853375e-06,
      "loss": 1.1715,
      "step": 9550
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.5381767153739929,
      "eval_runtime": 8.0521,
      "eval_samples_per_second": 2111.262,
      "eval_steps_per_second": 66.07,
      "step": 9568
    },
    {
      "epoch": 8.005856515373353,
      "grad_norm": 60.718021392822266,
      "learning_rate": 6.519010369292341e-06,
      "loss": 1.2106,
      "step": 9575
    },
    {
      "epoch": 8.026772641706756,
      "grad_norm": 48.78333282470703,
      "learning_rate": 6.450791340731308e-06,
      "loss": 1.2001,
      "step": 9600
    },
    {
      "epoch": 8.047688768040159,
      "grad_norm": 76.94561004638672,
      "learning_rate": 6.382572312170275e-06,
      "loss": 1.2402,
      "step": 9625
    },
    {
      "epoch": 8.068604894373562,
      "grad_norm": 53.080970764160156,
      "learning_rate": 6.314353283609242e-06,
      "loss": 1.1983,
      "step": 9650
    },
    {
      "epoch": 8.089521020706965,
      "grad_norm": 129.56243896484375,
      "learning_rate": 6.2461342550482085e-06,
      "loss": 1.2042,
      "step": 9675
    },
    {
      "epoch": 8.110437147040368,
      "grad_norm": 60.458194732666016,
      "learning_rate": 6.177915226487175e-06,
      "loss": 1.2645,
      "step": 9700
    },
    {
      "epoch": 8.13135327337377,
      "grad_norm": 52.74210739135742,
      "learning_rate": 6.109696197926141e-06,
      "loss": 1.2925,
      "step": 9725
    },
    {
      "epoch": 8.152269399707174,
      "grad_norm": 142.73391723632812,
      "learning_rate": 6.041477169365109e-06,
      "loss": 1.2022,
      "step": 9750
    },
    {
      "epoch": 8.173185526040577,
      "grad_norm": 49.043636322021484,
      "learning_rate": 5.973258140804075e-06,
      "loss": 1.2146,
      "step": 9775
    },
    {
      "epoch": 8.19410165237398,
      "grad_norm": 55.426109313964844,
      "learning_rate": 5.905039112243042e-06,
      "loss": 1.1749,
      "step": 9800
    },
    {
      "epoch": 8.215017778707383,
      "grad_norm": 63.284454345703125,
      "learning_rate": 5.836820083682009e-06,
      "loss": 1.2332,
      "step": 9825
    },
    {
      "epoch": 8.235933905040786,
      "grad_norm": 46.68082046508789,
      "learning_rate": 5.768601055120975e-06,
      "loss": 1.1626,
      "step": 9850
    },
    {
      "epoch": 8.256850031374189,
      "grad_norm": 61.015350341796875,
      "learning_rate": 5.700382026559942e-06,
      "loss": 1.285,
      "step": 9875
    },
    {
      "epoch": 8.277766157707592,
      "grad_norm": 45.48870849609375,
      "learning_rate": 5.632162997998908e-06,
      "loss": 1.241,
      "step": 9900
    },
    {
      "epoch": 8.298682284040996,
      "grad_norm": 88.36088562011719,
      "learning_rate": 5.563943969437875e-06,
      "loss": 1.2578,
      "step": 9925
    },
    {
      "epoch": 8.319598410374399,
      "grad_norm": 216.48590087890625,
      "learning_rate": 5.495724940876842e-06,
      "loss": 1.149,
      "step": 9950
    },
    {
      "epoch": 8.340514536707802,
      "grad_norm": 53.60832214355469,
      "learning_rate": 5.427505912315809e-06,
      "loss": 1.2628,
      "step": 9975
    },
    {
      "epoch": 8.361430663041205,
      "grad_norm": 45.742897033691406,
      "learning_rate": 5.3592868837547754e-06,
      "loss": 1.1719,
      "step": 10000
    },
    {
      "epoch": 8.382346789374608,
      "grad_norm": 46.66136169433594,
      "learning_rate": 5.291067855193743e-06,
      "loss": 1.185,
      "step": 10025
    },
    {
      "epoch": 8.40326291570801,
      "grad_norm": 62.357810974121094,
      "learning_rate": 5.222848826632709e-06,
      "loss": 1.387,
      "step": 10050
    },
    {
      "epoch": 8.424179042041414,
      "grad_norm": 51.905731201171875,
      "learning_rate": 5.1546297980716756e-06,
      "loss": 1.219,
      "step": 10075
    },
    {
      "epoch": 8.445095168374817,
      "grad_norm": 58.847007751464844,
      "learning_rate": 5.086410769510642e-06,
      "loss": 1.1838,
      "step": 10100
    },
    {
      "epoch": 8.46601129470822,
      "grad_norm": 62.93781280517578,
      "learning_rate": 5.018191740949609e-06,
      "loss": 1.1868,
      "step": 10125
    },
    {
      "epoch": 8.486927421041623,
      "grad_norm": 84.96051025390625,
      "learning_rate": 4.949972712388576e-06,
      "loss": 1.2127,
      "step": 10150
    },
    {
      "epoch": 8.507843547375026,
      "grad_norm": 50.70122528076172,
      "learning_rate": 4.881753683827543e-06,
      "loss": 1.179,
      "step": 10175
    },
    {
      "epoch": 8.528759673708429,
      "grad_norm": 43.136138916015625,
      "learning_rate": 4.813534655266509e-06,
      "loss": 1.2175,
      "step": 10200
    },
    {
      "epoch": 8.549675800041832,
      "grad_norm": 44.10986328125,
      "learning_rate": 4.745315626705476e-06,
      "loss": 1.1798,
      "step": 10225
    },
    {
      "epoch": 8.570591926375235,
      "grad_norm": 47.64037322998047,
      "learning_rate": 4.677096598144442e-06,
      "loss": 1.1369,
      "step": 10250
    },
    {
      "epoch": 8.591508052708638,
      "grad_norm": 43.80244445800781,
      "learning_rate": 4.608877569583409e-06,
      "loss": 1.1484,
      "step": 10275
    },
    {
      "epoch": 8.61242417904204,
      "grad_norm": 46.716522216796875,
      "learning_rate": 4.540658541022376e-06,
      "loss": 1.1654,
      "step": 10300
    },
    {
      "epoch": 8.633340305375445,
      "grad_norm": 52.261077880859375,
      "learning_rate": 4.472439512461342e-06,
      "loss": 1.2833,
      "step": 10325
    },
    {
      "epoch": 8.654256431708848,
      "grad_norm": 49.08324432373047,
      "learning_rate": 4.40422048390031e-06,
      "loss": 1.1613,
      "step": 10350
    },
    {
      "epoch": 8.67517255804225,
      "grad_norm": 62.5078239440918,
      "learning_rate": 4.336001455339276e-06,
      "loss": 1.2383,
      "step": 10375
    },
    {
      "epoch": 8.696088684375654,
      "grad_norm": 54.677001953125,
      "learning_rate": 4.267782426778243e-06,
      "loss": 1.2603,
      "step": 10400
    },
    {
      "epoch": 8.717004810709057,
      "grad_norm": 43.32417678833008,
      "learning_rate": 4.199563398217209e-06,
      "loss": 1.1515,
      "step": 10425
    },
    {
      "epoch": 8.73792093704246,
      "grad_norm": 37.44027328491211,
      "learning_rate": 4.131344369656176e-06,
      "loss": 1.1914,
      "step": 10450
    },
    {
      "epoch": 8.758837063375863,
      "grad_norm": 36.797061920166016,
      "learning_rate": 4.063125341095143e-06,
      "loss": 1.1099,
      "step": 10475
    },
    {
      "epoch": 8.779753189709266,
      "grad_norm": 49.730892181396484,
      "learning_rate": 3.99490631253411e-06,
      "loss": 1.2897,
      "step": 10500
    },
    {
      "epoch": 8.800669316042669,
      "grad_norm": 54.858795166015625,
      "learning_rate": 3.926687283973076e-06,
      "loss": 1.1796,
      "step": 10525
    },
    {
      "epoch": 8.821585442376072,
      "grad_norm": 49.19490051269531,
      "learning_rate": 3.8584682554120435e-06,
      "loss": 1.1895,
      "step": 10550
    },
    {
      "epoch": 8.842501568709475,
      "grad_norm": 54.31814956665039,
      "learning_rate": 3.79024922685101e-06,
      "loss": 1.289,
      "step": 10575
    },
    {
      "epoch": 8.863417695042878,
      "grad_norm": 49.04741668701172,
      "learning_rate": 3.7220301982899764e-06,
      "loss": 1.2408,
      "step": 10600
    },
    {
      "epoch": 8.88433382137628,
      "grad_norm": 37.51622772216797,
      "learning_rate": 3.6538111697289432e-06,
      "loss": 1.1896,
      "step": 10625
    },
    {
      "epoch": 8.905249947709684,
      "grad_norm": 45.2328987121582,
      "learning_rate": 3.58559214116791e-06,
      "loss": 1.0744,
      "step": 10650
    },
    {
      "epoch": 8.926166074043087,
      "grad_norm": 43.00437545776367,
      "learning_rate": 3.5173731126068765e-06,
      "loss": 1.0684,
      "step": 10675
    },
    {
      "epoch": 8.94708220037649,
      "grad_norm": 104.70178985595703,
      "learning_rate": 3.4491540840458434e-06,
      "loss": 1.2395,
      "step": 10700
    },
    {
      "epoch": 8.967998326709893,
      "grad_norm": 48.26473617553711,
      "learning_rate": 3.3809350554848102e-06,
      "loss": 1.2127,
      "step": 10725
    },
    {
      "epoch": 8.988914453043297,
      "grad_norm": 48.29652404785156,
      "learning_rate": 3.312716026923777e-06,
      "loss": 1.0345,
      "step": 10750
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.518979549407959,
      "eval_runtime": 7.9745,
      "eval_samples_per_second": 2131.806,
      "eval_steps_per_second": 66.713,
      "step": 10764
    },
    {
      "epoch": 9.009203095586697,
      "grad_norm": 42.65078353881836,
      "learning_rate": 3.2444969983627435e-06,
      "loss": 1.1541,
      "step": 10775
    },
    {
      "epoch": 9.030119221920101,
      "grad_norm": 45.75841522216797,
      "learning_rate": 3.17627796980171e-06,
      "loss": 1.1123,
      "step": 10800
    },
    {
      "epoch": 9.051035348253503,
      "grad_norm": 74.63848114013672,
      "learning_rate": 3.1080589412406768e-06,
      "loss": 1.1156,
      "step": 10825
    },
    {
      "epoch": 9.071951474586907,
      "grad_norm": 47.80039978027344,
      "learning_rate": 3.039839912679643e-06,
      "loss": 1.0912,
      "step": 10850
    },
    {
      "epoch": 9.092867600920309,
      "grad_norm": 42.50808334350586,
      "learning_rate": 2.97162088411861e-06,
      "loss": 1.1304,
      "step": 10875
    },
    {
      "epoch": 9.113783727253713,
      "grad_norm": 44.431480407714844,
      "learning_rate": 2.903401855557577e-06,
      "loss": 1.0935,
      "step": 10900
    },
    {
      "epoch": 9.134699853587115,
      "grad_norm": 42.39858627319336,
      "learning_rate": 2.8351828269965437e-06,
      "loss": 1.225,
      "step": 10925
    },
    {
      "epoch": 9.155615979920519,
      "grad_norm": 41.835426330566406,
      "learning_rate": 2.76696379843551e-06,
      "loss": 1.1124,
      "step": 10950
    },
    {
      "epoch": 9.176532106253921,
      "grad_norm": 141.8780059814453,
      "learning_rate": 2.698744769874477e-06,
      "loss": 1.1434,
      "step": 10975
    },
    {
      "epoch": 9.197448232587325,
      "grad_norm": 46.40107727050781,
      "learning_rate": 2.630525741313444e-06,
      "loss": 1.1377,
      "step": 11000
    },
    {
      "epoch": 9.218364358920727,
      "grad_norm": 49.37913513183594,
      "learning_rate": 2.5623067127524107e-06,
      "loss": 1.1538,
      "step": 11025
    },
    {
      "epoch": 9.239280485254131,
      "grad_norm": 43.150367736816406,
      "learning_rate": 2.494087684191377e-06,
      "loss": 1.1947,
      "step": 11050
    },
    {
      "epoch": 9.260196611587533,
      "grad_norm": 44.67154312133789,
      "learning_rate": 2.425868655630344e-06,
      "loss": 1.0736,
      "step": 11075
    },
    {
      "epoch": 9.281112737920937,
      "grad_norm": 39.08327865600586,
      "learning_rate": 2.357649627069311e-06,
      "loss": 1.0518,
      "step": 11100
    },
    {
      "epoch": 9.302028864254341,
      "grad_norm": 72.80323791503906,
      "learning_rate": 2.2894305985082772e-06,
      "loss": 1.2051,
      "step": 11125
    },
    {
      "epoch": 9.322944990587743,
      "grad_norm": 64.72706604003906,
      "learning_rate": 2.221211569947244e-06,
      "loss": 1.0998,
      "step": 11150
    },
    {
      "epoch": 9.343861116921147,
      "grad_norm": 49.302207946777344,
      "learning_rate": 2.152992541386211e-06,
      "loss": 1.0105,
      "step": 11175
    },
    {
      "epoch": 9.364777243254549,
      "grad_norm": 50.14400100708008,
      "learning_rate": 2.0847735128251774e-06,
      "loss": 1.1695,
      "step": 11200
    },
    {
      "epoch": 9.385693369587953,
      "grad_norm": 62.76263427734375,
      "learning_rate": 2.0165544842641438e-06,
      "loss": 1.0959,
      "step": 11225
    },
    {
      "epoch": 9.406609495921355,
      "grad_norm": 45.978294372558594,
      "learning_rate": 1.9483354557031106e-06,
      "loss": 1.1288,
      "step": 11250
    },
    {
      "epoch": 9.427525622254759,
      "grad_norm": 44.78234100341797,
      "learning_rate": 1.8801164271420777e-06,
      "loss": 1.1414,
      "step": 11275
    },
    {
      "epoch": 9.448441748588161,
      "grad_norm": 40.86542892456055,
      "learning_rate": 1.8118973985810441e-06,
      "loss": 1.0788,
      "step": 11300
    },
    {
      "epoch": 9.469357874921565,
      "grad_norm": 39.37490463256836,
      "learning_rate": 1.743678370020011e-06,
      "loss": 1.1237,
      "step": 11325
    },
    {
      "epoch": 9.490274001254967,
      "grad_norm": 54.699005126953125,
      "learning_rate": 1.6754593414589776e-06,
      "loss": 1.153,
      "step": 11350
    },
    {
      "epoch": 9.511190127588371,
      "grad_norm": 57.05842208862305,
      "learning_rate": 1.6072403128979444e-06,
      "loss": 1.1356,
      "step": 11375
    },
    {
      "epoch": 9.532106253921773,
      "grad_norm": 63.186920166015625,
      "learning_rate": 1.539021284336911e-06,
      "loss": 1.1615,
      "step": 11400
    },
    {
      "epoch": 9.553022380255177,
      "grad_norm": 44.35051727294922,
      "learning_rate": 1.470802255775878e-06,
      "loss": 1.2092,
      "step": 11425
    },
    {
      "epoch": 9.57393850658858,
      "grad_norm": 39.18497848510742,
      "learning_rate": 1.4025832272148446e-06,
      "loss": 1.1708,
      "step": 11450
    },
    {
      "epoch": 9.594854632921983,
      "grad_norm": 50.94379806518555,
      "learning_rate": 1.3343641986538112e-06,
      "loss": 1.1514,
      "step": 11475
    },
    {
      "epoch": 9.615770759255385,
      "grad_norm": 54.37767028808594,
      "learning_rate": 1.2661451700927778e-06,
      "loss": 1.109,
      "step": 11500
    },
    {
      "epoch": 9.636686885588789,
      "grad_norm": 35.744468688964844,
      "learning_rate": 1.1979261415317445e-06,
      "loss": 1.0681,
      "step": 11525
    },
    {
      "epoch": 9.657603011922191,
      "grad_norm": 52.72093200683594,
      "learning_rate": 1.1297071129707113e-06,
      "loss": 1.1138,
      "step": 11550
    },
    {
      "epoch": 9.678519138255595,
      "grad_norm": 54.089210510253906,
      "learning_rate": 1.061488084409678e-06,
      "loss": 1.0752,
      "step": 11575
    },
    {
      "epoch": 9.699435264588999,
      "grad_norm": 46.003849029541016,
      "learning_rate": 9.932690558486448e-07,
      "loss": 1.1278,
      "step": 11600
    },
    {
      "epoch": 9.720351390922401,
      "grad_norm": 44.047080993652344,
      "learning_rate": 9.250500272876114e-07,
      "loss": 1.1797,
      "step": 11625
    },
    {
      "epoch": 9.741267517255805,
      "grad_norm": 53.69533920288086,
      "learning_rate": 8.568309987265781e-07,
      "loss": 1.1509,
      "step": 11650
    },
    {
      "epoch": 9.762183643589207,
      "grad_norm": 41.495277404785156,
      "learning_rate": 7.886119701655448e-07,
      "loss": 1.1377,
      "step": 11675
    },
    {
      "epoch": 9.783099769922611,
      "grad_norm": 40.80923843383789,
      "learning_rate": 7.203929416045116e-07,
      "loss": 1.1384,
      "step": 11700
    },
    {
      "epoch": 9.804015896256013,
      "grad_norm": 55.741943359375,
      "learning_rate": 6.521739130434783e-07,
      "loss": 1.1375,
      "step": 11725
    },
    {
      "epoch": 9.824932022589417,
      "grad_norm": 63.67601013183594,
      "learning_rate": 5.839548844824449e-07,
      "loss": 1.176,
      "step": 11750
    },
    {
      "epoch": 9.84584814892282,
      "grad_norm": 48.065589904785156,
      "learning_rate": 5.157358559214117e-07,
      "loss": 1.1155,
      "step": 11775
    },
    {
      "epoch": 9.866764275256223,
      "grad_norm": 47.166603088378906,
      "learning_rate": 4.4751682736037843e-07,
      "loss": 1.1114,
      "step": 11800
    },
    {
      "epoch": 9.887680401589625,
      "grad_norm": 46.51051712036133,
      "learning_rate": 3.792977987993451e-07,
      "loss": 1.0621,
      "step": 11825
    },
    {
      "epoch": 9.908596527923029,
      "grad_norm": 40.68550491333008,
      "learning_rate": 3.110787702383118e-07,
      "loss": 1.0567,
      "step": 11850
    },
    {
      "epoch": 9.929512654256431,
      "grad_norm": 85.87374877929688,
      "learning_rate": 2.428597416772785e-07,
      "loss": 1.1742,
      "step": 11875
    },
    {
      "epoch": 9.950428780589835,
      "grad_norm": 43.881526947021484,
      "learning_rate": 1.7464071311624524e-07,
      "loss": 1.1123,
      "step": 11900
    },
    {
      "epoch": 9.971344906923237,
      "grad_norm": 31.50031089782715,
      "learning_rate": 1.0642168455521194e-07,
      "loss": 1.0973,
      "step": 11925
    },
    {
      "epoch": 9.992261033256641,
      "grad_norm": 39.137908935546875,
      "learning_rate": 3.8202655994178645e-08,
      "loss": 1.1446,
      "step": 11950
    }
  ],
  "logging_steps": 25,
  "max_steps": 11950,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
