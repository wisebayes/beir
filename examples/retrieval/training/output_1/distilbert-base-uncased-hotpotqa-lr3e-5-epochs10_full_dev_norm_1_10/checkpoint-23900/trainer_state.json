{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 19.98389458272328,
  "eval_steps": 500,
  "global_step": 23900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020916126333403055,
      "grad_norm": 5400.39892578125,
      "learning_rate": 4.707112970711297e-07,
      "loss": 614.4612,
      "step": 25
    },
    {
      "epoch": 0.04183225266680611,
      "grad_norm": 5050.5712890625,
      "learning_rate": 1.2552301255230125e-06,
      "loss": 544.868,
      "step": 50
    },
    {
      "epoch": 0.06274837900020916,
      "grad_norm": 3440.327392578125,
      "learning_rate": 2.0397489539748953e-06,
      "loss": 410.5588,
      "step": 75
    },
    {
      "epoch": 0.08366450533361222,
      "grad_norm": 1824.9468994140625,
      "learning_rate": 2.824267782426778e-06,
      "loss": 246.8498,
      "step": 100
    },
    {
      "epoch": 0.10458063166701527,
      "grad_norm": 868.1990966796875,
      "learning_rate": 3.608786610878661e-06,
      "loss": 128.7299,
      "step": 125
    },
    {
      "epoch": 0.12549675800041832,
      "grad_norm": 403.3534851074219,
      "learning_rate": 4.3933054393305435e-06,
      "loss": 59.0294,
      "step": 150
    },
    {
      "epoch": 0.14641288433382138,
      "grad_norm": 137.78469848632812,
      "learning_rate": 5.177824267782427e-06,
      "loss": 24.9046,
      "step": 175
    },
    {
      "epoch": 0.16732901066722444,
      "grad_norm": 75.9774398803711,
      "learning_rate": 5.96234309623431e-06,
      "loss": 14.0487,
      "step": 200
    },
    {
      "epoch": 0.18824513700062748,
      "grad_norm": 38.212711334228516,
      "learning_rate": 6.7468619246861925e-06,
      "loss": 11.9214,
      "step": 225
    },
    {
      "epoch": 0.20916126333403054,
      "grad_norm": 32.30985641479492,
      "learning_rate": 7.531380753138075e-06,
      "loss": 11.4853,
      "step": 250
    },
    {
      "epoch": 0.2300773896674336,
      "grad_norm": 25.5565185546875,
      "learning_rate": 8.315899581589959e-06,
      "loss": 11.337,
      "step": 275
    },
    {
      "epoch": 0.25099351600083664,
      "grad_norm": 23.04122543334961,
      "learning_rate": 9.10041841004184e-06,
      "loss": 11.2873,
      "step": 300
    },
    {
      "epoch": 0.2719096423342397,
      "grad_norm": 19.188146591186523,
      "learning_rate": 9.884937238493724e-06,
      "loss": 11.2497,
      "step": 325
    },
    {
      "epoch": 0.29282576866764276,
      "grad_norm": 17.96991539001465,
      "learning_rate": 1.0669456066945606e-05,
      "loss": 11.2389,
      "step": 350
    },
    {
      "epoch": 0.3137418950010458,
      "grad_norm": 17.646469116210938,
      "learning_rate": 1.145397489539749e-05,
      "loss": 11.2119,
      "step": 375
    },
    {
      "epoch": 0.3346580213344489,
      "grad_norm": 37.6409912109375,
      "learning_rate": 1.2238493723849372e-05,
      "loss": 11.1853,
      "step": 400
    },
    {
      "epoch": 0.3555741476678519,
      "grad_norm": 19.156179428100586,
      "learning_rate": 1.3023012552301255e-05,
      "loss": 11.2057,
      "step": 425
    },
    {
      "epoch": 0.37649027400125495,
      "grad_norm": 15.911468505859375,
      "learning_rate": 1.3807531380753137e-05,
      "loss": 11.1673,
      "step": 450
    },
    {
      "epoch": 0.397406400334658,
      "grad_norm": 15.823843955993652,
      "learning_rate": 1.459205020920502e-05,
      "loss": 11.1487,
      "step": 475
    },
    {
      "epoch": 0.4183225266680611,
      "grad_norm": 15.100357055664062,
      "learning_rate": 1.5376569037656903e-05,
      "loss": 11.1376,
      "step": 500
    },
    {
      "epoch": 0.43923865300146414,
      "grad_norm": 13.949849128723145,
      "learning_rate": 1.6161087866108788e-05,
      "loss": 11.1123,
      "step": 525
    },
    {
      "epoch": 0.4601547793348672,
      "grad_norm": 13.398735046386719,
      "learning_rate": 1.694560669456067e-05,
      "loss": 11.1075,
      "step": 550
    },
    {
      "epoch": 0.4810709056682702,
      "grad_norm": 13.775729179382324,
      "learning_rate": 1.7730125523012555e-05,
      "loss": 11.0876,
      "step": 575
    },
    {
      "epoch": 0.5019870320016733,
      "grad_norm": 17.06467628479004,
      "learning_rate": 1.8514644351464433e-05,
      "loss": 11.0598,
      "step": 600
    },
    {
      "epoch": 0.5229031583350764,
      "grad_norm": 14.366186141967773,
      "learning_rate": 1.929916317991632e-05,
      "loss": 11.0428,
      "step": 625
    },
    {
      "epoch": 0.5438192846684794,
      "grad_norm": 23.493080139160156,
      "learning_rate": 2.00836820083682e-05,
      "loss": 11.0247,
      "step": 650
    },
    {
      "epoch": 0.5647354110018824,
      "grad_norm": 87.44949340820312,
      "learning_rate": 2.0868200836820086e-05,
      "loss": 10.8647,
      "step": 675
    },
    {
      "epoch": 0.5856515373352855,
      "grad_norm": 31.62745475769043,
      "learning_rate": 2.1652719665271968e-05,
      "loss": 10.3901,
      "step": 700
    },
    {
      "epoch": 0.6065676636686885,
      "grad_norm": 29.54443359375,
      "learning_rate": 2.243723849372385e-05,
      "loss": 10.077,
      "step": 725
    },
    {
      "epoch": 0.6274837900020916,
      "grad_norm": 39.457210540771484,
      "learning_rate": 2.322175732217573e-05,
      "loss": 9.5194,
      "step": 750
    },
    {
      "epoch": 0.6483999163354947,
      "grad_norm": 44.28434371948242,
      "learning_rate": 2.4006276150627617e-05,
      "loss": 9.4812,
      "step": 775
    },
    {
      "epoch": 0.6693160426688978,
      "grad_norm": 39.105201721191406,
      "learning_rate": 2.47907949790795e-05,
      "loss": 9.1373,
      "step": 800
    },
    {
      "epoch": 0.6902321690023008,
      "grad_norm": 34.316375732421875,
      "learning_rate": 2.557531380753138e-05,
      "loss": 8.606,
      "step": 825
    },
    {
      "epoch": 0.7111482953357038,
      "grad_norm": 46.7210693359375,
      "learning_rate": 2.6359832635983262e-05,
      "loss": 8.5544,
      "step": 850
    },
    {
      "epoch": 0.7320644216691069,
      "grad_norm": 44.76898193359375,
      "learning_rate": 2.7144351464435148e-05,
      "loss": 8.2767,
      "step": 875
    },
    {
      "epoch": 0.7529805480025099,
      "grad_norm": 96.21385955810547,
      "learning_rate": 2.792887029288703e-05,
      "loss": 8.3528,
      "step": 900
    },
    {
      "epoch": 0.773896674335913,
      "grad_norm": 60.18143844604492,
      "learning_rate": 2.8713389121338915e-05,
      "loss": 8.3404,
      "step": 925
    },
    {
      "epoch": 0.794812800669316,
      "grad_norm": 45.53428649902344,
      "learning_rate": 2.9497907949790793e-05,
      "loss": 8.1798,
      "step": 950
    },
    {
      "epoch": 0.8157289270027192,
      "grad_norm": 59.13080596923828,
      "learning_rate": 2.9988232217573223e-05,
      "loss": 7.9366,
      "step": 975
    },
    {
      "epoch": 0.8366450533361222,
      "grad_norm": 43.46519470214844,
      "learning_rate": 2.9955543933054394e-05,
      "loss": 7.8792,
      "step": 1000
    },
    {
      "epoch": 0.8575611796695252,
      "grad_norm": 53.701480865478516,
      "learning_rate": 2.9922855648535565e-05,
      "loss": 7.6574,
      "step": 1025
    },
    {
      "epoch": 0.8784773060029283,
      "grad_norm": 45.497432708740234,
      "learning_rate": 2.989016736401674e-05,
      "loss": 7.1094,
      "step": 1050
    },
    {
      "epoch": 0.8993934323363313,
      "grad_norm": 49.49336624145508,
      "learning_rate": 2.985747907949791e-05,
      "loss": 7.0538,
      "step": 1075
    },
    {
      "epoch": 0.9203095586697344,
      "grad_norm": 38.43732452392578,
      "learning_rate": 2.982479079497908e-05,
      "loss": 6.8486,
      "step": 1100
    },
    {
      "epoch": 0.9412256850031374,
      "grad_norm": 49.343963623046875,
      "learning_rate": 2.979210251046025e-05,
      "loss": 6.6782,
      "step": 1125
    },
    {
      "epoch": 0.9621418113365404,
      "grad_norm": 35.412879943847656,
      "learning_rate": 2.9759414225941424e-05,
      "loss": 6.7144,
      "step": 1150
    },
    {
      "epoch": 0.9830579376699435,
      "grad_norm": 47.917598724365234,
      "learning_rate": 2.9726725941422595e-05,
      "loss": 6.4156,
      "step": 1175
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.5533455610275269,
      "eval_runtime": 8.4846,
      "eval_samples_per_second": 2003.638,
      "eval_steps_per_second": 62.702,
      "step": 1196
    },
    {
      "epoch": 1.0033465802133446,
      "grad_norm": 57.200313568115234,
      "learning_rate": 2.9694037656903766e-05,
      "loss": 6.1375,
      "step": 1200
    },
    {
      "epoch": 1.0242627065467476,
      "grad_norm": 54.88780212402344,
      "learning_rate": 2.9661349372384938e-05,
      "loss": 6.1071,
      "step": 1225
    },
    {
      "epoch": 1.0451788328801506,
      "grad_norm": 57.37696075439453,
      "learning_rate": 2.962866108786611e-05,
      "loss": 6.1192,
      "step": 1250
    },
    {
      "epoch": 1.0660949592135536,
      "grad_norm": 56.68855667114258,
      "learning_rate": 2.9595972803347283e-05,
      "loss": 6.0809,
      "step": 1275
    },
    {
      "epoch": 1.0870110855469568,
      "grad_norm": 36.18335723876953,
      "learning_rate": 2.9563284518828454e-05,
      "loss": 5.9927,
      "step": 1300
    },
    {
      "epoch": 1.1079272118803598,
      "grad_norm": 38.543521881103516,
      "learning_rate": 2.9530596234309622e-05,
      "loss": 5.9079,
      "step": 1325
    },
    {
      "epoch": 1.1288433382137628,
      "grad_norm": 38.82518005371094,
      "learning_rate": 2.9497907949790793e-05,
      "loss": 5.7574,
      "step": 1350
    },
    {
      "epoch": 1.1497594645471658,
      "grad_norm": 40.15243148803711,
      "learning_rate": 2.9465219665271968e-05,
      "loss": 5.5779,
      "step": 1375
    },
    {
      "epoch": 1.1706755908805688,
      "grad_norm": 42.095401763916016,
      "learning_rate": 2.943253138075314e-05,
      "loss": 5.6516,
      "step": 1400
    },
    {
      "epoch": 1.191591717213972,
      "grad_norm": 48.205020904541016,
      "learning_rate": 2.939984309623431e-05,
      "loss": 5.685,
      "step": 1425
    },
    {
      "epoch": 1.212507843547375,
      "grad_norm": 34.613868713378906,
      "learning_rate": 2.936715481171548e-05,
      "loss": 5.4679,
      "step": 1450
    },
    {
      "epoch": 1.233423969880778,
      "grad_norm": 38.41480255126953,
      "learning_rate": 2.9334466527196656e-05,
      "loss": 5.2991,
      "step": 1475
    },
    {
      "epoch": 1.254340096214181,
      "grad_norm": 39.942481994628906,
      "learning_rate": 2.9301778242677827e-05,
      "loss": 5.3907,
      "step": 1500
    },
    {
      "epoch": 1.275256222547584,
      "grad_norm": 37.187889099121094,
      "learning_rate": 2.9269089958158995e-05,
      "loss": 5.3169,
      "step": 1525
    },
    {
      "epoch": 1.2961723488809873,
      "grad_norm": 45.92313766479492,
      "learning_rate": 2.9236401673640166e-05,
      "loss": 5.2292,
      "step": 1550
    },
    {
      "epoch": 1.3170884752143903,
      "grad_norm": 39.5207633972168,
      "learning_rate": 2.9203713389121337e-05,
      "loss": 4.8167,
      "step": 1575
    },
    {
      "epoch": 1.3380046015477933,
      "grad_norm": 45.42135238647461,
      "learning_rate": 2.9171025104602512e-05,
      "loss": 4.8537,
      "step": 1600
    },
    {
      "epoch": 1.3589207278811963,
      "grad_norm": 46.22455596923828,
      "learning_rate": 2.9138336820083683e-05,
      "loss": 4.8508,
      "step": 1625
    },
    {
      "epoch": 1.3798368542145996,
      "grad_norm": 41.989036560058594,
      "learning_rate": 2.9105648535564854e-05,
      "loss": 4.7141,
      "step": 1650
    },
    {
      "epoch": 1.4007529805480026,
      "grad_norm": 39.02372741699219,
      "learning_rate": 2.9072960251046025e-05,
      "loss": 4.8972,
      "step": 1675
    },
    {
      "epoch": 1.4216691068814056,
      "grad_norm": 46.348228454589844,
      "learning_rate": 2.90402719665272e-05,
      "loss": 5.0055,
      "step": 1700
    },
    {
      "epoch": 1.4425852332148086,
      "grad_norm": 46.07050704956055,
      "learning_rate": 2.900758368200837e-05,
      "loss": 4.5549,
      "step": 1725
    },
    {
      "epoch": 1.4635013595482116,
      "grad_norm": 51.54418182373047,
      "learning_rate": 2.897489539748954e-05,
      "loss": 4.4827,
      "step": 1750
    },
    {
      "epoch": 1.4844174858816146,
      "grad_norm": 50.712581634521484,
      "learning_rate": 2.894220711297071e-05,
      "loss": 4.3768,
      "step": 1775
    },
    {
      "epoch": 1.5053336122150178,
      "grad_norm": 47.070777893066406,
      "learning_rate": 2.8909518828451885e-05,
      "loss": 4.4797,
      "step": 1800
    },
    {
      "epoch": 1.5262497385484208,
      "grad_norm": 54.29220199584961,
      "learning_rate": 2.8876830543933056e-05,
      "loss": 4.3233,
      "step": 1825
    },
    {
      "epoch": 1.5471658648818238,
      "grad_norm": 63.696929931640625,
      "learning_rate": 2.8844142259414227e-05,
      "loss": 4.4144,
      "step": 1850
    },
    {
      "epoch": 1.568081991215227,
      "grad_norm": 59.75983810424805,
      "learning_rate": 2.8811453974895398e-05,
      "loss": 4.3645,
      "step": 1875
    },
    {
      "epoch": 1.58899811754863,
      "grad_norm": 48.04032516479492,
      "learning_rate": 2.877876569037657e-05,
      "loss": 4.1553,
      "step": 1900
    },
    {
      "epoch": 1.609914243882033,
      "grad_norm": 51.91908264160156,
      "learning_rate": 2.8746077405857744e-05,
      "loss": 4.0135,
      "step": 1925
    },
    {
      "epoch": 1.630830370215436,
      "grad_norm": 41.61498260498047,
      "learning_rate": 2.8713389121338915e-05,
      "loss": 3.893,
      "step": 1950
    },
    {
      "epoch": 1.651746496548839,
      "grad_norm": 43.766754150390625,
      "learning_rate": 2.8680700836820083e-05,
      "loss": 3.8662,
      "step": 1975
    },
    {
      "epoch": 1.672662622882242,
      "grad_norm": 46.171531677246094,
      "learning_rate": 2.8648012552301254e-05,
      "loss": 3.7716,
      "step": 2000
    },
    {
      "epoch": 1.693578749215645,
      "grad_norm": 42.792240142822266,
      "learning_rate": 2.861532426778243e-05,
      "loss": 3.6518,
      "step": 2025
    },
    {
      "epoch": 1.7144948755490483,
      "grad_norm": 40.03166198730469,
      "learning_rate": 2.85826359832636e-05,
      "loss": 3.8584,
      "step": 2050
    },
    {
      "epoch": 1.7354110018824513,
      "grad_norm": 36.54743957519531,
      "learning_rate": 2.854994769874477e-05,
      "loss": 3.6515,
      "step": 2075
    },
    {
      "epoch": 1.7563271282158546,
      "grad_norm": 41.48677062988281,
      "learning_rate": 2.8517259414225942e-05,
      "loss": 3.7006,
      "step": 2100
    },
    {
      "epoch": 1.7772432545492576,
      "grad_norm": 57.55256271362305,
      "learning_rate": 2.8484571129707113e-05,
      "loss": 3.5859,
      "step": 2125
    },
    {
      "epoch": 1.7981593808826606,
      "grad_norm": 42.509300231933594,
      "learning_rate": 2.8451882845188288e-05,
      "loss": 3.517,
      "step": 2150
    },
    {
      "epoch": 1.8190755072160636,
      "grad_norm": 48.37084197998047,
      "learning_rate": 2.8419194560669455e-05,
      "loss": 3.4801,
      "step": 2175
    },
    {
      "epoch": 1.8399916335494666,
      "grad_norm": 51.923099517822266,
      "learning_rate": 2.8386506276150627e-05,
      "loss": 3.5071,
      "step": 2200
    },
    {
      "epoch": 1.8609077598828696,
      "grad_norm": 39.82243728637695,
      "learning_rate": 2.8353817991631798e-05,
      "loss": 3.4548,
      "step": 2225
    },
    {
      "epoch": 1.8818238862162726,
      "grad_norm": 35.415531158447266,
      "learning_rate": 2.8321129707112972e-05,
      "loss": 3.551,
      "step": 2250
    },
    {
      "epoch": 1.9027400125496758,
      "grad_norm": 42.85454559326172,
      "learning_rate": 2.8288441422594143e-05,
      "loss": 3.2982,
      "step": 2275
    },
    {
      "epoch": 1.9236561388830788,
      "grad_norm": 40.97826385498047,
      "learning_rate": 2.8255753138075315e-05,
      "loss": 3.2655,
      "step": 2300
    },
    {
      "epoch": 1.9445722652164819,
      "grad_norm": 36.843753814697266,
      "learning_rate": 2.8223064853556486e-05,
      "loss": 3.299,
      "step": 2325
    },
    {
      "epoch": 1.965488391549885,
      "grad_norm": 48.9100227355957,
      "learning_rate": 2.819037656903766e-05,
      "loss": 3.116,
      "step": 2350
    },
    {
      "epoch": 1.986404517883288,
      "grad_norm": 45.533267974853516,
      "learning_rate": 2.815768828451883e-05,
      "loss": 3.2563,
      "step": 2375
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.7869695425033569,
      "eval_runtime": 8.5213,
      "eval_samples_per_second": 1995.011,
      "eval_steps_per_second": 62.432,
      "step": 2392
    },
    {
      "epoch": 2.006693160426689,
      "grad_norm": 38.5340690612793,
      "learning_rate": 2.8125e-05,
      "loss": 3.1901,
      "step": 2400
    },
    {
      "epoch": 2.027609286760092,
      "grad_norm": 49.22029113769531,
      "learning_rate": 2.809231171548117e-05,
      "loss": 3.0254,
      "step": 2425
    },
    {
      "epoch": 2.048525413093495,
      "grad_norm": 37.2214469909668,
      "learning_rate": 2.805962343096234e-05,
      "loss": 2.9005,
      "step": 2450
    },
    {
      "epoch": 2.069441539426898,
      "grad_norm": 42.736106872558594,
      "learning_rate": 2.8026935146443516e-05,
      "loss": 3.0706,
      "step": 2475
    },
    {
      "epoch": 2.090357665760301,
      "grad_norm": 40.59181594848633,
      "learning_rate": 2.7994246861924687e-05,
      "loss": 2.9535,
      "step": 2500
    },
    {
      "epoch": 2.111273792093704,
      "grad_norm": 37.340576171875,
      "learning_rate": 2.796155857740586e-05,
      "loss": 3.0661,
      "step": 2525
    },
    {
      "epoch": 2.132189918427107,
      "grad_norm": 36.093292236328125,
      "learning_rate": 2.792887029288703e-05,
      "loss": 3.0015,
      "step": 2550
    },
    {
      "epoch": 2.15310604476051,
      "grad_norm": 40.876529693603516,
      "learning_rate": 2.7896182008368204e-05,
      "loss": 3.0453,
      "step": 2575
    },
    {
      "epoch": 2.1740221710939136,
      "grad_norm": 52.470943450927734,
      "learning_rate": 2.7863493723849375e-05,
      "loss": 2.9803,
      "step": 2600
    },
    {
      "epoch": 2.1949382974273166,
      "grad_norm": 47.95063781738281,
      "learning_rate": 2.7830805439330543e-05,
      "loss": 2.8875,
      "step": 2625
    },
    {
      "epoch": 2.2158544237607196,
      "grad_norm": 35.663360595703125,
      "learning_rate": 2.7798117154811714e-05,
      "loss": 2.9146,
      "step": 2650
    },
    {
      "epoch": 2.2367705500941226,
      "grad_norm": 33.985443115234375,
      "learning_rate": 2.776542887029289e-05,
      "loss": 3.0393,
      "step": 2675
    },
    {
      "epoch": 2.2576866764275256,
      "grad_norm": 42.17802429199219,
      "learning_rate": 2.773274058577406e-05,
      "loss": 2.8208,
      "step": 2700
    },
    {
      "epoch": 2.2786028027609286,
      "grad_norm": 47.048736572265625,
      "learning_rate": 2.770005230125523e-05,
      "loss": 2.9839,
      "step": 2725
    },
    {
      "epoch": 2.2995189290943316,
      "grad_norm": 45.67631912231445,
      "learning_rate": 2.7667364016736402e-05,
      "loss": 2.7769,
      "step": 2750
    },
    {
      "epoch": 2.3204350554277346,
      "grad_norm": 45.4599494934082,
      "learning_rate": 2.7634675732217573e-05,
      "loss": 2.8205,
      "step": 2775
    },
    {
      "epoch": 2.3413511817611377,
      "grad_norm": 35.68381118774414,
      "learning_rate": 2.7601987447698748e-05,
      "loss": 2.9002,
      "step": 2800
    },
    {
      "epoch": 2.362267308094541,
      "grad_norm": 44.7004280090332,
      "learning_rate": 2.7569299163179916e-05,
      "loss": 2.8541,
      "step": 2825
    },
    {
      "epoch": 2.383183434427944,
      "grad_norm": 42.20182800292969,
      "learning_rate": 2.7536610878661087e-05,
      "loss": 2.8854,
      "step": 2850
    },
    {
      "epoch": 2.404099560761347,
      "grad_norm": 39.62211608886719,
      "learning_rate": 2.7503922594142258e-05,
      "loss": 2.835,
      "step": 2875
    },
    {
      "epoch": 2.42501568709475,
      "grad_norm": 41.126747131347656,
      "learning_rate": 2.7471234309623433e-05,
      "loss": 2.856,
      "step": 2900
    },
    {
      "epoch": 2.445931813428153,
      "grad_norm": 38.5886344909668,
      "learning_rate": 2.7438546025104604e-05,
      "loss": 2.7811,
      "step": 2925
    },
    {
      "epoch": 2.466847939761556,
      "grad_norm": 40.439186096191406,
      "learning_rate": 2.7405857740585775e-05,
      "loss": 2.8198,
      "step": 2950
    },
    {
      "epoch": 2.487764066094959,
      "grad_norm": 45.96098709106445,
      "learning_rate": 2.7373169456066946e-05,
      "loss": 2.8754,
      "step": 2975
    },
    {
      "epoch": 2.508680192428362,
      "grad_norm": 42.95064163208008,
      "learning_rate": 2.7340481171548117e-05,
      "loss": 2.8293,
      "step": 3000
    },
    {
      "epoch": 2.529596318761765,
      "grad_norm": 53.64760208129883,
      "learning_rate": 2.7307792887029292e-05,
      "loss": 2.7363,
      "step": 3025
    },
    {
      "epoch": 2.550512445095168,
      "grad_norm": 35.75900650024414,
      "learning_rate": 2.727510460251046e-05,
      "loss": 2.9321,
      "step": 3050
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 51.944923400878906,
      "learning_rate": 2.724241631799163e-05,
      "loss": 2.7308,
      "step": 3075
    },
    {
      "epoch": 2.5923446977619746,
      "grad_norm": 35.148231506347656,
      "learning_rate": 2.7209728033472802e-05,
      "loss": 2.8159,
      "step": 3100
    },
    {
      "epoch": 2.6132608240953776,
      "grad_norm": 39.896854400634766,
      "learning_rate": 2.7177039748953977e-05,
      "loss": 2.8787,
      "step": 3125
    },
    {
      "epoch": 2.6341769504287806,
      "grad_norm": 37.7285270690918,
      "learning_rate": 2.7144351464435148e-05,
      "loss": 2.7792,
      "step": 3150
    },
    {
      "epoch": 2.6550930767621836,
      "grad_norm": 36.88833236694336,
      "learning_rate": 2.711166317991632e-05,
      "loss": 2.6451,
      "step": 3175
    },
    {
      "epoch": 2.6760092030955867,
      "grad_norm": 40.334590911865234,
      "learning_rate": 2.707897489539749e-05,
      "loss": 2.6023,
      "step": 3200
    },
    {
      "epoch": 2.6969253294289897,
      "grad_norm": 34.47077178955078,
      "learning_rate": 2.7046286610878665e-05,
      "loss": 2.8164,
      "step": 3225
    },
    {
      "epoch": 2.7178414557623927,
      "grad_norm": 38.911590576171875,
      "learning_rate": 2.7013598326359832e-05,
      "loss": 2.7252,
      "step": 3250
    },
    {
      "epoch": 2.738757582095796,
      "grad_norm": 37.815975189208984,
      "learning_rate": 2.6980910041841004e-05,
      "loss": 2.6332,
      "step": 3275
    },
    {
      "epoch": 2.759673708429199,
      "grad_norm": 42.813629150390625,
      "learning_rate": 2.6948221757322175e-05,
      "loss": 2.7831,
      "step": 3300
    },
    {
      "epoch": 2.780589834762602,
      "grad_norm": 37.446895599365234,
      "learning_rate": 2.6915533472803346e-05,
      "loss": 2.5779,
      "step": 3325
    },
    {
      "epoch": 2.801505961096005,
      "grad_norm": 31.052223205566406,
      "learning_rate": 2.688284518828452e-05,
      "loss": 2.6805,
      "step": 3350
    },
    {
      "epoch": 2.822422087429408,
      "grad_norm": 35.38392639160156,
      "learning_rate": 2.685015690376569e-05,
      "loss": 2.5948,
      "step": 3375
    },
    {
      "epoch": 2.843338213762811,
      "grad_norm": 34.216007232666016,
      "learning_rate": 2.6817468619246863e-05,
      "loss": 2.7419,
      "step": 3400
    },
    {
      "epoch": 2.864254340096214,
      "grad_norm": 31.970991134643555,
      "learning_rate": 2.6784780334728034e-05,
      "loss": 2.5551,
      "step": 3425
    },
    {
      "epoch": 2.885170466429617,
      "grad_norm": 45.4676399230957,
      "learning_rate": 2.675209205020921e-05,
      "loss": 2.7116,
      "step": 3450
    },
    {
      "epoch": 2.90608659276302,
      "grad_norm": 44.70127487182617,
      "learning_rate": 2.6719403765690376e-05,
      "loss": 2.6787,
      "step": 3475
    },
    {
      "epoch": 2.927002719096423,
      "grad_norm": 38.760902404785156,
      "learning_rate": 2.6686715481171547e-05,
      "loss": 2.7321,
      "step": 3500
    },
    {
      "epoch": 2.947918845429826,
      "grad_norm": 36.15479278564453,
      "learning_rate": 2.665402719665272e-05,
      "loss": 2.5877,
      "step": 3525
    },
    {
      "epoch": 2.968834971763229,
      "grad_norm": 35.00775909423828,
      "learning_rate": 2.6621338912133893e-05,
      "loss": 2.606,
      "step": 3550
    },
    {
      "epoch": 2.9897510980966326,
      "grad_norm": 49.13982391357422,
      "learning_rate": 2.6588650627615064e-05,
      "loss": 2.5242,
      "step": 3575
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6581467986106873,
      "eval_runtime": 8.5365,
      "eval_samples_per_second": 1991.46,
      "eval_steps_per_second": 62.321,
      "step": 3588
    },
    {
      "epoch": 3.0100397406400337,
      "grad_norm": 38.00890350341797,
      "learning_rate": 2.6555962343096235e-05,
      "loss": 2.382,
      "step": 3600
    },
    {
      "epoch": 3.0309558669734367,
      "grad_norm": 40.18994903564453,
      "learning_rate": 2.6523274058577407e-05,
      "loss": 2.3716,
      "step": 3625
    },
    {
      "epoch": 3.0518719933068397,
      "grad_norm": 35.140743255615234,
      "learning_rate": 2.6490585774058578e-05,
      "loss": 2.4102,
      "step": 3650
    },
    {
      "epoch": 3.0727881196402427,
      "grad_norm": 37.69943618774414,
      "learning_rate": 2.6457897489539752e-05,
      "loss": 2.4072,
      "step": 3675
    },
    {
      "epoch": 3.0937042459736457,
      "grad_norm": 39.663429260253906,
      "learning_rate": 2.642520920502092e-05,
      "loss": 2.5143,
      "step": 3700
    },
    {
      "epoch": 3.1146203723070487,
      "grad_norm": 31.906238555908203,
      "learning_rate": 2.639252092050209e-05,
      "loss": 2.4602,
      "step": 3725
    },
    {
      "epoch": 3.1355364986404517,
      "grad_norm": 37.000244140625,
      "learning_rate": 2.6359832635983262e-05,
      "loss": 2.408,
      "step": 3750
    },
    {
      "epoch": 3.1564526249738547,
      "grad_norm": 32.52442932128906,
      "learning_rate": 2.6327144351464437e-05,
      "loss": 2.3126,
      "step": 3775
    },
    {
      "epoch": 3.1773687513072577,
      "grad_norm": 38.22536849975586,
      "learning_rate": 2.6294456066945608e-05,
      "loss": 2.2041,
      "step": 3800
    },
    {
      "epoch": 3.1982848776406607,
      "grad_norm": 31.394052505493164,
      "learning_rate": 2.626176778242678e-05,
      "loss": 2.5298,
      "step": 3825
    },
    {
      "epoch": 3.219201003974064,
      "grad_norm": 34.0186882019043,
      "learning_rate": 2.622907949790795e-05,
      "loss": 2.5002,
      "step": 3850
    },
    {
      "epoch": 3.240117130307467,
      "grad_norm": 44.939849853515625,
      "learning_rate": 2.619639121338912e-05,
      "loss": 2.4565,
      "step": 3875
    },
    {
      "epoch": 3.26103325664087,
      "grad_norm": 46.42436981201172,
      "learning_rate": 2.6163702928870293e-05,
      "loss": 2.5427,
      "step": 3900
    },
    {
      "epoch": 3.281949382974273,
      "grad_norm": 34.502052307128906,
      "learning_rate": 2.6131014644351464e-05,
      "loss": 2.4844,
      "step": 3925
    },
    {
      "epoch": 3.302865509307676,
      "grad_norm": 40.53841018676758,
      "learning_rate": 2.6098326359832635e-05,
      "loss": 2.4338,
      "step": 3950
    },
    {
      "epoch": 3.323781635641079,
      "grad_norm": 34.72478485107422,
      "learning_rate": 2.6065638075313806e-05,
      "loss": 2.3609,
      "step": 3975
    },
    {
      "epoch": 3.344697761974482,
      "grad_norm": 39.837093353271484,
      "learning_rate": 2.603294979079498e-05,
      "loss": 2.4852,
      "step": 4000
    },
    {
      "epoch": 3.365613888307885,
      "grad_norm": 42.775882720947266,
      "learning_rate": 2.6000261506276152e-05,
      "loss": 2.3381,
      "step": 4025
    },
    {
      "epoch": 3.3865300146412887,
      "grad_norm": 37.352882385253906,
      "learning_rate": 2.5967573221757323e-05,
      "loss": 2.2397,
      "step": 4050
    },
    {
      "epoch": 3.4074461409746917,
      "grad_norm": 34.35127639770508,
      "learning_rate": 2.5934884937238494e-05,
      "loss": 2.4058,
      "step": 4075
    },
    {
      "epoch": 3.4283622673080947,
      "grad_norm": 38.82127380371094,
      "learning_rate": 2.590219665271967e-05,
      "loss": 2.4348,
      "step": 4100
    },
    {
      "epoch": 3.4492783936414977,
      "grad_norm": 38.87916946411133,
      "learning_rate": 2.5869508368200837e-05,
      "loss": 2.2591,
      "step": 4125
    },
    {
      "epoch": 3.4701945199749007,
      "grad_norm": 35.88007736206055,
      "learning_rate": 2.5836820083682008e-05,
      "loss": 2.091,
      "step": 4150
    },
    {
      "epoch": 3.4911106463083037,
      "grad_norm": 30.668380737304688,
      "learning_rate": 2.580413179916318e-05,
      "loss": 2.2936,
      "step": 4175
    },
    {
      "epoch": 3.5120267726417067,
      "grad_norm": 37.709938049316406,
      "learning_rate": 2.577144351464435e-05,
      "loss": 2.1838,
      "step": 4200
    },
    {
      "epoch": 3.5329428989751097,
      "grad_norm": 44.42420959472656,
      "learning_rate": 2.5738755230125525e-05,
      "loss": 2.2318,
      "step": 4225
    },
    {
      "epoch": 3.5538590253085127,
      "grad_norm": 33.969329833984375,
      "learning_rate": 2.5706066945606696e-05,
      "loss": 2.314,
      "step": 4250
    },
    {
      "epoch": 3.5747751516419157,
      "grad_norm": 39.21636962890625,
      "learning_rate": 2.5673378661087867e-05,
      "loss": 2.3764,
      "step": 4275
    },
    {
      "epoch": 3.5956912779753187,
      "grad_norm": 90.32324981689453,
      "learning_rate": 2.5640690376569038e-05,
      "loss": 2.2741,
      "step": 4300
    },
    {
      "epoch": 3.616607404308722,
      "grad_norm": 38.82451248168945,
      "learning_rate": 2.5608002092050213e-05,
      "loss": 2.1413,
      "step": 4325
    },
    {
      "epoch": 3.637523530642125,
      "grad_norm": 24.9266300201416,
      "learning_rate": 2.557531380753138e-05,
      "loss": 2.1878,
      "step": 4350
    },
    {
      "epoch": 3.658439656975528,
      "grad_norm": 32.64738464355469,
      "learning_rate": 2.5542625523012552e-05,
      "loss": 2.3597,
      "step": 4375
    },
    {
      "epoch": 3.679355783308931,
      "grad_norm": 25.64309310913086,
      "learning_rate": 2.5509937238493723e-05,
      "loss": 2.3349,
      "step": 4400
    },
    {
      "epoch": 3.700271909642334,
      "grad_norm": 40.85431671142578,
      "learning_rate": 2.5477248953974897e-05,
      "loss": 2.1769,
      "step": 4425
    },
    {
      "epoch": 3.721188035975737,
      "grad_norm": 33.09577178955078,
      "learning_rate": 2.544456066945607e-05,
      "loss": 2.3394,
      "step": 4450
    },
    {
      "epoch": 3.7421041623091402,
      "grad_norm": 37.00690460205078,
      "learning_rate": 2.541187238493724e-05,
      "loss": 2.3737,
      "step": 4475
    },
    {
      "epoch": 3.7630202886425437,
      "grad_norm": 39.36859893798828,
      "learning_rate": 2.537918410041841e-05,
      "loss": 2.2395,
      "step": 4500
    },
    {
      "epoch": 3.7839364149759467,
      "grad_norm": 35.92881774902344,
      "learning_rate": 2.5346495815899582e-05,
      "loss": 2.2581,
      "step": 4525
    },
    {
      "epoch": 3.8048525413093497,
      "grad_norm": 37.90081024169922,
      "learning_rate": 2.5313807531380753e-05,
      "loss": 2.2883,
      "step": 4550
    },
    {
      "epoch": 3.8257686676427527,
      "grad_norm": 30.94475746154785,
      "learning_rate": 2.5281119246861924e-05,
      "loss": 2.0617,
      "step": 4575
    },
    {
      "epoch": 3.8466847939761557,
      "grad_norm": 34.117340087890625,
      "learning_rate": 2.5248430962343096e-05,
      "loss": 2.0931,
      "step": 4600
    },
    {
      "epoch": 3.8676009203095587,
      "grad_norm": 27.530630111694336,
      "learning_rate": 2.5215742677824267e-05,
      "loss": 2.1205,
      "step": 4625
    },
    {
      "epoch": 3.8885170466429617,
      "grad_norm": 29.903762817382812,
      "learning_rate": 2.518305439330544e-05,
      "loss": 2.0244,
      "step": 4650
    },
    {
      "epoch": 3.9094331729763647,
      "grad_norm": 42.268497467041016,
      "learning_rate": 2.5150366108786612e-05,
      "loss": 2.1658,
      "step": 4675
    },
    {
      "epoch": 3.9303492993097677,
      "grad_norm": 39.99273681640625,
      "learning_rate": 2.5117677824267784e-05,
      "loss": 2.1116,
      "step": 4700
    },
    {
      "epoch": 3.9512654256431707,
      "grad_norm": 30.044898986816406,
      "learning_rate": 2.5084989539748955e-05,
      "loss": 2.1416,
      "step": 4725
    },
    {
      "epoch": 3.9721815519765737,
      "grad_norm": 27.161867141723633,
      "learning_rate": 2.505230125523013e-05,
      "loss": 2.1715,
      "step": 4750
    },
    {
      "epoch": 3.9930976783099768,
      "grad_norm": 38.669334411621094,
      "learning_rate": 2.5019612970711297e-05,
      "loss": 2.1152,
      "step": 4775
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.559990406036377,
      "eval_runtime": 8.4832,
      "eval_samples_per_second": 2003.965,
      "eval_steps_per_second": 62.712,
      "step": 4784
    },
    {
      "epoch": 4.013386320853378,
      "grad_norm": 26.976253509521484,
      "learning_rate": 2.498692468619247e-05,
      "loss": 1.8343,
      "step": 4800
    },
    {
      "epoch": 4.034302447186781,
      "grad_norm": 30.597766876220703,
      "learning_rate": 2.495423640167364e-05,
      "loss": 1.8235,
      "step": 4825
    },
    {
      "epoch": 4.055218573520184,
      "grad_norm": 33.2861213684082,
      "learning_rate": 2.492154811715481e-05,
      "loss": 2.143,
      "step": 4850
    },
    {
      "epoch": 4.076134699853587,
      "grad_norm": 42.455257415771484,
      "learning_rate": 2.4888859832635985e-05,
      "loss": 1.8487,
      "step": 4875
    },
    {
      "epoch": 4.09705082618699,
      "grad_norm": 39.05501174926758,
      "learning_rate": 2.4856171548117156e-05,
      "loss": 1.9212,
      "step": 4900
    },
    {
      "epoch": 4.117966952520393,
      "grad_norm": 42.14129638671875,
      "learning_rate": 2.4823483263598328e-05,
      "loss": 1.8809,
      "step": 4925
    },
    {
      "epoch": 4.138883078853796,
      "grad_norm": 30.236093521118164,
      "learning_rate": 2.47907949790795e-05,
      "loss": 1.859,
      "step": 4950
    },
    {
      "epoch": 4.159799205187199,
      "grad_norm": 44.4649543762207,
      "learning_rate": 2.4758106694560673e-05,
      "loss": 1.8767,
      "step": 4975
    },
    {
      "epoch": 4.180715331520602,
      "grad_norm": 42.26103591918945,
      "learning_rate": 2.472541841004184e-05,
      "loss": 1.9545,
      "step": 5000
    },
    {
      "epoch": 4.201631457854005,
      "grad_norm": 34.36223220825195,
      "learning_rate": 2.4692730125523012e-05,
      "loss": 1.7631,
      "step": 5025
    },
    {
      "epoch": 4.222547584187408,
      "grad_norm": 42.752174377441406,
      "learning_rate": 2.4660041841004183e-05,
      "loss": 1.9293,
      "step": 5050
    },
    {
      "epoch": 4.243463710520811,
      "grad_norm": 38.93925094604492,
      "learning_rate": 2.4627353556485355e-05,
      "loss": 1.8834,
      "step": 5075
    },
    {
      "epoch": 4.264379836854214,
      "grad_norm": 47.95783233642578,
      "learning_rate": 2.459466527196653e-05,
      "loss": 1.913,
      "step": 5100
    },
    {
      "epoch": 4.285295963187617,
      "grad_norm": 31.35797882080078,
      "learning_rate": 2.45619769874477e-05,
      "loss": 1.7772,
      "step": 5125
    },
    {
      "epoch": 4.30621208952102,
      "grad_norm": 28.540489196777344,
      "learning_rate": 2.452928870292887e-05,
      "loss": 1.9589,
      "step": 5150
    },
    {
      "epoch": 4.327128215854424,
      "grad_norm": 28.529783248901367,
      "learning_rate": 2.4496600418410043e-05,
      "loss": 1.9194,
      "step": 5175
    },
    {
      "epoch": 4.348044342187827,
      "grad_norm": 42.76601791381836,
      "learning_rate": 2.4463912133891214e-05,
      "loss": 1.9532,
      "step": 5200
    },
    {
      "epoch": 4.36896046852123,
      "grad_norm": 34.145896911621094,
      "learning_rate": 2.4431223849372385e-05,
      "loss": 1.8412,
      "step": 5225
    },
    {
      "epoch": 4.389876594854633,
      "grad_norm": 30.03343963623047,
      "learning_rate": 2.4398535564853556e-05,
      "loss": 1.7999,
      "step": 5250
    },
    {
      "epoch": 4.410792721188036,
      "grad_norm": 30.171772003173828,
      "learning_rate": 2.4365847280334727e-05,
      "loss": 1.8209,
      "step": 5275
    },
    {
      "epoch": 4.431708847521439,
      "grad_norm": 38.48090744018555,
      "learning_rate": 2.4333158995815902e-05,
      "loss": 1.8233,
      "step": 5300
    },
    {
      "epoch": 4.452624973854842,
      "grad_norm": 34.29977035522461,
      "learning_rate": 2.4300470711297073e-05,
      "loss": 2.0254,
      "step": 5325
    },
    {
      "epoch": 4.473541100188245,
      "grad_norm": 40.307247161865234,
      "learning_rate": 2.4267782426778244e-05,
      "loss": 1.7253,
      "step": 5350
    },
    {
      "epoch": 4.494457226521648,
      "grad_norm": 30.445825576782227,
      "learning_rate": 2.4235094142259415e-05,
      "loss": 1.8376,
      "step": 5375
    },
    {
      "epoch": 4.515373352855051,
      "grad_norm": 32.6163330078125,
      "learning_rate": 2.4202405857740586e-05,
      "loss": 1.9594,
      "step": 5400
    },
    {
      "epoch": 4.536289479188454,
      "grad_norm": 30.84419059753418,
      "learning_rate": 2.4169717573221758e-05,
      "loss": 1.9021,
      "step": 5425
    },
    {
      "epoch": 4.557205605521857,
      "grad_norm": 39.46393585205078,
      "learning_rate": 2.413702928870293e-05,
      "loss": 1.9151,
      "step": 5450
    },
    {
      "epoch": 4.57812173185526,
      "grad_norm": 36.579105377197266,
      "learning_rate": 2.41043410041841e-05,
      "loss": 1.6769,
      "step": 5475
    },
    {
      "epoch": 4.599037858188663,
      "grad_norm": 33.1759033203125,
      "learning_rate": 2.407165271966527e-05,
      "loss": 1.8645,
      "step": 5500
    },
    {
      "epoch": 4.619953984522066,
      "grad_norm": 33.50223159790039,
      "learning_rate": 2.4038964435146446e-05,
      "loss": 1.8434,
      "step": 5525
    },
    {
      "epoch": 4.640870110855469,
      "grad_norm": 39.631282806396484,
      "learning_rate": 2.4006276150627617e-05,
      "loss": 1.8237,
      "step": 5550
    },
    {
      "epoch": 4.661786237188872,
      "grad_norm": 51.42473602294922,
      "learning_rate": 2.3973587866108788e-05,
      "loss": 1.857,
      "step": 5575
    },
    {
      "epoch": 4.682702363522275,
      "grad_norm": 37.58946990966797,
      "learning_rate": 2.394089958158996e-05,
      "loss": 1.7337,
      "step": 5600
    },
    {
      "epoch": 4.703618489855678,
      "grad_norm": 33.70241928100586,
      "learning_rate": 2.390821129707113e-05,
      "loss": 1.7153,
      "step": 5625
    },
    {
      "epoch": 4.724534616189082,
      "grad_norm": 36.2171745300293,
      "learning_rate": 2.38755230125523e-05,
      "loss": 1.8292,
      "step": 5650
    },
    {
      "epoch": 4.745450742522485,
      "grad_norm": 41.70452117919922,
      "learning_rate": 2.3842834728033473e-05,
      "loss": 1.6642,
      "step": 5675
    },
    {
      "epoch": 4.766366868855888,
      "grad_norm": 37.633277893066406,
      "learning_rate": 2.3810146443514644e-05,
      "loss": 1.8444,
      "step": 5700
    },
    {
      "epoch": 4.787282995189291,
      "grad_norm": 28.878374099731445,
      "learning_rate": 2.3777458158995815e-05,
      "loss": 1.9442,
      "step": 5725
    },
    {
      "epoch": 4.808199121522694,
      "grad_norm": 30.541975021362305,
      "learning_rate": 2.374476987447699e-05,
      "loss": 1.8337,
      "step": 5750
    },
    {
      "epoch": 4.829115247856097,
      "grad_norm": 40.24222183227539,
      "learning_rate": 2.371208158995816e-05,
      "loss": 1.7749,
      "step": 5775
    },
    {
      "epoch": 4.8500313741895,
      "grad_norm": 28.57440948486328,
      "learning_rate": 2.3679393305439332e-05,
      "loss": 1.7826,
      "step": 5800
    },
    {
      "epoch": 4.870947500522903,
      "grad_norm": 41.95674514770508,
      "learning_rate": 2.3646705020920503e-05,
      "loss": 1.8249,
      "step": 5825
    },
    {
      "epoch": 4.891863626856306,
      "grad_norm": 39.799842834472656,
      "learning_rate": 2.3614016736401674e-05,
      "loss": 1.8178,
      "step": 5850
    },
    {
      "epoch": 4.912779753189709,
      "grad_norm": 34.65478515625,
      "learning_rate": 2.3581328451882845e-05,
      "loss": 1.7442,
      "step": 5875
    },
    {
      "epoch": 4.933695879523112,
      "grad_norm": 33.862728118896484,
      "learning_rate": 2.3548640167364016e-05,
      "loss": 1.7301,
      "step": 5900
    },
    {
      "epoch": 4.954612005856515,
      "grad_norm": 28.015365600585938,
      "learning_rate": 2.3515951882845188e-05,
      "loss": 1.7219,
      "step": 5925
    },
    {
      "epoch": 4.975528132189918,
      "grad_norm": 34.834415435791016,
      "learning_rate": 2.348326359832636e-05,
      "loss": 1.6625,
      "step": 5950
    },
    {
      "epoch": 4.996444258523321,
      "grad_norm": 36.143882751464844,
      "learning_rate": 2.3450575313807533e-05,
      "loss": 1.7832,
      "step": 5975
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5571882724761963,
      "eval_runtime": 8.5163,
      "eval_samples_per_second": 1996.169,
      "eval_steps_per_second": 62.468,
      "step": 5980
    },
    {
      "epoch": 5.016732901066723,
      "grad_norm": 32.1154670715332,
      "learning_rate": 2.3417887029288705e-05,
      "loss": 1.6186,
      "step": 6000
    },
    {
      "epoch": 5.037649027400126,
      "grad_norm": 39.96543502807617,
      "learning_rate": 2.3385198744769876e-05,
      "loss": 1.7149,
      "step": 6025
    },
    {
      "epoch": 5.058565153733529,
      "grad_norm": 36.822410583496094,
      "learning_rate": 2.3352510460251047e-05,
      "loss": 1.5507,
      "step": 6050
    },
    {
      "epoch": 5.079481280066932,
      "grad_norm": 44.40045928955078,
      "learning_rate": 2.3319822175732218e-05,
      "loss": 1.6611,
      "step": 6075
    },
    {
      "epoch": 5.100397406400335,
      "grad_norm": 33.54010772705078,
      "learning_rate": 2.328713389121339e-05,
      "loss": 1.7019,
      "step": 6100
    },
    {
      "epoch": 5.121313532733738,
      "grad_norm": 36.249691009521484,
      "learning_rate": 2.325444560669456e-05,
      "loss": 1.6496,
      "step": 6125
    },
    {
      "epoch": 5.142229659067141,
      "grad_norm": 40.32558059692383,
      "learning_rate": 2.322175732217573e-05,
      "loss": 1.6236,
      "step": 6150
    },
    {
      "epoch": 5.163145785400544,
      "grad_norm": 39.13908767700195,
      "learning_rate": 2.3189069037656906e-05,
      "loss": 1.6661,
      "step": 6175
    },
    {
      "epoch": 5.184061911733947,
      "grad_norm": 40.881134033203125,
      "learning_rate": 2.3156380753138077e-05,
      "loss": 1.6319,
      "step": 6200
    },
    {
      "epoch": 5.20497803806735,
      "grad_norm": 34.11958694458008,
      "learning_rate": 2.312369246861925e-05,
      "loss": 1.4661,
      "step": 6225
    },
    {
      "epoch": 5.225894164400753,
      "grad_norm": 34.32810974121094,
      "learning_rate": 2.309100418410042e-05,
      "loss": 1.5992,
      "step": 6250
    },
    {
      "epoch": 5.246810290734156,
      "grad_norm": 28.465131759643555,
      "learning_rate": 2.3058315899581587e-05,
      "loss": 1.6284,
      "step": 6275
    },
    {
      "epoch": 5.267726417067559,
      "grad_norm": 25.649587631225586,
      "learning_rate": 2.3025627615062762e-05,
      "loss": 1.701,
      "step": 6300
    },
    {
      "epoch": 5.288642543400962,
      "grad_norm": 21.71462059020996,
      "learning_rate": 2.2992939330543933e-05,
      "loss": 1.608,
      "step": 6325
    },
    {
      "epoch": 5.309558669734365,
      "grad_norm": 28.87844467163086,
      "learning_rate": 2.2960251046025104e-05,
      "loss": 1.503,
      "step": 6350
    },
    {
      "epoch": 5.330474796067768,
      "grad_norm": 40.036224365234375,
      "learning_rate": 2.2927562761506275e-05,
      "loss": 1.5214,
      "step": 6375
    },
    {
      "epoch": 5.351390922401171,
      "grad_norm": 36.487648010253906,
      "learning_rate": 2.289487447698745e-05,
      "loss": 1.6017,
      "step": 6400
    },
    {
      "epoch": 5.372307048734575,
      "grad_norm": 31.970714569091797,
      "learning_rate": 2.286218619246862e-05,
      "loss": 1.4845,
      "step": 6425
    },
    {
      "epoch": 5.393223175067978,
      "grad_norm": 35.62137985229492,
      "learning_rate": 2.2829497907949792e-05,
      "loss": 1.536,
      "step": 6450
    },
    {
      "epoch": 5.414139301401381,
      "grad_norm": 32.83613586425781,
      "learning_rate": 2.2796809623430963e-05,
      "loss": 1.5897,
      "step": 6475
    },
    {
      "epoch": 5.435055427734784,
      "grad_norm": 35.88273620605469,
      "learning_rate": 2.2764121338912135e-05,
      "loss": 1.5673,
      "step": 6500
    },
    {
      "epoch": 5.455971554068187,
      "grad_norm": 28.693172454833984,
      "learning_rate": 2.2731433054393306e-05,
      "loss": 1.4894,
      "step": 6525
    },
    {
      "epoch": 5.47688768040159,
      "grad_norm": 35.980464935302734,
      "learning_rate": 2.2698744769874477e-05,
      "loss": 1.5885,
      "step": 6550
    },
    {
      "epoch": 5.497803806734993,
      "grad_norm": 38.863189697265625,
      "learning_rate": 2.2666056485355648e-05,
      "loss": 1.5837,
      "step": 6575
    },
    {
      "epoch": 5.518719933068396,
      "grad_norm": 31.484832763671875,
      "learning_rate": 2.263336820083682e-05,
      "loss": 1.5758,
      "step": 6600
    },
    {
      "epoch": 5.539636059401799,
      "grad_norm": 31.502220153808594,
      "learning_rate": 2.2600679916317994e-05,
      "loss": 1.5556,
      "step": 6625
    },
    {
      "epoch": 5.560552185735202,
      "grad_norm": 32.45839309692383,
      "learning_rate": 2.2567991631799165e-05,
      "loss": 1.6003,
      "step": 6650
    },
    {
      "epoch": 5.581468312068605,
      "grad_norm": 44.14933395385742,
      "learning_rate": 2.2535303347280336e-05,
      "loss": 1.4955,
      "step": 6675
    },
    {
      "epoch": 5.602384438402008,
      "grad_norm": 29.633941650390625,
      "learning_rate": 2.2502615062761507e-05,
      "loss": 1.5281,
      "step": 6700
    },
    {
      "epoch": 5.623300564735411,
      "grad_norm": 30.220352172851562,
      "learning_rate": 2.246992677824268e-05,
      "loss": 1.5394,
      "step": 6725
    },
    {
      "epoch": 5.644216691068814,
      "grad_norm": 49.69001007080078,
      "learning_rate": 2.243723849372385e-05,
      "loss": 1.5364,
      "step": 6750
    },
    {
      "epoch": 5.665132817402217,
      "grad_norm": 26.133251190185547,
      "learning_rate": 2.240455020920502e-05,
      "loss": 1.6196,
      "step": 6775
    },
    {
      "epoch": 5.68604894373562,
      "grad_norm": 29.224239349365234,
      "learning_rate": 2.2371861924686192e-05,
      "loss": 1.6091,
      "step": 6800
    },
    {
      "epoch": 5.706965070069023,
      "grad_norm": 45.969993591308594,
      "learning_rate": 2.2339173640167363e-05,
      "loss": 1.4852,
      "step": 6825
    },
    {
      "epoch": 5.727881196402427,
      "grad_norm": 33.7819709777832,
      "learning_rate": 2.2306485355648538e-05,
      "loss": 1.5032,
      "step": 6850
    },
    {
      "epoch": 5.748797322735829,
      "grad_norm": 38.84572219848633,
      "learning_rate": 2.227379707112971e-05,
      "loss": 1.6116,
      "step": 6875
    },
    {
      "epoch": 5.769713449069233,
      "grad_norm": 25.383718490600586,
      "learning_rate": 2.224110878661088e-05,
      "loss": 1.6532,
      "step": 6900
    },
    {
      "epoch": 5.790629575402636,
      "grad_norm": 34.56314468383789,
      "learning_rate": 2.2208420502092048e-05,
      "loss": 1.5542,
      "step": 6925
    },
    {
      "epoch": 5.811545701736039,
      "grad_norm": 43.031185150146484,
      "learning_rate": 2.2175732217573222e-05,
      "loss": 1.6638,
      "step": 6950
    },
    {
      "epoch": 5.832461828069442,
      "grad_norm": 39.144325256347656,
      "learning_rate": 2.2143043933054394e-05,
      "loss": 1.6459,
      "step": 6975
    },
    {
      "epoch": 5.853377954402845,
      "grad_norm": 34.127872467041016,
      "learning_rate": 2.2110355648535565e-05,
      "loss": 1.5373,
      "step": 7000
    },
    {
      "epoch": 5.874294080736248,
      "grad_norm": 33.822669982910156,
      "learning_rate": 2.2077667364016736e-05,
      "loss": 1.5356,
      "step": 7025
    },
    {
      "epoch": 5.895210207069651,
      "grad_norm": 38.954833984375,
      "learning_rate": 2.204497907949791e-05,
      "loss": 1.6021,
      "step": 7050
    },
    {
      "epoch": 5.916126333403054,
      "grad_norm": 41.15413284301758,
      "learning_rate": 2.201229079497908e-05,
      "loss": 1.4595,
      "step": 7075
    },
    {
      "epoch": 5.937042459736457,
      "grad_norm": 32.71194839477539,
      "learning_rate": 2.1979602510460253e-05,
      "loss": 1.6702,
      "step": 7100
    },
    {
      "epoch": 5.95795858606986,
      "grad_norm": 42.020694732666016,
      "learning_rate": 2.1946914225941424e-05,
      "loss": 1.5389,
      "step": 7125
    },
    {
      "epoch": 5.978874712403263,
      "grad_norm": 33.305145263671875,
      "learning_rate": 2.191422594142259e-05,
      "loss": 1.6248,
      "step": 7150
    },
    {
      "epoch": 5.999790838736666,
      "grad_norm": 32.67292022705078,
      "learning_rate": 2.1881537656903766e-05,
      "loss": 1.4984,
      "step": 7175
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5099654197692871,
      "eval_runtime": 8.4437,
      "eval_samples_per_second": 2013.332,
      "eval_steps_per_second": 63.005,
      "step": 7176
    },
    {
      "epoch": 6.020079481280067,
      "grad_norm": 40.38900375366211,
      "learning_rate": 2.1848849372384937e-05,
      "loss": 1.3695,
      "step": 7200
    },
    {
      "epoch": 6.04099560761347,
      "grad_norm": 26.928569793701172,
      "learning_rate": 2.181616108786611e-05,
      "loss": 1.3322,
      "step": 7225
    },
    {
      "epoch": 6.061911733946873,
      "grad_norm": 33.16782760620117,
      "learning_rate": 2.178347280334728e-05,
      "loss": 1.4368,
      "step": 7250
    },
    {
      "epoch": 6.082827860280276,
      "grad_norm": 41.569549560546875,
      "learning_rate": 2.1750784518828454e-05,
      "loss": 1.4408,
      "step": 7275
    },
    {
      "epoch": 6.103743986613679,
      "grad_norm": 31.154685974121094,
      "learning_rate": 2.1718096234309625e-05,
      "loss": 1.3616,
      "step": 7300
    },
    {
      "epoch": 6.124660112947082,
      "grad_norm": 26.09613609313965,
      "learning_rate": 2.1685407949790797e-05,
      "loss": 1.4339,
      "step": 7325
    },
    {
      "epoch": 6.145576239280485,
      "grad_norm": 27.100086212158203,
      "learning_rate": 2.1652719665271968e-05,
      "loss": 1.3903,
      "step": 7350
    },
    {
      "epoch": 6.166492365613888,
      "grad_norm": 26.85737419128418,
      "learning_rate": 2.162003138075314e-05,
      "loss": 1.3642,
      "step": 7375
    },
    {
      "epoch": 6.187408491947291,
      "grad_norm": 38.267398834228516,
      "learning_rate": 2.158734309623431e-05,
      "loss": 1.4748,
      "step": 7400
    },
    {
      "epoch": 6.208324618280694,
      "grad_norm": 48.80539321899414,
      "learning_rate": 2.155465481171548e-05,
      "loss": 1.3995,
      "step": 7425
    },
    {
      "epoch": 6.229240744614097,
      "grad_norm": 38.47106170654297,
      "learning_rate": 2.1521966527196652e-05,
      "loss": 1.4648,
      "step": 7450
    },
    {
      "epoch": 6.2501568709475,
      "grad_norm": 33.07378005981445,
      "learning_rate": 2.1489278242677824e-05,
      "loss": 1.3695,
      "step": 7475
    },
    {
      "epoch": 6.271072997280903,
      "grad_norm": 17.479772567749023,
      "learning_rate": 2.1456589958158998e-05,
      "loss": 1.4351,
      "step": 7500
    },
    {
      "epoch": 6.291989123614306,
      "grad_norm": 31.107046127319336,
      "learning_rate": 2.142390167364017e-05,
      "loss": 1.2769,
      "step": 7525
    },
    {
      "epoch": 6.312905249947709,
      "grad_norm": 30.198307037353516,
      "learning_rate": 2.139121338912134e-05,
      "loss": 1.3428,
      "step": 7550
    },
    {
      "epoch": 6.333821376281112,
      "grad_norm": 25.30113983154297,
      "learning_rate": 2.1358525104602508e-05,
      "loss": 1.4283,
      "step": 7575
    },
    {
      "epoch": 6.354737502614515,
      "grad_norm": 32.5764045715332,
      "learning_rate": 2.1325836820083683e-05,
      "loss": 1.3561,
      "step": 7600
    },
    {
      "epoch": 6.375653628947919,
      "grad_norm": 38.8521728515625,
      "learning_rate": 2.1293148535564854e-05,
      "loss": 1.3511,
      "step": 7625
    },
    {
      "epoch": 6.396569755281321,
      "grad_norm": 25.964067459106445,
      "learning_rate": 2.1260460251046025e-05,
      "loss": 1.269,
      "step": 7650
    },
    {
      "epoch": 6.417485881614725,
      "grad_norm": 41.00361633300781,
      "learning_rate": 2.1227771966527196e-05,
      "loss": 1.397,
      "step": 7675
    },
    {
      "epoch": 6.438402007948128,
      "grad_norm": 38.42497253417969,
      "learning_rate": 2.1195083682008367e-05,
      "loss": 1.3518,
      "step": 7700
    },
    {
      "epoch": 6.459318134281531,
      "grad_norm": 27.333850860595703,
      "learning_rate": 2.1162395397489542e-05,
      "loss": 1.4609,
      "step": 7725
    },
    {
      "epoch": 6.480234260614934,
      "grad_norm": 38.18269348144531,
      "learning_rate": 2.1129707112970713e-05,
      "loss": 1.4078,
      "step": 7750
    },
    {
      "epoch": 6.501150386948337,
      "grad_norm": 36.897247314453125,
      "learning_rate": 2.1097018828451884e-05,
      "loss": 1.402,
      "step": 7775
    },
    {
      "epoch": 6.52206651328174,
      "grad_norm": 31.913145065307617,
      "learning_rate": 2.1064330543933052e-05,
      "loss": 1.5066,
      "step": 7800
    },
    {
      "epoch": 6.542982639615143,
      "grad_norm": 29.148845672607422,
      "learning_rate": 2.1031642259414227e-05,
      "loss": 1.4362,
      "step": 7825
    },
    {
      "epoch": 6.563898765948546,
      "grad_norm": 34.00059509277344,
      "learning_rate": 2.0998953974895398e-05,
      "loss": 1.3338,
      "step": 7850
    },
    {
      "epoch": 6.584814892281949,
      "grad_norm": 29.383134841918945,
      "learning_rate": 2.096626569037657e-05,
      "loss": 1.3674,
      "step": 7875
    },
    {
      "epoch": 6.605731018615352,
      "grad_norm": 32.341156005859375,
      "learning_rate": 2.093357740585774e-05,
      "loss": 1.5026,
      "step": 7900
    },
    {
      "epoch": 6.626647144948755,
      "grad_norm": 33.76921463012695,
      "learning_rate": 2.0900889121338915e-05,
      "loss": 1.4372,
      "step": 7925
    },
    {
      "epoch": 6.647563271282158,
      "grad_norm": 31.85840606689453,
      "learning_rate": 2.0868200836820086e-05,
      "loss": 1.4082,
      "step": 7950
    },
    {
      "epoch": 6.668479397615561,
      "grad_norm": 30.279190063476562,
      "learning_rate": 2.0835512552301257e-05,
      "loss": 1.4072,
      "step": 7975
    },
    {
      "epoch": 6.689395523948964,
      "grad_norm": 30.382469177246094,
      "learning_rate": 2.0802824267782428e-05,
      "loss": 1.3574,
      "step": 8000
    },
    {
      "epoch": 6.710311650282367,
      "grad_norm": 31.36379051208496,
      "learning_rate": 2.0770135983263596e-05,
      "loss": 1.4536,
      "step": 8025
    },
    {
      "epoch": 6.73122777661577,
      "grad_norm": 37.385833740234375,
      "learning_rate": 2.073744769874477e-05,
      "loss": 1.3384,
      "step": 8050
    },
    {
      "epoch": 6.752143902949173,
      "grad_norm": 35.48694610595703,
      "learning_rate": 2.0704759414225942e-05,
      "loss": 1.359,
      "step": 8075
    },
    {
      "epoch": 6.773060029282577,
      "grad_norm": 32.149906158447266,
      "learning_rate": 2.0672071129707113e-05,
      "loss": 1.345,
      "step": 8100
    },
    {
      "epoch": 6.7939761556159795,
      "grad_norm": 29.96004295349121,
      "learning_rate": 2.0639382845188284e-05,
      "loss": 1.4538,
      "step": 8125
    },
    {
      "epoch": 6.814892281949383,
      "grad_norm": 34.697471618652344,
      "learning_rate": 2.060669456066946e-05,
      "loss": 1.4173,
      "step": 8150
    },
    {
      "epoch": 6.835808408282786,
      "grad_norm": 30.61328125,
      "learning_rate": 2.057400627615063e-05,
      "loss": 1.3518,
      "step": 8175
    },
    {
      "epoch": 6.856724534616189,
      "grad_norm": 28.3394718170166,
      "learning_rate": 2.05413179916318e-05,
      "loss": 1.5022,
      "step": 8200
    },
    {
      "epoch": 6.877640660949592,
      "grad_norm": 35.582515716552734,
      "learning_rate": 2.050862970711297e-05,
      "loss": 1.3478,
      "step": 8225
    },
    {
      "epoch": 6.898556787282995,
      "grad_norm": 44.907554626464844,
      "learning_rate": 2.0475941422594143e-05,
      "loss": 1.3726,
      "step": 8250
    },
    {
      "epoch": 6.919472913616398,
      "grad_norm": 34.049095153808594,
      "learning_rate": 2.0443253138075314e-05,
      "loss": 1.3032,
      "step": 8275
    },
    {
      "epoch": 6.940389039949801,
      "grad_norm": 39.88676452636719,
      "learning_rate": 2.0410564853556486e-05,
      "loss": 1.4186,
      "step": 8300
    },
    {
      "epoch": 6.961305166283204,
      "grad_norm": 27.51042938232422,
      "learning_rate": 2.0377876569037657e-05,
      "loss": 1.3607,
      "step": 8325
    },
    {
      "epoch": 6.982221292616607,
      "grad_norm": 34.37415313720703,
      "learning_rate": 2.0345188284518828e-05,
      "loss": 1.3865,
      "step": 8350
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5317940711975098,
      "eval_runtime": 8.4122,
      "eval_samples_per_second": 2020.886,
      "eval_steps_per_second": 63.242,
      "step": 8372
    },
    {
      "epoch": 7.002509935160008,
      "grad_norm": 36.09473419189453,
      "learning_rate": 2.0312500000000002e-05,
      "loss": 1.3294,
      "step": 8375
    },
    {
      "epoch": 7.023426061493412,
      "grad_norm": 29.700428009033203,
      "learning_rate": 2.0279811715481174e-05,
      "loss": 1.2017,
      "step": 8400
    },
    {
      "epoch": 7.044342187826815,
      "grad_norm": 32.58342742919922,
      "learning_rate": 2.0247123430962345e-05,
      "loss": 1.3195,
      "step": 8425
    },
    {
      "epoch": 7.065258314160218,
      "grad_norm": 35.26693344116211,
      "learning_rate": 2.0214435146443513e-05,
      "loss": 1.3368,
      "step": 8450
    },
    {
      "epoch": 7.086174440493621,
      "grad_norm": 27.038105010986328,
      "learning_rate": 2.0181746861924687e-05,
      "loss": 1.1384,
      "step": 8475
    },
    {
      "epoch": 7.107090566827024,
      "grad_norm": 36.47840881347656,
      "learning_rate": 2.0149058577405858e-05,
      "loss": 1.1681,
      "step": 8500
    },
    {
      "epoch": 7.128006693160427,
      "grad_norm": 36.410579681396484,
      "learning_rate": 2.011637029288703e-05,
      "loss": 1.2337,
      "step": 8525
    },
    {
      "epoch": 7.14892281949383,
      "grad_norm": 34.712100982666016,
      "learning_rate": 2.00836820083682e-05,
      "loss": 1.2014,
      "step": 8550
    },
    {
      "epoch": 7.169838945827233,
      "grad_norm": 35.41664505004883,
      "learning_rate": 2.0050993723849372e-05,
      "loss": 1.2172,
      "step": 8575
    },
    {
      "epoch": 7.190755072160636,
      "grad_norm": 30.736711502075195,
      "learning_rate": 2.0018305439330546e-05,
      "loss": 1.1622,
      "step": 8600
    },
    {
      "epoch": 7.211671198494039,
      "grad_norm": 24.949796676635742,
      "learning_rate": 1.9985617154811717e-05,
      "loss": 1.28,
      "step": 8625
    },
    {
      "epoch": 7.232587324827442,
      "grad_norm": 32.88331985473633,
      "learning_rate": 1.9952928870292885e-05,
      "loss": 1.1788,
      "step": 8650
    },
    {
      "epoch": 7.253503451160845,
      "grad_norm": 29.562030792236328,
      "learning_rate": 1.9920240585774056e-05,
      "loss": 1.2071,
      "step": 8675
    },
    {
      "epoch": 7.274419577494248,
      "grad_norm": 35.294105529785156,
      "learning_rate": 1.988755230125523e-05,
      "loss": 1.3058,
      "step": 8700
    },
    {
      "epoch": 7.295335703827651,
      "grad_norm": 25.44869613647461,
      "learning_rate": 1.9854864016736402e-05,
      "loss": 1.0641,
      "step": 8725
    },
    {
      "epoch": 7.316251830161054,
      "grad_norm": 29.942523956298828,
      "learning_rate": 1.9822175732217573e-05,
      "loss": 1.2148,
      "step": 8750
    },
    {
      "epoch": 7.337167956494457,
      "grad_norm": 37.400333404541016,
      "learning_rate": 1.9789487447698744e-05,
      "loss": 1.2733,
      "step": 8775
    },
    {
      "epoch": 7.35808408282786,
      "grad_norm": 31.384536743164062,
      "learning_rate": 1.975679916317992e-05,
      "loss": 1.3021,
      "step": 8800
    },
    {
      "epoch": 7.379000209161263,
      "grad_norm": 38.98894500732422,
      "learning_rate": 1.972411087866109e-05,
      "loss": 1.2135,
      "step": 8825
    },
    {
      "epoch": 7.399916335494666,
      "grad_norm": 37.74589920043945,
      "learning_rate": 1.969142259414226e-05,
      "loss": 1.2448,
      "step": 8850
    },
    {
      "epoch": 7.42083246182807,
      "grad_norm": 42.9708137512207,
      "learning_rate": 1.965873430962343e-05,
      "loss": 1.2395,
      "step": 8875
    },
    {
      "epoch": 7.441748588161473,
      "grad_norm": 37.89341735839844,
      "learning_rate": 1.96260460251046e-05,
      "loss": 1.2092,
      "step": 8900
    },
    {
      "epoch": 7.462664714494876,
      "grad_norm": 33.72842025756836,
      "learning_rate": 1.9593357740585775e-05,
      "loss": 1.2014,
      "step": 8925
    },
    {
      "epoch": 7.483580840828279,
      "grad_norm": 27.5230770111084,
      "learning_rate": 1.9560669456066946e-05,
      "loss": 1.2796,
      "step": 8950
    },
    {
      "epoch": 7.504496967161682,
      "grad_norm": 29.1596622467041,
      "learning_rate": 1.9527981171548117e-05,
      "loss": 1.1799,
      "step": 8975
    },
    {
      "epoch": 7.525413093495085,
      "grad_norm": 43.06945037841797,
      "learning_rate": 1.949529288702929e-05,
      "loss": 1.2803,
      "step": 9000
    },
    {
      "epoch": 7.546329219828488,
      "grad_norm": 23.593242645263672,
      "learning_rate": 1.9462604602510463e-05,
      "loss": 1.2035,
      "step": 9025
    },
    {
      "epoch": 7.567245346161891,
      "grad_norm": 22.913795471191406,
      "learning_rate": 1.943122384937239e-05,
      "loss": 1.322,
      "step": 9050
    },
    {
      "epoch": 7.588161472495294,
      "grad_norm": 33.54692840576172,
      "learning_rate": 1.9398535564853556e-05,
      "loss": 1.1686,
      "step": 9075
    },
    {
      "epoch": 7.609077598828697,
      "grad_norm": 30.73847007751465,
      "learning_rate": 1.9365847280334728e-05,
      "loss": 1.2026,
      "step": 9100
    },
    {
      "epoch": 7.6299937251621,
      "grad_norm": 40.774288177490234,
      "learning_rate": 1.93331589958159e-05,
      "loss": 1.166,
      "step": 9125
    },
    {
      "epoch": 7.650909851495503,
      "grad_norm": 34.480804443359375,
      "learning_rate": 1.930047071129707e-05,
      "loss": 1.1726,
      "step": 9150
    },
    {
      "epoch": 7.671825977828906,
      "grad_norm": 27.02526092529297,
      "learning_rate": 1.9267782426778245e-05,
      "loss": 1.1399,
      "step": 9175
    },
    {
      "epoch": 7.692742104162309,
      "grad_norm": 26.79431915283203,
      "learning_rate": 1.9235094142259416e-05,
      "loss": 1.247,
      "step": 9200
    },
    {
      "epoch": 7.713658230495712,
      "grad_norm": 28.210796356201172,
      "learning_rate": 1.9202405857740587e-05,
      "loss": 1.1745,
      "step": 9225
    },
    {
      "epoch": 7.734574356829115,
      "grad_norm": 35.597476959228516,
      "learning_rate": 1.9169717573221758e-05,
      "loss": 1.2009,
      "step": 9250
    },
    {
      "epoch": 7.755490483162518,
      "grad_norm": 35.62144088745117,
      "learning_rate": 1.913702928870293e-05,
      "loss": 1.1583,
      "step": 9275
    },
    {
      "epoch": 7.776406609495921,
      "grad_norm": 23.885515213012695,
      "learning_rate": 1.91043410041841e-05,
      "loss": 1.1602,
      "step": 9300
    },
    {
      "epoch": 7.797322735829324,
      "grad_norm": 29.345787048339844,
      "learning_rate": 1.907165271966527e-05,
      "loss": 1.1662,
      "step": 9325
    },
    {
      "epoch": 7.818238862162728,
      "grad_norm": 34.09832000732422,
      "learning_rate": 1.9038964435146443e-05,
      "loss": 1.1921,
      "step": 9350
    },
    {
      "epoch": 7.839154988496131,
      "grad_norm": 28.39063835144043,
      "learning_rate": 1.9006276150627614e-05,
      "loss": 1.3283,
      "step": 9375
    },
    {
      "epoch": 7.860071114829534,
      "grad_norm": 39.51025390625,
      "learning_rate": 1.897358786610879e-05,
      "loss": 1.2297,
      "step": 9400
    },
    {
      "epoch": 7.880987241162937,
      "grad_norm": 27.756425857543945,
      "learning_rate": 1.894089958158996e-05,
      "loss": 1.1741,
      "step": 9425
    },
    {
      "epoch": 7.90190336749634,
      "grad_norm": 27.306486129760742,
      "learning_rate": 1.890821129707113e-05,
      "loss": 1.1693,
      "step": 9450
    },
    {
      "epoch": 7.922819493829743,
      "grad_norm": 29.5734806060791,
      "learning_rate": 1.8875523012552302e-05,
      "loss": 1.2106,
      "step": 9475
    },
    {
      "epoch": 7.943735620163146,
      "grad_norm": 36.87259292602539,
      "learning_rate": 1.8842834728033473e-05,
      "loss": 1.2158,
      "step": 9500
    },
    {
      "epoch": 7.964651746496549,
      "grad_norm": 37.257869720458984,
      "learning_rate": 1.8810146443514644e-05,
      "loss": 1.2278,
      "step": 9525
    },
    {
      "epoch": 7.985567872829952,
      "grad_norm": 25.889978408813477,
      "learning_rate": 1.8777458158995815e-05,
      "loss": 1.1588,
      "step": 9550
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.4835962951183319,
      "eval_runtime": 8.6318,
      "eval_samples_per_second": 1969.454,
      "eval_steps_per_second": 61.632,
      "step": 9568
    },
    {
      "epoch": 8.005856515373353,
      "grad_norm": 29.453933715820312,
      "learning_rate": 1.8744769874476987e-05,
      "loss": 1.0466,
      "step": 9575
    },
    {
      "epoch": 8.026772641706756,
      "grad_norm": 32.83457946777344,
      "learning_rate": 1.871208158995816e-05,
      "loss": 1.0246,
      "step": 9600
    },
    {
      "epoch": 8.047688768040159,
      "grad_norm": 43.357391357421875,
      "learning_rate": 1.8679393305439332e-05,
      "loss": 1.0779,
      "step": 9625
    },
    {
      "epoch": 8.068604894373562,
      "grad_norm": 28.217405319213867,
      "learning_rate": 1.8646705020920503e-05,
      "loss": 1.108,
      "step": 9650
    },
    {
      "epoch": 8.089521020706965,
      "grad_norm": 29.34651756286621,
      "learning_rate": 1.8614016736401675e-05,
      "loss": 1.1298,
      "step": 9675
    },
    {
      "epoch": 8.110437147040368,
      "grad_norm": 31.26268196105957,
      "learning_rate": 1.8581328451882846e-05,
      "loss": 0.9884,
      "step": 9700
    },
    {
      "epoch": 8.13135327337377,
      "grad_norm": 34.70888900756836,
      "learning_rate": 1.8548640167364017e-05,
      "loss": 1.1435,
      "step": 9725
    },
    {
      "epoch": 8.152269399707174,
      "grad_norm": 38.931976318359375,
      "learning_rate": 1.8515951882845188e-05,
      "loss": 1.0603,
      "step": 9750
    },
    {
      "epoch": 8.173185526040577,
      "grad_norm": 27.78042984008789,
      "learning_rate": 1.848326359832636e-05,
      "loss": 1.0275,
      "step": 9775
    },
    {
      "epoch": 8.19410165237398,
      "grad_norm": 28.572925567626953,
      "learning_rate": 1.845057531380753e-05,
      "loss": 1.0149,
      "step": 9800
    },
    {
      "epoch": 8.215017778707383,
      "grad_norm": 26.19028663635254,
      "learning_rate": 1.8417887029288705e-05,
      "loss": 1.1173,
      "step": 9825
    },
    {
      "epoch": 8.235933905040786,
      "grad_norm": 34.55207824707031,
      "learning_rate": 1.8385198744769876e-05,
      "loss": 1.0842,
      "step": 9850
    },
    {
      "epoch": 8.256850031374189,
      "grad_norm": 39.22492599487305,
      "learning_rate": 1.8352510460251047e-05,
      "loss": 1.1222,
      "step": 9875
    },
    {
      "epoch": 8.277766157707592,
      "grad_norm": 30.992765426635742,
      "learning_rate": 1.831982217573222e-05,
      "loss": 1.1037,
      "step": 9900
    },
    {
      "epoch": 8.298682284040996,
      "grad_norm": 29.49632453918457,
      "learning_rate": 1.828713389121339e-05,
      "loss": 1.1466,
      "step": 9925
    },
    {
      "epoch": 8.319598410374399,
      "grad_norm": 26.51078224182129,
      "learning_rate": 1.825444560669456e-05,
      "loss": 1.1432,
      "step": 9950
    },
    {
      "epoch": 8.340514536707802,
      "grad_norm": 30.616634368896484,
      "learning_rate": 1.8221757322175732e-05,
      "loss": 1.1033,
      "step": 9975
    },
    {
      "epoch": 8.361430663041205,
      "grad_norm": 37.86799621582031,
      "learning_rate": 1.8189069037656903e-05,
      "loss": 1.1009,
      "step": 10000
    },
    {
      "epoch": 8.382346789374608,
      "grad_norm": 29.715530395507812,
      "learning_rate": 1.8156380753138074e-05,
      "loss": 1.0739,
      "step": 10025
    },
    {
      "epoch": 8.40326291570801,
      "grad_norm": 24.889860153198242,
      "learning_rate": 1.812369246861925e-05,
      "loss": 1.1836,
      "step": 10050
    },
    {
      "epoch": 8.424179042041414,
      "grad_norm": 36.40085983276367,
      "learning_rate": 1.809100418410042e-05,
      "loss": 1.0854,
      "step": 10075
    },
    {
      "epoch": 8.445095168374817,
      "grad_norm": 26.67049789428711,
      "learning_rate": 1.805831589958159e-05,
      "loss": 1.0688,
      "step": 10100
    },
    {
      "epoch": 8.46601129470822,
      "grad_norm": 28.993318557739258,
      "learning_rate": 1.8025627615062762e-05,
      "loss": 1.1397,
      "step": 10125
    },
    {
      "epoch": 8.486927421041623,
      "grad_norm": 21.256450653076172,
      "learning_rate": 1.7992939330543934e-05,
      "loss": 1.0196,
      "step": 10150
    },
    {
      "epoch": 8.507843547375026,
      "grad_norm": 51.14802169799805,
      "learning_rate": 1.7960251046025105e-05,
      "loss": 1.0563,
      "step": 10175
    },
    {
      "epoch": 8.528759673708429,
      "grad_norm": 37.81368637084961,
      "learning_rate": 1.7927562761506276e-05,
      "loss": 1.0692,
      "step": 10200
    },
    {
      "epoch": 8.549675800041832,
      "grad_norm": 30.85506820678711,
      "learning_rate": 1.7894874476987447e-05,
      "loss": 1.1298,
      "step": 10225
    },
    {
      "epoch": 8.570591926375235,
      "grad_norm": 28.291332244873047,
      "learning_rate": 1.7862186192468618e-05,
      "loss": 1.0564,
      "step": 10250
    },
    {
      "epoch": 8.591508052708638,
      "grad_norm": 26.972692489624023,
      "learning_rate": 1.7829497907949793e-05,
      "loss": 1.0236,
      "step": 10275
    },
    {
      "epoch": 8.61242417904204,
      "grad_norm": 31.890470504760742,
      "learning_rate": 1.7796809623430964e-05,
      "loss": 1.0193,
      "step": 10300
    },
    {
      "epoch": 8.633340305375445,
      "grad_norm": 28.512470245361328,
      "learning_rate": 1.7764121338912135e-05,
      "loss": 1.146,
      "step": 10325
    },
    {
      "epoch": 8.654256431708848,
      "grad_norm": 37.11262130737305,
      "learning_rate": 1.7731433054393306e-05,
      "loss": 1.0225,
      "step": 10350
    },
    {
      "epoch": 8.67517255804225,
      "grad_norm": 39.6890869140625,
      "learning_rate": 1.7698744769874477e-05,
      "loss": 1.1176,
      "step": 10375
    },
    {
      "epoch": 8.696088684375654,
      "grad_norm": 30.270496368408203,
      "learning_rate": 1.766605648535565e-05,
      "loss": 1.1484,
      "step": 10400
    },
    {
      "epoch": 8.717004810709057,
      "grad_norm": 30.108230590820312,
      "learning_rate": 1.763336820083682e-05,
      "loss": 1.0531,
      "step": 10425
    },
    {
      "epoch": 8.73792093704246,
      "grad_norm": 30.310230255126953,
      "learning_rate": 1.760067991631799e-05,
      "loss": 1.1234,
      "step": 10450
    },
    {
      "epoch": 8.758837063375863,
      "grad_norm": 27.635896682739258,
      "learning_rate": 1.7567991631799165e-05,
      "loss": 1.0093,
      "step": 10475
    },
    {
      "epoch": 8.779753189709266,
      "grad_norm": 37.08384704589844,
      "learning_rate": 1.7535303347280337e-05,
      "loss": 1.1289,
      "step": 10500
    },
    {
      "epoch": 8.800669316042669,
      "grad_norm": 29.990276336669922,
      "learning_rate": 1.7502615062761508e-05,
      "loss": 1.0553,
      "step": 10525
    },
    {
      "epoch": 8.821585442376072,
      "grad_norm": 39.01900863647461,
      "learning_rate": 1.746992677824268e-05,
      "loss": 1.051,
      "step": 10550
    },
    {
      "epoch": 8.842501568709475,
      "grad_norm": 30.835216522216797,
      "learning_rate": 1.7437238493723847e-05,
      "loss": 1.1768,
      "step": 10575
    },
    {
      "epoch": 8.863417695042878,
      "grad_norm": 32.992454528808594,
      "learning_rate": 1.740455020920502e-05,
      "loss": 1.1101,
      "step": 10600
    },
    {
      "epoch": 8.88433382137628,
      "grad_norm": 25.436925888061523,
      "learning_rate": 1.7371861924686192e-05,
      "loss": 1.0749,
      "step": 10625
    },
    {
      "epoch": 8.905249947709684,
      "grad_norm": 34.70539474487305,
      "learning_rate": 1.7339173640167364e-05,
      "loss": 1.065,
      "step": 10650
    },
    {
      "epoch": 8.926166074043087,
      "grad_norm": 27.670040130615234,
      "learning_rate": 1.7306485355648535e-05,
      "loss": 0.9639,
      "step": 10675
    },
    {
      "epoch": 8.94708220037649,
      "grad_norm": 47.982872009277344,
      "learning_rate": 1.727379707112971e-05,
      "loss": 1.1494,
      "step": 10700
    },
    {
      "epoch": 8.967998326709893,
      "grad_norm": 24.424331665039062,
      "learning_rate": 1.724110878661088e-05,
      "loss": 1.0733,
      "step": 10725
    },
    {
      "epoch": 8.988914453043297,
      "grad_norm": 31.385665893554688,
      "learning_rate": 1.720842050209205e-05,
      "loss": 1.0315,
      "step": 10750
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.4748515486717224,
      "eval_runtime": 8.5079,
      "eval_samples_per_second": 1998.135,
      "eval_steps_per_second": 62.53,
      "step": 10764
    },
    {
      "epoch": 9.009203095586697,
      "grad_norm": 28.163646697998047,
      "learning_rate": 1.7175732217573223e-05,
      "loss": 0.9844,
      "step": 10775
    },
    {
      "epoch": 9.030119221920101,
      "grad_norm": 25.219026565551758,
      "learning_rate": 1.7143043933054394e-05,
      "loss": 0.9321,
      "step": 10800
    },
    {
      "epoch": 9.051035348253503,
      "grad_norm": 36.17987823486328,
      "learning_rate": 1.7110355648535565e-05,
      "loss": 0.9686,
      "step": 10825
    },
    {
      "epoch": 9.071951474586907,
      "grad_norm": 25.17515754699707,
      "learning_rate": 1.7077667364016736e-05,
      "loss": 0.8765,
      "step": 10850
    },
    {
      "epoch": 9.092867600920309,
      "grad_norm": 26.52577781677246,
      "learning_rate": 1.7044979079497907e-05,
      "loss": 0.9307,
      "step": 10875
    },
    {
      "epoch": 9.113783727253713,
      "grad_norm": 26.384000778198242,
      "learning_rate": 1.701229079497908e-05,
      "loss": 0.9941,
      "step": 10900
    },
    {
      "epoch": 9.134699853587115,
      "grad_norm": 25.823806762695312,
      "learning_rate": 1.6979602510460253e-05,
      "loss": 1.0016,
      "step": 10925
    },
    {
      "epoch": 9.155615979920519,
      "grad_norm": 28.234495162963867,
      "learning_rate": 1.6946914225941424e-05,
      "loss": 0.9614,
      "step": 10950
    },
    {
      "epoch": 9.176532106253921,
      "grad_norm": 24.27943229675293,
      "learning_rate": 1.6914225941422595e-05,
      "loss": 0.9606,
      "step": 10975
    },
    {
      "epoch": 9.197448232587325,
      "grad_norm": 39.18170928955078,
      "learning_rate": 1.6881537656903763e-05,
      "loss": 0.9288,
      "step": 11000
    },
    {
      "epoch": 9.218364358920727,
      "grad_norm": 29.728851318359375,
      "learning_rate": 1.6848849372384938e-05,
      "loss": 0.9787,
      "step": 11025
    },
    {
      "epoch": 9.239280485254131,
      "grad_norm": 31.870159149169922,
      "learning_rate": 1.681616108786611e-05,
      "loss": 0.9742,
      "step": 11050
    },
    {
      "epoch": 9.260196611587533,
      "grad_norm": 31.624847412109375,
      "learning_rate": 1.678347280334728e-05,
      "loss": 0.9023,
      "step": 11075
    },
    {
      "epoch": 9.281112737920937,
      "grad_norm": 37.076438903808594,
      "learning_rate": 1.675078451882845e-05,
      "loss": 0.9229,
      "step": 11100
    },
    {
      "epoch": 9.302028864254341,
      "grad_norm": 26.00804901123047,
      "learning_rate": 1.6718096234309622e-05,
      "loss": 1.0158,
      "step": 11125
    },
    {
      "epoch": 9.322944990587743,
      "grad_norm": 32.9221076965332,
      "learning_rate": 1.6685407949790797e-05,
      "loss": 0.9306,
      "step": 11150
    },
    {
      "epoch": 9.343861116921147,
      "grad_norm": 30.759349822998047,
      "learning_rate": 1.6652719665271968e-05,
      "loss": 0.8686,
      "step": 11175
    },
    {
      "epoch": 9.364777243254549,
      "grad_norm": 36.57038497924805,
      "learning_rate": 1.662003138075314e-05,
      "loss": 0.9462,
      "step": 11200
    },
    {
      "epoch": 9.385693369587953,
      "grad_norm": 24.75457191467285,
      "learning_rate": 1.6587343096234307e-05,
      "loss": 0.982,
      "step": 11225
    },
    {
      "epoch": 9.406609495921355,
      "grad_norm": 27.20646858215332,
      "learning_rate": 1.6554654811715482e-05,
      "loss": 0.9382,
      "step": 11250
    },
    {
      "epoch": 9.427525622254759,
      "grad_norm": 40.57386016845703,
      "learning_rate": 1.6521966527196653e-05,
      "loss": 0.9802,
      "step": 11275
    },
    {
      "epoch": 9.448441748588161,
      "grad_norm": 27.447500228881836,
      "learning_rate": 1.6489278242677824e-05,
      "loss": 0.952,
      "step": 11300
    },
    {
      "epoch": 9.469357874921565,
      "grad_norm": 28.658401489257812,
      "learning_rate": 1.6456589958158995e-05,
      "loss": 0.9896,
      "step": 11325
    },
    {
      "epoch": 9.490274001254967,
      "grad_norm": 29.017562866210938,
      "learning_rate": 1.642390167364017e-05,
      "loss": 0.9342,
      "step": 11350
    },
    {
      "epoch": 9.511190127588371,
      "grad_norm": 30.381887435913086,
      "learning_rate": 1.639121338912134e-05,
      "loss": 0.9805,
      "step": 11375
    },
    {
      "epoch": 9.532106253921773,
      "grad_norm": 28.82288360595703,
      "learning_rate": 1.6358525104602512e-05,
      "loss": 1.0328,
      "step": 11400
    },
    {
      "epoch": 9.553022380255177,
      "grad_norm": 23.874649047851562,
      "learning_rate": 1.6325836820083683e-05,
      "loss": 1.0005,
      "step": 11425
    },
    {
      "epoch": 9.57393850658858,
      "grad_norm": 31.392045974731445,
      "learning_rate": 1.629314853556485e-05,
      "loss": 1.0205,
      "step": 11450
    },
    {
      "epoch": 9.594854632921983,
      "grad_norm": 31.88454246520996,
      "learning_rate": 1.6260460251046026e-05,
      "loss": 0.9793,
      "step": 11475
    },
    {
      "epoch": 9.615770759255385,
      "grad_norm": 31.65714454650879,
      "learning_rate": 1.6227771966527197e-05,
      "loss": 0.9898,
      "step": 11500
    },
    {
      "epoch": 9.636686885588789,
      "grad_norm": 30.85045623779297,
      "learning_rate": 1.6195083682008368e-05,
      "loss": 0.8843,
      "step": 11525
    },
    {
      "epoch": 9.657603011922191,
      "grad_norm": 30.552799224853516,
      "learning_rate": 1.616239539748954e-05,
      "loss": 1.0141,
      "step": 11550
    },
    {
      "epoch": 9.678519138255595,
      "grad_norm": 32.7514533996582,
      "learning_rate": 1.6129707112970714e-05,
      "loss": 0.9124,
      "step": 11575
    },
    {
      "epoch": 9.699435264588999,
      "grad_norm": 30.859394073486328,
      "learning_rate": 1.6097018828451885e-05,
      "loss": 0.913,
      "step": 11600
    },
    {
      "epoch": 9.720351390922401,
      "grad_norm": 25.007034301757812,
      "learning_rate": 1.6064330543933056e-05,
      "loss": 0.9523,
      "step": 11625
    },
    {
      "epoch": 9.741267517255805,
      "grad_norm": 28.92762565612793,
      "learning_rate": 1.6031642259414224e-05,
      "loss": 0.9536,
      "step": 11650
    },
    {
      "epoch": 9.762183643589207,
      "grad_norm": 24.083269119262695,
      "learning_rate": 1.5998953974895398e-05,
      "loss": 0.9891,
      "step": 11675
    },
    {
      "epoch": 9.783099769922611,
      "grad_norm": 28.367168426513672,
      "learning_rate": 1.596626569037657e-05,
      "loss": 1.009,
      "step": 11700
    },
    {
      "epoch": 9.804015896256013,
      "grad_norm": 32.86335754394531,
      "learning_rate": 1.593357740585774e-05,
      "loss": 0.9993,
      "step": 11725
    },
    {
      "epoch": 9.824932022589417,
      "grad_norm": 29.237857818603516,
      "learning_rate": 1.5900889121338912e-05,
      "loss": 0.993,
      "step": 11750
    },
    {
      "epoch": 9.84584814892282,
      "grad_norm": 28.30716323852539,
      "learning_rate": 1.5869508368200838e-05,
      "loss": 1.0415,
      "step": 11775
    },
    {
      "epoch": 9.866764275256223,
      "grad_norm": 25.100419998168945,
      "learning_rate": 1.583682008368201e-05,
      "loss": 0.9055,
      "step": 11800
    },
    {
      "epoch": 9.887680401589625,
      "grad_norm": 23.540630340576172,
      "learning_rate": 1.5804131799163183e-05,
      "loss": 0.9556,
      "step": 11825
    },
    {
      "epoch": 9.908596527923029,
      "grad_norm": 35.531028747558594,
      "learning_rate": 1.577144351464435e-05,
      "loss": 0.9079,
      "step": 11850
    },
    {
      "epoch": 9.929512654256431,
      "grad_norm": 34.867340087890625,
      "learning_rate": 1.5738755230125522e-05,
      "loss": 0.9678,
      "step": 11875
    },
    {
      "epoch": 9.950428780589835,
      "grad_norm": 30.93507957458496,
      "learning_rate": 1.5706066945606693e-05,
      "loss": 0.9804,
      "step": 11900
    },
    {
      "epoch": 9.971344906923237,
      "grad_norm": 18.221176147460938,
      "learning_rate": 1.5673378661087865e-05,
      "loss": 0.9406,
      "step": 11925
    },
    {
      "epoch": 9.992261033256641,
      "grad_norm": 28.23503875732422,
      "learning_rate": 1.564069037656904e-05,
      "loss": 1.0289,
      "step": 11950
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.4548487663269043,
      "eval_runtime": 8.4338,
      "eval_samples_per_second": 2015.709,
      "eval_steps_per_second": 63.08,
      "step": 11960
    },
    {
      "epoch": 10.012549675800042,
      "grad_norm": 24.473114013671875,
      "learning_rate": 1.560800209205021e-05,
      "loss": 0.9349,
      "step": 11975
    },
    {
      "epoch": 10.033465802133446,
      "grad_norm": 26.133533477783203,
      "learning_rate": 1.557531380753138e-05,
      "loss": 0.8442,
      "step": 12000
    },
    {
      "epoch": 10.054381928466848,
      "grad_norm": 29.662405014038086,
      "learning_rate": 1.5542625523012553e-05,
      "loss": 0.9024,
      "step": 12025
    },
    {
      "epoch": 10.075298054800252,
      "grad_norm": 32.34764862060547,
      "learning_rate": 1.5509937238493727e-05,
      "loss": 0.9275,
      "step": 12050
    },
    {
      "epoch": 10.096214181133654,
      "grad_norm": 18.24309539794922,
      "learning_rate": 1.5477248953974895e-05,
      "loss": 0.8773,
      "step": 12075
    },
    {
      "epoch": 10.117130307467058,
      "grad_norm": 28.372343063354492,
      "learning_rate": 1.5444560669456066e-05,
      "loss": 0.9343,
      "step": 12100
    },
    {
      "epoch": 10.13804643380046,
      "grad_norm": 26.447050094604492,
      "learning_rate": 1.5411872384937237e-05,
      "loss": 0.8364,
      "step": 12125
    },
    {
      "epoch": 10.158962560133864,
      "grad_norm": 23.682348251342773,
      "learning_rate": 1.5379184100418412e-05,
      "loss": 0.9005,
      "step": 12150
    },
    {
      "epoch": 10.179878686467266,
      "grad_norm": 30.68705177307129,
      "learning_rate": 1.5346495815899583e-05,
      "loss": 0.8115,
      "step": 12175
    },
    {
      "epoch": 10.20079481280067,
      "grad_norm": 23.688114166259766,
      "learning_rate": 1.5313807531380754e-05,
      "loss": 0.8444,
      "step": 12200
    },
    {
      "epoch": 10.221710939134072,
      "grad_norm": 24.71125030517578,
      "learning_rate": 1.5281119246861925e-05,
      "loss": 0.9078,
      "step": 12225
    },
    {
      "epoch": 10.242627065467476,
      "grad_norm": 36.334617614746094,
      "learning_rate": 1.5248430962343095e-05,
      "loss": 0.9739,
      "step": 12250
    },
    {
      "epoch": 10.263543191800878,
      "grad_norm": 30.986770629882812,
      "learning_rate": 1.521574267782427e-05,
      "loss": 0.8104,
      "step": 12275
    },
    {
      "epoch": 10.284459318134282,
      "grad_norm": 29.654666900634766,
      "learning_rate": 1.518305439330544e-05,
      "loss": 0.8658,
      "step": 12300
    },
    {
      "epoch": 10.305375444467685,
      "grad_norm": 27.860382080078125,
      "learning_rate": 1.515036610878661e-05,
      "loss": 0.8037,
      "step": 12325
    },
    {
      "epoch": 10.326291570801088,
      "grad_norm": 39.158607482910156,
      "learning_rate": 1.5117677824267781e-05,
      "loss": 0.9003,
      "step": 12350
    },
    {
      "epoch": 10.347207697134492,
      "grad_norm": 22.94873809814453,
      "learning_rate": 1.5084989539748956e-05,
      "loss": 0.8337,
      "step": 12375
    },
    {
      "epoch": 10.368123823467894,
      "grad_norm": 19.698932647705078,
      "learning_rate": 1.5052301255230127e-05,
      "loss": 0.9418,
      "step": 12400
    },
    {
      "epoch": 10.389039949801298,
      "grad_norm": 20.89352798461914,
      "learning_rate": 1.5019612970711298e-05,
      "loss": 0.9715,
      "step": 12425
    },
    {
      "epoch": 10.4099560761347,
      "grad_norm": 41.261192321777344,
      "learning_rate": 1.498692468619247e-05,
      "loss": 0.8675,
      "step": 12450
    },
    {
      "epoch": 10.430872202468104,
      "grad_norm": 31.413349151611328,
      "learning_rate": 1.495423640167364e-05,
      "loss": 0.948,
      "step": 12475
    },
    {
      "epoch": 10.451788328801506,
      "grad_norm": 18.52796173095703,
      "learning_rate": 1.4921548117154812e-05,
      "loss": 0.8591,
      "step": 12500
    },
    {
      "epoch": 10.47270445513491,
      "grad_norm": 24.735828399658203,
      "learning_rate": 1.4888859832635984e-05,
      "loss": 0.8112,
      "step": 12525
    },
    {
      "epoch": 10.493620581468312,
      "grad_norm": 29.56083106994629,
      "learning_rate": 1.4856171548117154e-05,
      "loss": 0.8664,
      "step": 12550
    },
    {
      "epoch": 10.514536707801716,
      "grad_norm": 30.147531509399414,
      "learning_rate": 1.4823483263598327e-05,
      "loss": 0.9338,
      "step": 12575
    },
    {
      "epoch": 10.535452834135118,
      "grad_norm": 23.033565521240234,
      "learning_rate": 1.4790794979079498e-05,
      "loss": 0.9484,
      "step": 12600
    },
    {
      "epoch": 10.556368960468522,
      "grad_norm": 26.897329330444336,
      "learning_rate": 1.475810669456067e-05,
      "loss": 0.8962,
      "step": 12625
    },
    {
      "epoch": 10.577285086801924,
      "grad_norm": 21.438114166259766,
      "learning_rate": 1.472541841004184e-05,
      "loss": 0.885,
      "step": 12650
    },
    {
      "epoch": 10.598201213135328,
      "grad_norm": 24.369186401367188,
      "learning_rate": 1.4692730125523013e-05,
      "loss": 0.8738,
      "step": 12675
    },
    {
      "epoch": 10.61911733946873,
      "grad_norm": 31.91090965270996,
      "learning_rate": 1.4660041841004184e-05,
      "loss": 0.8757,
      "step": 12700
    },
    {
      "epoch": 10.640033465802134,
      "grad_norm": 28.872577667236328,
      "learning_rate": 1.4627353556485357e-05,
      "loss": 0.9193,
      "step": 12725
    },
    {
      "epoch": 10.660949592135536,
      "grad_norm": 19.57573890686035,
      "learning_rate": 1.4594665271966528e-05,
      "loss": 0.8981,
      "step": 12750
    },
    {
      "epoch": 10.68186571846894,
      "grad_norm": 26.059322357177734,
      "learning_rate": 1.45619769874477e-05,
      "loss": 0.8541,
      "step": 12775
    },
    {
      "epoch": 10.702781844802342,
      "grad_norm": 32.76042556762695,
      "learning_rate": 1.452928870292887e-05,
      "loss": 0.8221,
      "step": 12800
    },
    {
      "epoch": 10.723697971135746,
      "grad_norm": 29.393171310424805,
      "learning_rate": 1.4496600418410042e-05,
      "loss": 0.8744,
      "step": 12825
    },
    {
      "epoch": 10.74461409746915,
      "grad_norm": 28.306501388549805,
      "learning_rate": 1.4463912133891215e-05,
      "loss": 0.9104,
      "step": 12850
    },
    {
      "epoch": 10.765530223802552,
      "grad_norm": 27.17999839782715,
      "learning_rate": 1.4431223849372384e-05,
      "loss": 0.7979,
      "step": 12875
    },
    {
      "epoch": 10.786446350135956,
      "grad_norm": 28.005990982055664,
      "learning_rate": 1.4398535564853557e-05,
      "loss": 0.9228,
      "step": 12900
    },
    {
      "epoch": 10.807362476469358,
      "grad_norm": 40.00999069213867,
      "learning_rate": 1.4365847280334728e-05,
      "loss": 0.966,
      "step": 12925
    },
    {
      "epoch": 10.828278602802762,
      "grad_norm": 27.873910903930664,
      "learning_rate": 1.4333158995815901e-05,
      "loss": 0.9986,
      "step": 12950
    },
    {
      "epoch": 10.849194729136164,
      "grad_norm": 25.920825958251953,
      "learning_rate": 1.430047071129707e-05,
      "loss": 0.8476,
      "step": 12975
    },
    {
      "epoch": 10.870110855469568,
      "grad_norm": 30.151338577270508,
      "learning_rate": 1.4267782426778243e-05,
      "loss": 0.8677,
      "step": 13000
    },
    {
      "epoch": 10.89102698180297,
      "grad_norm": 20.783164978027344,
      "learning_rate": 1.4235094142259414e-05,
      "loss": 0.8762,
      "step": 13025
    },
    {
      "epoch": 10.911943108136374,
      "grad_norm": 27.987014770507812,
      "learning_rate": 1.4202405857740587e-05,
      "loss": 0.7595,
      "step": 13050
    },
    {
      "epoch": 10.932859234469776,
      "grad_norm": 28.821107864379883,
      "learning_rate": 1.4169717573221758e-05,
      "loss": 0.8689,
      "step": 13075
    },
    {
      "epoch": 10.95377536080318,
      "grad_norm": 25.22879981994629,
      "learning_rate": 1.4137029288702928e-05,
      "loss": 0.8865,
      "step": 13100
    },
    {
      "epoch": 10.974691487136582,
      "grad_norm": 33.975101470947266,
      "learning_rate": 1.41043410041841e-05,
      "loss": 0.8458,
      "step": 13125
    },
    {
      "epoch": 10.995607613469986,
      "grad_norm": 31.600444793701172,
      "learning_rate": 1.4071652719665272e-05,
      "loss": 0.8227,
      "step": 13150
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.4537860155105591,
      "eval_runtime": 8.4637,
      "eval_samples_per_second": 2008.586,
      "eval_steps_per_second": 62.857,
      "step": 13156
    },
    {
      "epoch": 11.015896256013386,
      "grad_norm": 32.57483673095703,
      "learning_rate": 1.4038964435146445e-05,
      "loss": 0.7391,
      "step": 13175
    },
    {
      "epoch": 11.03681238234679,
      "grad_norm": 26.53407096862793,
      "learning_rate": 1.4006276150627614e-05,
      "loss": 0.8541,
      "step": 13200
    },
    {
      "epoch": 11.057728508680192,
      "grad_norm": 27.191707611083984,
      "learning_rate": 1.3973587866108787e-05,
      "loss": 0.8224,
      "step": 13225
    },
    {
      "epoch": 11.078644635013596,
      "grad_norm": 29.95233726501465,
      "learning_rate": 1.3940899581589958e-05,
      "loss": 0.8527,
      "step": 13250
    },
    {
      "epoch": 11.099560761346998,
      "grad_norm": 29.84902572631836,
      "learning_rate": 1.3908211297071131e-05,
      "loss": 0.7857,
      "step": 13275
    },
    {
      "epoch": 11.120476887680402,
      "grad_norm": 24.362794876098633,
      "learning_rate": 1.38755230125523e-05,
      "loss": 0.8313,
      "step": 13300
    },
    {
      "epoch": 11.141393014013804,
      "grad_norm": 32.20846176147461,
      "learning_rate": 1.3842834728033474e-05,
      "loss": 0.8267,
      "step": 13325
    },
    {
      "epoch": 11.162309140347208,
      "grad_norm": 31.42409896850586,
      "learning_rate": 1.3810146443514645e-05,
      "loss": 0.77,
      "step": 13350
    },
    {
      "epoch": 11.18322526668061,
      "grad_norm": 23.54171371459961,
      "learning_rate": 1.3777458158995818e-05,
      "loss": 0.8348,
      "step": 13375
    },
    {
      "epoch": 11.204141393014014,
      "grad_norm": 18.438703536987305,
      "learning_rate": 1.3744769874476989e-05,
      "loss": 0.8489,
      "step": 13400
    },
    {
      "epoch": 11.225057519347416,
      "grad_norm": 27.579818725585938,
      "learning_rate": 1.3712081589958158e-05,
      "loss": 0.8131,
      "step": 13425
    },
    {
      "epoch": 11.24597364568082,
      "grad_norm": 25.06566047668457,
      "learning_rate": 1.3679393305439331e-05,
      "loss": 0.8189,
      "step": 13450
    },
    {
      "epoch": 11.266889772014222,
      "grad_norm": 21.15331268310547,
      "learning_rate": 1.3646705020920502e-05,
      "loss": 0.8243,
      "step": 13475
    },
    {
      "epoch": 11.287805898347626,
      "grad_norm": 22.256250381469727,
      "learning_rate": 1.3614016736401675e-05,
      "loss": 0.7921,
      "step": 13500
    },
    {
      "epoch": 11.308722024681028,
      "grad_norm": 30.951889038085938,
      "learning_rate": 1.3581328451882845e-05,
      "loss": 0.855,
      "step": 13525
    },
    {
      "epoch": 11.329638151014432,
      "grad_norm": 27.99029541015625,
      "learning_rate": 1.3548640167364017e-05,
      "loss": 0.8256,
      "step": 13550
    },
    {
      "epoch": 11.350554277347836,
      "grad_norm": 27.866348266601562,
      "learning_rate": 1.3515951882845189e-05,
      "loss": 0.8343,
      "step": 13575
    },
    {
      "epoch": 11.371470403681238,
      "grad_norm": 23.308183670043945,
      "learning_rate": 1.3483263598326361e-05,
      "loss": 0.756,
      "step": 13600
    },
    {
      "epoch": 11.392386530014642,
      "grad_norm": 32.25267028808594,
      "learning_rate": 1.3450575313807531e-05,
      "loss": 0.7864,
      "step": 13625
    },
    {
      "epoch": 11.413302656348044,
      "grad_norm": 25.673381805419922,
      "learning_rate": 1.3417887029288704e-05,
      "loss": 0.7711,
      "step": 13650
    },
    {
      "epoch": 11.434218782681448,
      "grad_norm": 18.544843673706055,
      "learning_rate": 1.3385198744769875e-05,
      "loss": 0.7796,
      "step": 13675
    },
    {
      "epoch": 11.45513490901485,
      "grad_norm": 23.525890350341797,
      "learning_rate": 1.3352510460251046e-05,
      "loss": 0.7453,
      "step": 13700
    },
    {
      "epoch": 11.476051035348254,
      "grad_norm": 25.436019897460938,
      "learning_rate": 1.3319822175732217e-05,
      "loss": 0.8036,
      "step": 13725
    },
    {
      "epoch": 11.496967161681656,
      "grad_norm": 34.54426193237305,
      "learning_rate": 1.3287133891213388e-05,
      "loss": 0.8103,
      "step": 13750
    },
    {
      "epoch": 11.51788328801506,
      "grad_norm": 25.448936462402344,
      "learning_rate": 1.3254445606694561e-05,
      "loss": 0.7875,
      "step": 13775
    },
    {
      "epoch": 11.538799414348462,
      "grad_norm": 34.74794006347656,
      "learning_rate": 1.3221757322175732e-05,
      "loss": 0.8129,
      "step": 13800
    },
    {
      "epoch": 11.559715540681866,
      "grad_norm": 28.694915771484375,
      "learning_rate": 1.3189069037656905e-05,
      "loss": 0.7948,
      "step": 13825
    },
    {
      "epoch": 11.580631667015268,
      "grad_norm": 29.570541381835938,
      "learning_rate": 1.3156380753138075e-05,
      "loss": 0.7964,
      "step": 13850
    },
    {
      "epoch": 11.601547793348672,
      "grad_norm": 24.52883529663086,
      "learning_rate": 1.3125e-05,
      "loss": 0.7779,
      "step": 13875
    },
    {
      "epoch": 11.622463919682074,
      "grad_norm": 29.154817581176758,
      "learning_rate": 1.3092311715481172e-05,
      "loss": 0.8822,
      "step": 13900
    },
    {
      "epoch": 11.643380046015478,
      "grad_norm": 30.68929100036621,
      "learning_rate": 1.3059623430962343e-05,
      "loss": 0.816,
      "step": 13925
    },
    {
      "epoch": 11.66429617234888,
      "grad_norm": 34.40254592895508,
      "learning_rate": 1.3026935146443514e-05,
      "loss": 0.8655,
      "step": 13950
    },
    {
      "epoch": 11.685212298682284,
      "grad_norm": 27.014209747314453,
      "learning_rate": 1.2994246861924687e-05,
      "loss": 0.8618,
      "step": 13975
    },
    {
      "epoch": 11.706128425015686,
      "grad_norm": 26.43442153930664,
      "learning_rate": 1.2961558577405858e-05,
      "loss": 0.7549,
      "step": 14000
    },
    {
      "epoch": 11.72704455134909,
      "grad_norm": 25.28746795654297,
      "learning_rate": 1.292887029288703e-05,
      "loss": 0.7753,
      "step": 14025
    },
    {
      "epoch": 11.747960677682492,
      "grad_norm": 24.08802604675293,
      "learning_rate": 1.28961820083682e-05,
      "loss": 0.8333,
      "step": 14050
    },
    {
      "epoch": 11.768876804015896,
      "grad_norm": 23.68677520751953,
      "learning_rate": 1.2863493723849373e-05,
      "loss": 0.874,
      "step": 14075
    },
    {
      "epoch": 11.7897929303493,
      "grad_norm": 25.837617874145508,
      "learning_rate": 1.2830805439330544e-05,
      "loss": 0.8405,
      "step": 14100
    },
    {
      "epoch": 11.810709056682702,
      "grad_norm": 25.853883743286133,
      "learning_rate": 1.2798117154811717e-05,
      "loss": 0.7769,
      "step": 14125
    },
    {
      "epoch": 11.831625183016106,
      "grad_norm": 24.32193946838379,
      "learning_rate": 1.2765428870292887e-05,
      "loss": 0.9512,
      "step": 14150
    },
    {
      "epoch": 11.852541309349508,
      "grad_norm": 34.21247482299805,
      "learning_rate": 1.2732740585774058e-05,
      "loss": 0.8139,
      "step": 14175
    },
    {
      "epoch": 11.873457435682912,
      "grad_norm": 25.0959529876709,
      "learning_rate": 1.270005230125523e-05,
      "loss": 0.8443,
      "step": 14200
    },
    {
      "epoch": 11.894373562016314,
      "grad_norm": 29.364309310913086,
      "learning_rate": 1.2667364016736402e-05,
      "loss": 0.7202,
      "step": 14225
    },
    {
      "epoch": 11.915289688349718,
      "grad_norm": 23.815967559814453,
      "learning_rate": 1.2634675732217573e-05,
      "loss": 0.824,
      "step": 14250
    },
    {
      "epoch": 11.93620581468312,
      "grad_norm": 35.2075309753418,
      "learning_rate": 1.2601987447698744e-05,
      "loss": 0.8253,
      "step": 14275
    },
    {
      "epoch": 11.957121941016524,
      "grad_norm": 29.228105545043945,
      "learning_rate": 1.2569299163179917e-05,
      "loss": 0.8774,
      "step": 14300
    },
    {
      "epoch": 11.978038067349926,
      "grad_norm": 35.3935432434082,
      "learning_rate": 1.2536610878661088e-05,
      "loss": 0.8846,
      "step": 14325
    },
    {
      "epoch": 11.99895419368333,
      "grad_norm": 30.781940460205078,
      "learning_rate": 1.250392259414226e-05,
      "loss": 0.8147,
      "step": 14350
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.4422646760940552,
      "eval_runtime": 9.0768,
      "eval_samples_per_second": 1872.9,
      "eval_steps_per_second": 58.611,
      "step": 14352
    },
    {
      "epoch": 12.01924283622673,
      "grad_norm": 26.93894386291504,
      "learning_rate": 1.247123430962343e-05,
      "loss": 0.6773,
      "step": 14375
    },
    {
      "epoch": 12.040158962560135,
      "grad_norm": 33.50507736206055,
      "learning_rate": 1.2438546025104603e-05,
      "loss": 0.6991,
      "step": 14400
    },
    {
      "epoch": 12.061075088893537,
      "grad_norm": 25.864303588867188,
      "learning_rate": 1.2405857740585775e-05,
      "loss": 0.7091,
      "step": 14425
    },
    {
      "epoch": 12.08199121522694,
      "grad_norm": 28.438940048217773,
      "learning_rate": 1.2373169456066947e-05,
      "loss": 0.7756,
      "step": 14450
    },
    {
      "epoch": 12.102907341560343,
      "grad_norm": 25.454307556152344,
      "learning_rate": 1.2340481171548117e-05,
      "loss": 0.7402,
      "step": 14475
    },
    {
      "epoch": 12.123823467893747,
      "grad_norm": 24.831375122070312,
      "learning_rate": 1.2307792887029288e-05,
      "loss": 0.747,
      "step": 14500
    },
    {
      "epoch": 12.144739594227149,
      "grad_norm": 20.581087112426758,
      "learning_rate": 1.2275104602510461e-05,
      "loss": 0.7846,
      "step": 14525
    },
    {
      "epoch": 12.165655720560553,
      "grad_norm": 23.783151626586914,
      "learning_rate": 1.2242416317991632e-05,
      "loss": 0.6709,
      "step": 14550
    },
    {
      "epoch": 12.186571846893955,
      "grad_norm": 27.903949737548828,
      "learning_rate": 1.2209728033472803e-05,
      "loss": 0.7079,
      "step": 14575
    },
    {
      "epoch": 12.207487973227359,
      "grad_norm": 18.598743438720703,
      "learning_rate": 1.2177039748953974e-05,
      "loss": 0.6596,
      "step": 14600
    },
    {
      "epoch": 12.22840409956076,
      "grad_norm": 26.982807159423828,
      "learning_rate": 1.2144351464435147e-05,
      "loss": 0.7578,
      "step": 14625
    },
    {
      "epoch": 12.249320225894165,
      "grad_norm": 30.640769958496094,
      "learning_rate": 1.2111663179916319e-05,
      "loss": 0.7966,
      "step": 14650
    },
    {
      "epoch": 12.270236352227567,
      "grad_norm": 22.543025970458984,
      "learning_rate": 1.207897489539749e-05,
      "loss": 0.746,
      "step": 14675
    },
    {
      "epoch": 12.29115247856097,
      "grad_norm": 35.266502380371094,
      "learning_rate": 1.204628661087866e-05,
      "loss": 0.7483,
      "step": 14700
    },
    {
      "epoch": 12.312068604894373,
      "grad_norm": 16.88680076599121,
      "learning_rate": 1.2013598326359834e-05,
      "loss": 0.7809,
      "step": 14725
    },
    {
      "epoch": 12.332984731227777,
      "grad_norm": 30.122604370117188,
      "learning_rate": 1.1980910041841005e-05,
      "loss": 0.8817,
      "step": 14750
    },
    {
      "epoch": 12.353900857561179,
      "grad_norm": 24.756654739379883,
      "learning_rate": 1.1948221757322176e-05,
      "loss": 0.8136,
      "step": 14775
    },
    {
      "epoch": 12.374816983894583,
      "grad_norm": 21.215314865112305,
      "learning_rate": 1.1915533472803347e-05,
      "loss": 0.7599,
      "step": 14800
    },
    {
      "epoch": 12.395733110227987,
      "grad_norm": 32.09524917602539,
      "learning_rate": 1.1882845188284518e-05,
      "loss": 0.7058,
      "step": 14825
    },
    {
      "epoch": 12.416649236561389,
      "grad_norm": 18.986867904663086,
      "learning_rate": 1.1850156903765691e-05,
      "loss": 0.7921,
      "step": 14850
    },
    {
      "epoch": 12.437565362894793,
      "grad_norm": 12.5482177734375,
      "learning_rate": 1.1817468619246862e-05,
      "loss": 0.7388,
      "step": 14875
    },
    {
      "epoch": 12.458481489228195,
      "grad_norm": 28.265628814697266,
      "learning_rate": 1.1784780334728034e-05,
      "loss": 0.8223,
      "step": 14900
    },
    {
      "epoch": 12.479397615561599,
      "grad_norm": 20.281322479248047,
      "learning_rate": 1.1752092050209205e-05,
      "loss": 0.7876,
      "step": 14925
    },
    {
      "epoch": 12.500313741895,
      "grad_norm": 20.328754425048828,
      "learning_rate": 1.1719403765690378e-05,
      "loss": 0.7929,
      "step": 14950
    },
    {
      "epoch": 12.521229868228405,
      "grad_norm": 19.570451736450195,
      "learning_rate": 1.1686715481171549e-05,
      "loss": 0.7853,
      "step": 14975
    },
    {
      "epoch": 12.542145994561807,
      "grad_norm": 24.785968780517578,
      "learning_rate": 1.165402719665272e-05,
      "loss": 0.8239,
      "step": 15000
    },
    {
      "epoch": 12.56306212089521,
      "grad_norm": 25.525222778320312,
      "learning_rate": 1.1621338912133891e-05,
      "loss": 0.6847,
      "step": 15025
    },
    {
      "epoch": 12.583978247228613,
      "grad_norm": 20.253490447998047,
      "learning_rate": 1.1588650627615064e-05,
      "loss": 0.8068,
      "step": 15050
    },
    {
      "epoch": 12.604894373562017,
      "grad_norm": 22.156850814819336,
      "learning_rate": 1.1555962343096235e-05,
      "loss": 0.7443,
      "step": 15075
    },
    {
      "epoch": 12.625810499895419,
      "grad_norm": 23.581954956054688,
      "learning_rate": 1.1523274058577406e-05,
      "loss": 0.6757,
      "step": 15100
    },
    {
      "epoch": 12.646726626228823,
      "grad_norm": 28.418109893798828,
      "learning_rate": 1.1490585774058577e-05,
      "loss": 0.7256,
      "step": 15125
    },
    {
      "epoch": 12.667642752562225,
      "grad_norm": 25.36321449279785,
      "learning_rate": 1.1457897489539749e-05,
      "loss": 0.7592,
      "step": 15150
    },
    {
      "epoch": 12.688558878895629,
      "grad_norm": 12.866950988769531,
      "learning_rate": 1.1425209205020921e-05,
      "loss": 0.7707,
      "step": 15175
    },
    {
      "epoch": 12.70947500522903,
      "grad_norm": 27.154953002929688,
      "learning_rate": 1.1392520920502093e-05,
      "loss": 0.7829,
      "step": 15200
    },
    {
      "epoch": 12.730391131562435,
      "grad_norm": 29.562671661376953,
      "learning_rate": 1.1359832635983264e-05,
      "loss": 0.7196,
      "step": 15225
    },
    {
      "epoch": 12.751307257895839,
      "grad_norm": 31.66153907775879,
      "learning_rate": 1.1327144351464435e-05,
      "loss": 0.7939,
      "step": 15250
    },
    {
      "epoch": 12.77222338422924,
      "grad_norm": 25.170495986938477,
      "learning_rate": 1.1294456066945608e-05,
      "loss": 0.7345,
      "step": 15275
    },
    {
      "epoch": 12.793139510562643,
      "grad_norm": 25.599328994750977,
      "learning_rate": 1.1261767782426779e-05,
      "loss": 0.7611,
      "step": 15300
    },
    {
      "epoch": 12.814055636896047,
      "grad_norm": 22.26190948486328,
      "learning_rate": 1.122907949790795e-05,
      "loss": 0.7725,
      "step": 15325
    },
    {
      "epoch": 12.83497176322945,
      "grad_norm": 25.67167854309082,
      "learning_rate": 1.1196391213389121e-05,
      "loss": 0.7241,
      "step": 15350
    },
    {
      "epoch": 12.855887889562853,
      "grad_norm": 26.67754554748535,
      "learning_rate": 1.1163702928870292e-05,
      "loss": 0.7609,
      "step": 15375
    },
    {
      "epoch": 12.876804015896257,
      "grad_norm": 15.589390754699707,
      "learning_rate": 1.1131014644351465e-05,
      "loss": 0.7884,
      "step": 15400
    },
    {
      "epoch": 12.897720142229659,
      "grad_norm": 25.950956344604492,
      "learning_rate": 1.1098326359832636e-05,
      "loss": 0.7171,
      "step": 15425
    },
    {
      "epoch": 12.918636268563063,
      "grad_norm": 29.312423706054688,
      "learning_rate": 1.1065638075313808e-05,
      "loss": 0.6882,
      "step": 15450
    },
    {
      "epoch": 12.939552394896465,
      "grad_norm": 34.489871978759766,
      "learning_rate": 1.1032949790794979e-05,
      "loss": 0.7696,
      "step": 15475
    },
    {
      "epoch": 12.960468521229869,
      "grad_norm": 26.63931655883789,
      "learning_rate": 1.1000261506276152e-05,
      "loss": 0.7801,
      "step": 15500
    },
    {
      "epoch": 12.98138464756327,
      "grad_norm": 25.03589630126953,
      "learning_rate": 1.0967573221757323e-05,
      "loss": 0.7752,
      "step": 15525
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.46095675230026245,
      "eval_runtime": 8.5967,
      "eval_samples_per_second": 1977.504,
      "eval_steps_per_second": 61.884,
      "step": 15548
    },
    {
      "epoch": 13.001673290106671,
      "grad_norm": 15.909531593322754,
      "learning_rate": 1.0934884937238494e-05,
      "loss": 0.6396,
      "step": 15550
    },
    {
      "epoch": 13.022589416440075,
      "grad_norm": 23.304765701293945,
      "learning_rate": 1.0902196652719665e-05,
      "loss": 0.7092,
      "step": 15575
    },
    {
      "epoch": 13.04350554277348,
      "grad_norm": 25.043785095214844,
      "learning_rate": 1.0869508368200838e-05,
      "loss": 0.7368,
      "step": 15600
    },
    {
      "epoch": 13.064421669106881,
      "grad_norm": 23.775224685668945,
      "learning_rate": 1.083682008368201e-05,
      "loss": 0.7475,
      "step": 15625
    },
    {
      "epoch": 13.085337795440285,
      "grad_norm": 26.90199851989746,
      "learning_rate": 1.0804131799163179e-05,
      "loss": 0.6743,
      "step": 15650
    },
    {
      "epoch": 13.106253921773687,
      "grad_norm": 30.666006088256836,
      "learning_rate": 1.0771443514644352e-05,
      "loss": 0.7116,
      "step": 15675
    },
    {
      "epoch": 13.127170048107091,
      "grad_norm": 29.87236785888672,
      "learning_rate": 1.0738755230125523e-05,
      "loss": 0.7462,
      "step": 15700
    },
    {
      "epoch": 13.148086174440493,
      "grad_norm": 30.70296859741211,
      "learning_rate": 1.0706066945606696e-05,
      "loss": 0.6763,
      "step": 15725
    },
    {
      "epoch": 13.169002300773897,
      "grad_norm": 24.461618423461914,
      "learning_rate": 1.0673378661087867e-05,
      "loss": 0.7293,
      "step": 15750
    },
    {
      "epoch": 13.1899184271073,
      "grad_norm": 25.5599422454834,
      "learning_rate": 1.0640690376569038e-05,
      "loss": 0.6434,
      "step": 15775
    },
    {
      "epoch": 13.210834553440703,
      "grad_norm": 30.434484481811523,
      "learning_rate": 1.0608002092050209e-05,
      "loss": 0.6198,
      "step": 15800
    },
    {
      "epoch": 13.231750679774105,
      "grad_norm": 24.265323638916016,
      "learning_rate": 1.0575313807531382e-05,
      "loss": 0.7185,
      "step": 15825
    },
    {
      "epoch": 13.25266680610751,
      "grad_norm": 25.89689064025879,
      "learning_rate": 1.0542625523012553e-05,
      "loss": 0.7018,
      "step": 15850
    },
    {
      "epoch": 13.273582932440911,
      "grad_norm": 27.88167381286621,
      "learning_rate": 1.0509937238493724e-05,
      "loss": 0.6736,
      "step": 15875
    },
    {
      "epoch": 13.294499058774315,
      "grad_norm": 22.964202880859375,
      "learning_rate": 1.0477248953974895e-05,
      "loss": 0.697,
      "step": 15900
    },
    {
      "epoch": 13.315415185107717,
      "grad_norm": 21.226186752319336,
      "learning_rate": 1.0444560669456068e-05,
      "loss": 0.6995,
      "step": 15925
    },
    {
      "epoch": 13.336331311441121,
      "grad_norm": 19.437259674072266,
      "learning_rate": 1.041187238493724e-05,
      "loss": 0.723,
      "step": 15950
    },
    {
      "epoch": 13.357247437774523,
      "grad_norm": 25.248979568481445,
      "learning_rate": 1.0379184100418409e-05,
      "loss": 0.6622,
      "step": 15975
    },
    {
      "epoch": 13.378163564107927,
      "grad_norm": 27.493425369262695,
      "learning_rate": 1.0346495815899582e-05,
      "loss": 0.6897,
      "step": 16000
    },
    {
      "epoch": 13.39907969044133,
      "grad_norm": 32.20233154296875,
      "learning_rate": 1.0313807531380753e-05,
      "loss": 0.6385,
      "step": 16025
    },
    {
      "epoch": 13.419995816774733,
      "grad_norm": 27.97924041748047,
      "learning_rate": 1.0281119246861926e-05,
      "loss": 0.6531,
      "step": 16050
    },
    {
      "epoch": 13.440911943108137,
      "grad_norm": 25.23439598083496,
      "learning_rate": 1.0248430962343095e-05,
      "loss": 0.6754,
      "step": 16075
    },
    {
      "epoch": 13.46182806944154,
      "grad_norm": 29.08514404296875,
      "learning_rate": 1.0215742677824268e-05,
      "loss": 0.74,
      "step": 16100
    },
    {
      "epoch": 13.482744195774943,
      "grad_norm": 14.83765697479248,
      "learning_rate": 1.018305439330544e-05,
      "loss": 0.653,
      "step": 16125
    },
    {
      "epoch": 13.503660322108345,
      "grad_norm": 32.793575286865234,
      "learning_rate": 1.0150366108786612e-05,
      "loss": 0.692,
      "step": 16150
    },
    {
      "epoch": 13.52457644844175,
      "grad_norm": 28.030176162719727,
      "learning_rate": 1.0117677824267783e-05,
      "loss": 0.7085,
      "step": 16175
    },
    {
      "epoch": 13.545492574775151,
      "grad_norm": 27.53984832763672,
      "learning_rate": 1.0084989539748954e-05,
      "loss": 0.7456,
      "step": 16200
    },
    {
      "epoch": 13.566408701108555,
      "grad_norm": 19.197364807128906,
      "learning_rate": 1.0052301255230126e-05,
      "loss": 0.7109,
      "step": 16225
    },
    {
      "epoch": 13.587324827441957,
      "grad_norm": 22.143959045410156,
      "learning_rate": 1.0019612970711297e-05,
      "loss": 0.7132,
      "step": 16250
    },
    {
      "epoch": 13.608240953775361,
      "grad_norm": 17.620811462402344,
      "learning_rate": 9.98692468619247e-06,
      "loss": 0.6414,
      "step": 16275
    },
    {
      "epoch": 13.629157080108763,
      "grad_norm": 27.154186248779297,
      "learning_rate": 9.954236401673639e-06,
      "loss": 0.6783,
      "step": 16300
    },
    {
      "epoch": 13.650073206442167,
      "grad_norm": 27.027597427368164,
      "learning_rate": 9.921548117154812e-06,
      "loss": 0.6649,
      "step": 16325
    },
    {
      "epoch": 13.67098933277557,
      "grad_norm": 35.69718933105469,
      "learning_rate": 9.888859832635983e-06,
      "loss": 0.6865,
      "step": 16350
    },
    {
      "epoch": 13.691905459108973,
      "grad_norm": 26.556852340698242,
      "learning_rate": 9.856171548117156e-06,
      "loss": 0.6906,
      "step": 16375
    },
    {
      "epoch": 13.712821585442375,
      "grad_norm": 31.462135314941406,
      "learning_rate": 9.823483263598325e-06,
      "loss": 0.7297,
      "step": 16400
    },
    {
      "epoch": 13.73373771177578,
      "grad_norm": 29.585922241210938,
      "learning_rate": 9.790794979079498e-06,
      "loss": 0.7661,
      "step": 16425
    },
    {
      "epoch": 13.754653838109181,
      "grad_norm": 28.056711196899414,
      "learning_rate": 9.75810669456067e-06,
      "loss": 0.6821,
      "step": 16450
    },
    {
      "epoch": 13.775569964442585,
      "grad_norm": 22.759174346923828,
      "learning_rate": 9.725418410041842e-06,
      "loss": 0.7291,
      "step": 16475
    },
    {
      "epoch": 13.79648609077599,
      "grad_norm": 24.184640884399414,
      "learning_rate": 9.692730125523013e-06,
      "loss": 0.6903,
      "step": 16500
    },
    {
      "epoch": 13.817402217109391,
      "grad_norm": 22.91356658935547,
      "learning_rate": 9.660041841004183e-06,
      "loss": 0.79,
      "step": 16525
    },
    {
      "epoch": 13.838318343442795,
      "grad_norm": 30.007061004638672,
      "learning_rate": 9.627353556485356e-06,
      "loss": 0.6433,
      "step": 16550
    },
    {
      "epoch": 13.859234469776197,
      "grad_norm": 23.81678581237793,
      "learning_rate": 9.594665271966527e-06,
      "loss": 0.8043,
      "step": 16575
    },
    {
      "epoch": 13.880150596109601,
      "grad_norm": 18.569869995117188,
      "learning_rate": 9.5619769874477e-06,
      "loss": 0.6494,
      "step": 16600
    },
    {
      "epoch": 13.901066722443003,
      "grad_norm": 32.27301025390625,
      "learning_rate": 9.52928870292887e-06,
      "loss": 0.7725,
      "step": 16625
    },
    {
      "epoch": 13.921982848776407,
      "grad_norm": 20.02362632751465,
      "learning_rate": 9.496600418410042e-06,
      "loss": 0.6874,
      "step": 16650
    },
    {
      "epoch": 13.94289897510981,
      "grad_norm": 26.59134292602539,
      "learning_rate": 9.463912133891213e-06,
      "loss": 0.6188,
      "step": 16675
    },
    {
      "epoch": 13.963815101443213,
      "grad_norm": 25.717819213867188,
      "learning_rate": 9.431223849372386e-06,
      "loss": 0.6982,
      "step": 16700
    },
    {
      "epoch": 13.984731227776615,
      "grad_norm": 23.78156089782715,
      "learning_rate": 9.398535564853556e-06,
      "loss": 0.7526,
      "step": 16725
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.45063671469688416,
      "eval_runtime": 8.5479,
      "eval_samples_per_second": 1988.8,
      "eval_steps_per_second": 62.238,
      "step": 16744
    },
    {
      "epoch": 14.005019870320016,
      "grad_norm": 27.21746826171875,
      "learning_rate": 9.365847280334729e-06,
      "loss": 0.6476,
      "step": 16750
    },
    {
      "epoch": 14.02593599665342,
      "grad_norm": 17.60322380065918,
      "learning_rate": 9.3331589958159e-06,
      "loss": 0.7261,
      "step": 16775
    },
    {
      "epoch": 14.046852122986824,
      "grad_norm": 22.876983642578125,
      "learning_rate": 9.300470711297073e-06,
      "loss": 0.6569,
      "step": 16800
    },
    {
      "epoch": 14.067768249320226,
      "grad_norm": 26.34349822998047,
      "learning_rate": 9.267782426778244e-06,
      "loss": 0.6602,
      "step": 16825
    },
    {
      "epoch": 14.08868437565363,
      "grad_norm": 17.439802169799805,
      "learning_rate": 9.235094142259413e-06,
      "loss": 0.5839,
      "step": 16850
    },
    {
      "epoch": 14.109600501987032,
      "grad_norm": 27.866127014160156,
      "learning_rate": 9.202405857740586e-06,
      "loss": 0.579,
      "step": 16875
    },
    {
      "epoch": 14.130516628320436,
      "grad_norm": 22.63666534423828,
      "learning_rate": 9.169717573221757e-06,
      "loss": 0.6535,
      "step": 16900
    },
    {
      "epoch": 14.151432754653838,
      "grad_norm": 34.39243698120117,
      "learning_rate": 9.13702928870293e-06,
      "loss": 0.6594,
      "step": 16925
    },
    {
      "epoch": 14.172348880987242,
      "grad_norm": 26.38691520690918,
      "learning_rate": 9.1043410041841e-06,
      "loss": 0.6087,
      "step": 16950
    },
    {
      "epoch": 14.193265007320644,
      "grad_norm": 18.290096282958984,
      "learning_rate": 9.071652719665272e-06,
      "loss": 0.6623,
      "step": 16975
    },
    {
      "epoch": 14.214181133654048,
      "grad_norm": 26.013477325439453,
      "learning_rate": 9.038964435146444e-06,
      "loss": 0.6831,
      "step": 17000
    },
    {
      "epoch": 14.23509725998745,
      "grad_norm": 17.6842098236084,
      "learning_rate": 9.006276150627616e-06,
      "loss": 0.6431,
      "step": 17025
    },
    {
      "epoch": 14.256013386320854,
      "grad_norm": 18.74855613708496,
      "learning_rate": 8.973587866108786e-06,
      "loss": 0.6878,
      "step": 17050
    },
    {
      "epoch": 14.276929512654256,
      "grad_norm": 23.079004287719727,
      "learning_rate": 8.940899581589959e-06,
      "loss": 0.5833,
      "step": 17075
    },
    {
      "epoch": 14.29784563898766,
      "grad_norm": 31.04058265686035,
      "learning_rate": 8.90821129707113e-06,
      "loss": 0.6051,
      "step": 17100
    },
    {
      "epoch": 14.318761765321062,
      "grad_norm": 29.011903762817383,
      "learning_rate": 8.875523012552301e-06,
      "loss": 0.6046,
      "step": 17125
    },
    {
      "epoch": 14.339677891654466,
      "grad_norm": 26.004091262817383,
      "learning_rate": 8.842834728033474e-06,
      "loss": 0.6972,
      "step": 17150
    },
    {
      "epoch": 14.360594017987868,
      "grad_norm": 21.49011993408203,
      "learning_rate": 8.810146443514643e-06,
      "loss": 0.653,
      "step": 17175
    },
    {
      "epoch": 14.381510144321272,
      "grad_norm": 23.728729248046875,
      "learning_rate": 8.777458158995816e-06,
      "loss": 0.6017,
      "step": 17200
    },
    {
      "epoch": 14.402426270654674,
      "grad_norm": 20.572425842285156,
      "learning_rate": 8.744769874476987e-06,
      "loss": 0.6916,
      "step": 17225
    },
    {
      "epoch": 14.423342396988078,
      "grad_norm": 30.2632999420166,
      "learning_rate": 8.71208158995816e-06,
      "loss": 0.6885,
      "step": 17250
    },
    {
      "epoch": 14.44425852332148,
      "grad_norm": 30.7268123626709,
      "learning_rate": 8.67939330543933e-06,
      "loss": 0.6409,
      "step": 17275
    },
    {
      "epoch": 14.465174649654884,
      "grad_norm": 34.40134811401367,
      "learning_rate": 8.646705020920503e-06,
      "loss": 0.646,
      "step": 17300
    },
    {
      "epoch": 14.486090775988288,
      "grad_norm": 28.4871768951416,
      "learning_rate": 8.614016736401674e-06,
      "loss": 0.6741,
      "step": 17325
    },
    {
      "epoch": 14.50700690232169,
      "grad_norm": 27.9281005859375,
      "learning_rate": 8.581328451882847e-06,
      "loss": 0.6596,
      "step": 17350
    },
    {
      "epoch": 14.527923028655094,
      "grad_norm": 27.875795364379883,
      "learning_rate": 8.548640167364016e-06,
      "loss": 0.591,
      "step": 17375
    },
    {
      "epoch": 14.548839154988496,
      "grad_norm": 31.706310272216797,
      "learning_rate": 8.515951882845189e-06,
      "loss": 0.6842,
      "step": 17400
    },
    {
      "epoch": 14.5697552813219,
      "grad_norm": 26.94153594970703,
      "learning_rate": 8.48326359832636e-06,
      "loss": 0.6819,
      "step": 17425
    },
    {
      "epoch": 14.590671407655302,
      "grad_norm": 27.228618621826172,
      "learning_rate": 8.450575313807531e-06,
      "loss": 0.7014,
      "step": 17450
    },
    {
      "epoch": 14.611587533988706,
      "grad_norm": 23.867399215698242,
      "learning_rate": 8.417887029288704e-06,
      "loss": 0.6357,
      "step": 17475
    },
    {
      "epoch": 14.632503660322108,
      "grad_norm": 24.42279052734375,
      "learning_rate": 8.385198744769874e-06,
      "loss": 0.6292,
      "step": 17500
    },
    {
      "epoch": 14.653419786655512,
      "grad_norm": 17.33199691772461,
      "learning_rate": 8.3538179916318e-06,
      "loss": 0.7117,
      "step": 17525
    },
    {
      "epoch": 14.674335912988914,
      "grad_norm": 23.66933822631836,
      "learning_rate": 8.321129707112972e-06,
      "loss": 0.661,
      "step": 17550
    },
    {
      "epoch": 14.695252039322318,
      "grad_norm": 22.87213134765625,
      "learning_rate": 8.288441422594142e-06,
      "loss": 0.6645,
      "step": 17575
    },
    {
      "epoch": 14.71616816565572,
      "grad_norm": 30.38180923461914,
      "learning_rate": 8.255753138075315e-06,
      "loss": 0.6753,
      "step": 17600
    },
    {
      "epoch": 14.737084291989124,
      "grad_norm": 23.322147369384766,
      "learning_rate": 8.223064853556486e-06,
      "loss": 0.6701,
      "step": 17625
    },
    {
      "epoch": 14.758000418322526,
      "grad_norm": 32.356136322021484,
      "learning_rate": 8.190376569037657e-06,
      "loss": 0.6218,
      "step": 17650
    },
    {
      "epoch": 14.77891654465593,
      "grad_norm": 27.28126335144043,
      "learning_rate": 8.157688284518828e-06,
      "loss": 0.6901,
      "step": 17675
    },
    {
      "epoch": 14.799832670989332,
      "grad_norm": 30.114545822143555,
      "learning_rate": 8.125e-06,
      "loss": 0.6762,
      "step": 17700
    },
    {
      "epoch": 14.820748797322736,
      "grad_norm": 33.17584991455078,
      "learning_rate": 8.092311715481172e-06,
      "loss": 0.7379,
      "step": 17725
    },
    {
      "epoch": 14.84166492365614,
      "grad_norm": 16.64292335510254,
      "learning_rate": 8.059623430962343e-06,
      "loss": 0.6016,
      "step": 17750
    },
    {
      "epoch": 14.862581049989542,
      "grad_norm": 27.2663631439209,
      "learning_rate": 8.026935146443516e-06,
      "loss": 0.7788,
      "step": 17775
    },
    {
      "epoch": 14.883497176322946,
      "grad_norm": 18.45258331298828,
      "learning_rate": 7.994246861924686e-06,
      "loss": 0.6845,
      "step": 17800
    },
    {
      "epoch": 14.904413302656348,
      "grad_norm": 25.04967498779297,
      "learning_rate": 7.961558577405858e-06,
      "loss": 0.6252,
      "step": 17825
    },
    {
      "epoch": 14.925329428989752,
      "grad_norm": 21.82621955871582,
      "learning_rate": 7.92887029288703e-06,
      "loss": 0.7042,
      "step": 17850
    },
    {
      "epoch": 14.946245555323154,
      "grad_norm": 21.983110427856445,
      "learning_rate": 7.896182008368203e-06,
      "loss": 0.6442,
      "step": 17875
    },
    {
      "epoch": 14.967161681656558,
      "grad_norm": 27.303857803344727,
      "learning_rate": 7.863493723849372e-06,
      "loss": 0.6373,
      "step": 17900
    },
    {
      "epoch": 14.98807780798996,
      "grad_norm": 35.700439453125,
      "learning_rate": 7.830805439330543e-06,
      "loss": 0.5784,
      "step": 17925
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.45624667406082153,
      "eval_runtime": 8.5259,
      "eval_samples_per_second": 1993.931,
      "eval_steps_per_second": 62.398,
      "step": 17940
    },
    {
      "epoch": 15.00836645053336,
      "grad_norm": 29.434890747070312,
      "learning_rate": 7.798117154811716e-06,
      "loss": 0.5881,
      "step": 17950
    },
    {
      "epoch": 15.029282576866764,
      "grad_norm": 9.632792472839355,
      "learning_rate": 7.765428870292887e-06,
      "loss": 0.5685,
      "step": 17975
    },
    {
      "epoch": 15.050198703200167,
      "grad_norm": 29.344385147094727,
      "learning_rate": 7.732740585774058e-06,
      "loss": 0.582,
      "step": 18000
    },
    {
      "epoch": 15.07111482953357,
      "grad_norm": 20.27712631225586,
      "learning_rate": 7.70005230125523e-06,
      "loss": 0.589,
      "step": 18025
    },
    {
      "epoch": 15.092030955866974,
      "grad_norm": 19.181148529052734,
      "learning_rate": 7.667364016736402e-06,
      "loss": 0.6258,
      "step": 18050
    },
    {
      "epoch": 15.112947082200376,
      "grad_norm": 21.827241897583008,
      "learning_rate": 7.634675732217574e-06,
      "loss": 0.563,
      "step": 18075
    },
    {
      "epoch": 15.13386320853378,
      "grad_norm": 25.394987106323242,
      "learning_rate": 7.6019874476987455e-06,
      "loss": 0.729,
      "step": 18100
    },
    {
      "epoch": 15.154779334867182,
      "grad_norm": 19.26894187927246,
      "learning_rate": 7.569299163179917e-06,
      "loss": 0.58,
      "step": 18125
    },
    {
      "epoch": 15.175695461200586,
      "grad_norm": 94.33802032470703,
      "learning_rate": 7.536610878661089e-06,
      "loss": 0.6685,
      "step": 18150
    },
    {
      "epoch": 15.196611587533988,
      "grad_norm": 36.27992630004883,
      "learning_rate": 7.50392259414226e-06,
      "loss": 0.5521,
      "step": 18175
    },
    {
      "epoch": 15.217527713867392,
      "grad_norm": 25.91314125061035,
      "learning_rate": 7.471234309623431e-06,
      "loss": 0.5728,
      "step": 18200
    },
    {
      "epoch": 15.238443840200794,
      "grad_norm": 27.69879722595215,
      "learning_rate": 7.438546025104603e-06,
      "loss": 0.6593,
      "step": 18225
    },
    {
      "epoch": 15.259359966534198,
      "grad_norm": 31.658187866210938,
      "learning_rate": 7.405857740585774e-06,
      "loss": 0.5694,
      "step": 18250
    },
    {
      "epoch": 15.2802760928676,
      "grad_norm": 34.73810577392578,
      "learning_rate": 7.373169456066946e-06,
      "loss": 0.5822,
      "step": 18275
    },
    {
      "epoch": 15.301192219201004,
      "grad_norm": 19.417741775512695,
      "learning_rate": 7.340481171548117e-06,
      "loss": 0.6531,
      "step": 18300
    },
    {
      "epoch": 15.322108345534406,
      "grad_norm": 27.048917770385742,
      "learning_rate": 7.307792887029289e-06,
      "loss": 0.6675,
      "step": 18325
    },
    {
      "epoch": 15.34302447186781,
      "grad_norm": 19.731996536254883,
      "learning_rate": 7.27510460251046e-06,
      "loss": 0.5966,
      "step": 18350
    },
    {
      "epoch": 15.363940598201212,
      "grad_norm": 26.918025970458984,
      "learning_rate": 7.242416317991632e-06,
      "loss": 0.58,
      "step": 18375
    },
    {
      "epoch": 15.384856724534616,
      "grad_norm": 17.82736587524414,
      "learning_rate": 7.209728033472803e-06,
      "loss": 0.6075,
      "step": 18400
    },
    {
      "epoch": 15.405772850868019,
      "grad_norm": 19.415218353271484,
      "learning_rate": 7.177039748953975e-06,
      "loss": 0.6257,
      "step": 18425
    },
    {
      "epoch": 15.426688977201422,
      "grad_norm": 22.310649871826172,
      "learning_rate": 7.144351464435147e-06,
      "loss": 0.6292,
      "step": 18450
    },
    {
      "epoch": 15.447605103534825,
      "grad_norm": 24.788942337036133,
      "learning_rate": 7.111663179916318e-06,
      "loss": 0.5595,
      "step": 18475
    },
    {
      "epoch": 15.468521229868228,
      "grad_norm": 26.90462303161621,
      "learning_rate": 7.07897489539749e-06,
      "loss": 0.6655,
      "step": 18500
    },
    {
      "epoch": 15.489437356201632,
      "grad_norm": 25.189428329467773,
      "learning_rate": 7.046286610878661e-06,
      "loss": 0.6413,
      "step": 18525
    },
    {
      "epoch": 15.510353482535034,
      "grad_norm": 26.48740005493164,
      "learning_rate": 7.013598326359833e-06,
      "loss": 0.6069,
      "step": 18550
    },
    {
      "epoch": 15.531269608868438,
      "grad_norm": 28.22166633605957,
      "learning_rate": 6.9809100418410045e-06,
      "loss": 0.6854,
      "step": 18575
    },
    {
      "epoch": 15.55218573520184,
      "grad_norm": 19.687023162841797,
      "learning_rate": 6.9482217573221765e-06,
      "loss": 0.5882,
      "step": 18600
    },
    {
      "epoch": 15.573101861535244,
      "grad_norm": 21.004444122314453,
      "learning_rate": 6.915533472803348e-06,
      "loss": 0.5884,
      "step": 18625
    },
    {
      "epoch": 15.594017987868646,
      "grad_norm": 16.368255615234375,
      "learning_rate": 6.882845188284519e-06,
      "loss": 0.6388,
      "step": 18650
    },
    {
      "epoch": 15.61493411420205,
      "grad_norm": 16.212125778198242,
      "learning_rate": 6.85015690376569e-06,
      "loss": 0.6495,
      "step": 18675
    },
    {
      "epoch": 15.635850240535452,
      "grad_norm": 27.79647445678711,
      "learning_rate": 6.817468619246862e-06,
      "loss": 0.6271,
      "step": 18700
    },
    {
      "epoch": 15.656766366868856,
      "grad_norm": 19.745248794555664,
      "learning_rate": 6.784780334728033e-06,
      "loss": 0.5517,
      "step": 18725
    },
    {
      "epoch": 15.677682493202258,
      "grad_norm": 23.991899490356445,
      "learning_rate": 6.752092050209205e-06,
      "loss": 0.5516,
      "step": 18750
    },
    {
      "epoch": 15.698598619535662,
      "grad_norm": 24.869688034057617,
      "learning_rate": 6.719403765690376e-06,
      "loss": 0.5962,
      "step": 18775
    },
    {
      "epoch": 15.719514745869064,
      "grad_norm": 31.081968307495117,
      "learning_rate": 6.686715481171548e-06,
      "loss": 0.6247,
      "step": 18800
    },
    {
      "epoch": 15.740430872202468,
      "grad_norm": 28.852811813354492,
      "learning_rate": 6.65402719665272e-06,
      "loss": 0.5642,
      "step": 18825
    },
    {
      "epoch": 15.76134699853587,
      "grad_norm": 25.388277053833008,
      "learning_rate": 6.6213389121338915e-06,
      "loss": 0.6036,
      "step": 18850
    },
    {
      "epoch": 15.782263124869274,
      "grad_norm": 26.333370208740234,
      "learning_rate": 6.5886506276150635e-06,
      "loss": 0.548,
      "step": 18875
    },
    {
      "epoch": 15.803179251202677,
      "grad_norm": 29.040122985839844,
      "learning_rate": 6.555962343096235e-06,
      "loss": 0.6763,
      "step": 18900
    },
    {
      "epoch": 15.82409537753608,
      "grad_norm": 29.1243896484375,
      "learning_rate": 6.523274058577406e-06,
      "loss": 0.6853,
      "step": 18925
    },
    {
      "epoch": 15.845011503869483,
      "grad_norm": 24.71427345275879,
      "learning_rate": 6.490585774058577e-06,
      "loss": 0.64,
      "step": 18950
    },
    {
      "epoch": 15.865927630202886,
      "grad_norm": 22.605274200439453,
      "learning_rate": 6.457897489539749e-06,
      "loss": 0.6034,
      "step": 18975
    },
    {
      "epoch": 15.88684375653629,
      "grad_norm": 24.99295997619629,
      "learning_rate": 6.42520920502092e-06,
      "loss": 0.564,
      "step": 19000
    },
    {
      "epoch": 15.907759882869692,
      "grad_norm": 25.16373062133789,
      "learning_rate": 6.392520920502092e-06,
      "loss": 0.5936,
      "step": 19025
    },
    {
      "epoch": 15.928676009203096,
      "grad_norm": 26.38625144958496,
      "learning_rate": 6.359832635983263e-06,
      "loss": 0.5937,
      "step": 19050
    },
    {
      "epoch": 15.949592135536498,
      "grad_norm": 30.4986515045166,
      "learning_rate": 6.327144351464435e-06,
      "loss": 0.6297,
      "step": 19075
    },
    {
      "epoch": 15.970508261869902,
      "grad_norm": 15.1677885055542,
      "learning_rate": 6.2944560669456065e-06,
      "loss": 0.6943,
      "step": 19100
    },
    {
      "epoch": 15.991424388203304,
      "grad_norm": 14.622049331665039,
      "learning_rate": 6.2617677824267785e-06,
      "loss": 0.5509,
      "step": 19125
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.43542468547821045,
      "eval_runtime": 8.5175,
      "eval_samples_per_second": 1995.889,
      "eval_steps_per_second": 62.46,
      "step": 19136
    },
    {
      "epoch": 16.011713030746705,
      "grad_norm": 21.456384658813477,
      "learning_rate": 6.2290794979079506e-06,
      "loss": 0.5487,
      "step": 19150
    },
    {
      "epoch": 16.03262915708011,
      "grad_norm": 20.04269027709961,
      "learning_rate": 6.196391213389122e-06,
      "loss": 0.5844,
      "step": 19175
    },
    {
      "epoch": 16.053545283413513,
      "grad_norm": 24.92833709716797,
      "learning_rate": 6.163702928870294e-06,
      "loss": 0.5832,
      "step": 19200
    },
    {
      "epoch": 16.074461409746913,
      "grad_norm": 20.6861515045166,
      "learning_rate": 6.131014644351464e-06,
      "loss": 0.5547,
      "step": 19225
    },
    {
      "epoch": 16.095377536080317,
      "grad_norm": 29.76327896118164,
      "learning_rate": 6.098326359832636e-06,
      "loss": 0.5673,
      "step": 19250
    },
    {
      "epoch": 16.11629366241372,
      "grad_norm": 26.326913833618164,
      "learning_rate": 6.065638075313807e-06,
      "loss": 0.601,
      "step": 19275
    },
    {
      "epoch": 16.137209788747125,
      "grad_norm": 25.745677947998047,
      "learning_rate": 6.032949790794979e-06,
      "loss": 0.6294,
      "step": 19300
    },
    {
      "epoch": 16.15812591508053,
      "grad_norm": 24.40106964111328,
      "learning_rate": 6.00026150627615e-06,
      "loss": 0.5535,
      "step": 19325
    },
    {
      "epoch": 16.17904204141393,
      "grad_norm": 33.319091796875,
      "learning_rate": 5.967573221757322e-06,
      "loss": 0.6101,
      "step": 19350
    },
    {
      "epoch": 16.199958167747333,
      "grad_norm": 27.991657257080078,
      "learning_rate": 5.934884937238494e-06,
      "loss": 0.5296,
      "step": 19375
    },
    {
      "epoch": 16.220874294080737,
      "grad_norm": 23.595748901367188,
      "learning_rate": 5.902196652719666e-06,
      "loss": 0.5613,
      "step": 19400
    },
    {
      "epoch": 16.24179042041414,
      "grad_norm": 29.23347282409668,
      "learning_rate": 5.869508368200837e-06,
      "loss": 0.5873,
      "step": 19425
    },
    {
      "epoch": 16.26270654674754,
      "grad_norm": 24.653825759887695,
      "learning_rate": 5.836820083682009e-06,
      "loss": 0.5674,
      "step": 19450
    },
    {
      "epoch": 16.283622673080945,
      "grad_norm": 21.551992416381836,
      "learning_rate": 5.804131799163181e-06,
      "loss": 0.5287,
      "step": 19475
    },
    {
      "epoch": 16.30453879941435,
      "grad_norm": 16.133729934692383,
      "learning_rate": 5.771443514644352e-06,
      "loss": 0.6249,
      "step": 19500
    },
    {
      "epoch": 16.325454925747753,
      "grad_norm": 23.659862518310547,
      "learning_rate": 5.738755230125523e-06,
      "loss": 0.5603,
      "step": 19525
    },
    {
      "epoch": 16.346371052081153,
      "grad_norm": 29.190366744995117,
      "learning_rate": 5.706066945606694e-06,
      "loss": 0.5431,
      "step": 19550
    },
    {
      "epoch": 16.367287178414557,
      "grad_norm": 27.483041763305664,
      "learning_rate": 5.673378661087866e-06,
      "loss": 0.6321,
      "step": 19575
    },
    {
      "epoch": 16.38820330474796,
      "grad_norm": 23.454917907714844,
      "learning_rate": 5.6406903765690374e-06,
      "loss": 0.6021,
      "step": 19600
    },
    {
      "epoch": 16.409119431081365,
      "grad_norm": 26.86640739440918,
      "learning_rate": 5.6080020920502095e-06,
      "loss": 0.5824,
      "step": 19625
    },
    {
      "epoch": 16.430035557414765,
      "grad_norm": 25.996641159057617,
      "learning_rate": 5.575313807531381e-06,
      "loss": 0.5851,
      "step": 19650
    },
    {
      "epoch": 16.45095168374817,
      "grad_norm": 24.970218658447266,
      "learning_rate": 5.542625523012553e-06,
      "loss": 0.5976,
      "step": 19675
    },
    {
      "epoch": 16.471867810081573,
      "grad_norm": 25.167898178100586,
      "learning_rate": 5.509937238493724e-06,
      "loss": 0.5936,
      "step": 19700
    },
    {
      "epoch": 16.492783936414977,
      "grad_norm": 29.463064193725586,
      "learning_rate": 5.477248953974896e-06,
      "loss": 0.6091,
      "step": 19725
    },
    {
      "epoch": 16.513700062748377,
      "grad_norm": 29.862857818603516,
      "learning_rate": 5.444560669456067e-06,
      "loss": 0.5668,
      "step": 19750
    },
    {
      "epoch": 16.53461618908178,
      "grad_norm": 19.82440757751465,
      "learning_rate": 5.411872384937239e-06,
      "loss": 0.5864,
      "step": 19775
    },
    {
      "epoch": 16.555532315415185,
      "grad_norm": 19.891569137573242,
      "learning_rate": 5.37918410041841e-06,
      "loss": 0.5877,
      "step": 19800
    },
    {
      "epoch": 16.57644844174859,
      "grad_norm": 24.955678939819336,
      "learning_rate": 5.346495815899581e-06,
      "loss": 0.5256,
      "step": 19825
    },
    {
      "epoch": 16.597364568081993,
      "grad_norm": 20.827842712402344,
      "learning_rate": 5.313807531380753e-06,
      "loss": 0.6262,
      "step": 19850
    },
    {
      "epoch": 16.618280694415393,
      "grad_norm": 25.030508041381836,
      "learning_rate": 5.2811192468619245e-06,
      "loss": 0.544,
      "step": 19875
    },
    {
      "epoch": 16.639196820748797,
      "grad_norm": 28.54717445373535,
      "learning_rate": 5.2484309623430965e-06,
      "loss": 0.6014,
      "step": 19900
    },
    {
      "epoch": 16.6601129470822,
      "grad_norm": 21.18910026550293,
      "learning_rate": 5.215742677824268e-06,
      "loss": 0.5266,
      "step": 19925
    },
    {
      "epoch": 16.681029073415605,
      "grad_norm": 18.142093658447266,
      "learning_rate": 5.18305439330544e-06,
      "loss": 0.6296,
      "step": 19950
    },
    {
      "epoch": 16.701945199749005,
      "grad_norm": 24.273056030273438,
      "learning_rate": 5.150366108786611e-06,
      "loss": 0.5509,
      "step": 19975
    },
    {
      "epoch": 16.72286132608241,
      "grad_norm": 20.217103958129883,
      "learning_rate": 5.117677824267783e-06,
      "loss": 0.5638,
      "step": 20000
    },
    {
      "epoch": 16.743777452415813,
      "grad_norm": 22.943490982055664,
      "learning_rate": 5.084989539748954e-06,
      "loss": 0.5723,
      "step": 20025
    },
    {
      "epoch": 16.764693578749217,
      "grad_norm": 26.842992782592773,
      "learning_rate": 5.052301255230126e-06,
      "loss": 0.6048,
      "step": 20050
    },
    {
      "epoch": 16.785609705082617,
      "grad_norm": 28.04810905456543,
      "learning_rate": 5.019612970711297e-06,
      "loss": 0.584,
      "step": 20075
    },
    {
      "epoch": 16.80652583141602,
      "grad_norm": 22.041385650634766,
      "learning_rate": 4.986924686192468e-06,
      "loss": 0.6025,
      "step": 20100
    },
    {
      "epoch": 16.827441957749425,
      "grad_norm": 20.3675537109375,
      "learning_rate": 4.9542364016736395e-06,
      "loss": 0.5264,
      "step": 20125
    },
    {
      "epoch": 16.84835808408283,
      "grad_norm": 22.57901382446289,
      "learning_rate": 4.9215481171548115e-06,
      "loss": 0.5268,
      "step": 20150
    },
    {
      "epoch": 16.86927421041623,
      "grad_norm": 27.352994918823242,
      "learning_rate": 4.8888598326359836e-06,
      "loss": 0.6272,
      "step": 20175
    },
    {
      "epoch": 16.890190336749633,
      "grad_norm": 16.658157348632812,
      "learning_rate": 4.856171548117155e-06,
      "loss": 0.5727,
      "step": 20200
    },
    {
      "epoch": 16.911106463083037,
      "grad_norm": 28.211088180541992,
      "learning_rate": 4.823483263598327e-06,
      "loss": 0.5847,
      "step": 20225
    },
    {
      "epoch": 16.93202258941644,
      "grad_norm": 21.374284744262695,
      "learning_rate": 4.790794979079498e-06,
      "loss": 0.5681,
      "step": 20250
    },
    {
      "epoch": 16.952938715749845,
      "grad_norm": 21.63615608215332,
      "learning_rate": 4.75810669456067e-06,
      "loss": 0.6079,
      "step": 20275
    },
    {
      "epoch": 16.973854842083245,
      "grad_norm": 22.494966506958008,
      "learning_rate": 4.725418410041841e-06,
      "loss": 0.5505,
      "step": 20300
    },
    {
      "epoch": 16.99477096841665,
      "grad_norm": 24.865148544311523,
      "learning_rate": 4.692730125523013e-06,
      "loss": 0.5985,
      "step": 20325
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.452887624502182,
      "eval_runtime": 8.6588,
      "eval_samples_per_second": 1963.318,
      "eval_steps_per_second": 61.44,
      "step": 20332
    },
    {
      "epoch": 17.01505961096005,
      "grad_norm": 25.27718734741211,
      "learning_rate": 4.660041841004184e-06,
      "loss": 0.5469,
      "step": 20350
    },
    {
      "epoch": 17.03597573729345,
      "grad_norm": 20.0494327545166,
      "learning_rate": 4.627353556485356e-06,
      "loss": 0.5448,
      "step": 20375
    },
    {
      "epoch": 17.056891863626856,
      "grad_norm": 20.197439193725586,
      "learning_rate": 4.594665271966527e-06,
      "loss": 0.5774,
      "step": 20400
    },
    {
      "epoch": 17.07780798996026,
      "grad_norm": 29.073387145996094,
      "learning_rate": 4.561976987447699e-06,
      "loss": 0.5858,
      "step": 20425
    },
    {
      "epoch": 17.098724116293663,
      "grad_norm": 22.61433982849121,
      "learning_rate": 4.52928870292887e-06,
      "loss": 0.5509,
      "step": 20450
    },
    {
      "epoch": 17.119640242627064,
      "grad_norm": 30.086811065673828,
      "learning_rate": 4.496600418410042e-06,
      "loss": 0.5456,
      "step": 20475
    },
    {
      "epoch": 17.140556368960468,
      "grad_norm": 25.036619186401367,
      "learning_rate": 4.463912133891214e-06,
      "loss": 0.6074,
      "step": 20500
    },
    {
      "epoch": 17.16147249529387,
      "grad_norm": 30.586782455444336,
      "learning_rate": 4.431223849372385e-06,
      "loss": 0.5221,
      "step": 20525
    },
    {
      "epoch": 17.182388621627275,
      "grad_norm": 20.237926483154297,
      "learning_rate": 4.398535564853557e-06,
      "loss": 0.5144,
      "step": 20550
    },
    {
      "epoch": 17.20330474796068,
      "grad_norm": 29.33299446105957,
      "learning_rate": 4.365847280334728e-06,
      "loss": 0.5419,
      "step": 20575
    },
    {
      "epoch": 17.22422087429408,
      "grad_norm": 31.553020477294922,
      "learning_rate": 4.3331589958159e-06,
      "loss": 0.5716,
      "step": 20600
    },
    {
      "epoch": 17.245137000627484,
      "grad_norm": 18.576616287231445,
      "learning_rate": 4.300470711297071e-06,
      "loss": 0.6052,
      "step": 20625
    },
    {
      "epoch": 17.266053126960887,
      "grad_norm": 21.85422706604004,
      "learning_rate": 4.267782426778243e-06,
      "loss": 0.6153,
      "step": 20650
    },
    {
      "epoch": 17.28696925329429,
      "grad_norm": 30.460468292236328,
      "learning_rate": 4.2350941422594145e-06,
      "loss": 0.5803,
      "step": 20675
    },
    {
      "epoch": 17.30788537962769,
      "grad_norm": 28.88724708557129,
      "learning_rate": 4.202405857740586e-06,
      "loss": 0.5157,
      "step": 20700
    },
    {
      "epoch": 17.328801505961096,
      "grad_norm": 19.0960636138916,
      "learning_rate": 4.169717573221757e-06,
      "loss": 0.6344,
      "step": 20725
    },
    {
      "epoch": 17.3497176322945,
      "grad_norm": 22.6420841217041,
      "learning_rate": 4.137029288702929e-06,
      "loss": 0.529,
      "step": 20750
    },
    {
      "epoch": 17.370633758627903,
      "grad_norm": 20.00295639038086,
      "learning_rate": 4.1043410041841e-06,
      "loss": 0.495,
      "step": 20775
    },
    {
      "epoch": 17.391549884961304,
      "grad_norm": 21.29647445678711,
      "learning_rate": 4.071652719665272e-06,
      "loss": 0.565,
      "step": 20800
    },
    {
      "epoch": 17.412466011294708,
      "grad_norm": 24.859243392944336,
      "learning_rate": 4.038964435146444e-06,
      "loss": 0.566,
      "step": 20825
    },
    {
      "epoch": 17.43338213762811,
      "grad_norm": 18.75313377380371,
      "learning_rate": 4.006276150627615e-06,
      "loss": 0.5206,
      "step": 20850
    },
    {
      "epoch": 17.454298263961515,
      "grad_norm": 24.31562614440918,
      "learning_rate": 3.973587866108787e-06,
      "loss": 0.5201,
      "step": 20875
    },
    {
      "epoch": 17.475214390294916,
      "grad_norm": 32.70683288574219,
      "learning_rate": 3.940899581589958e-06,
      "loss": 0.5462,
      "step": 20900
    },
    {
      "epoch": 17.49613051662832,
      "grad_norm": 36.003456115722656,
      "learning_rate": 3.90821129707113e-06,
      "loss": 0.5535,
      "step": 20925
    },
    {
      "epoch": 17.517046642961724,
      "grad_norm": 25.51317024230957,
      "learning_rate": 3.8755230125523015e-06,
      "loss": 0.5673,
      "step": 20950
    },
    {
      "epoch": 17.537962769295127,
      "grad_norm": 22.85143280029297,
      "learning_rate": 3.8428347280334735e-06,
      "loss": 0.6024,
      "step": 20975
    },
    {
      "epoch": 17.55887889562853,
      "grad_norm": 23.43942642211914,
      "learning_rate": 3.8101464435146443e-06,
      "loss": 0.5351,
      "step": 21000
    },
    {
      "epoch": 17.57979502196193,
      "grad_norm": 25.495681762695312,
      "learning_rate": 3.777458158995816e-06,
      "loss": 0.5469,
      "step": 21025
    },
    {
      "epoch": 17.600711148295336,
      "grad_norm": 31.294160842895508,
      "learning_rate": 3.744769874476988e-06,
      "loss": 0.5683,
      "step": 21050
    },
    {
      "epoch": 17.62162727462874,
      "grad_norm": 23.361785888671875,
      "learning_rate": 3.712081589958159e-06,
      "loss": 0.5636,
      "step": 21075
    },
    {
      "epoch": 17.642543400962143,
      "grad_norm": 25.85384750366211,
      "learning_rate": 3.6793933054393306e-06,
      "loss": 0.5921,
      "step": 21100
    },
    {
      "epoch": 17.663459527295544,
      "grad_norm": 16.19610595703125,
      "learning_rate": 3.6467050209205022e-06,
      "loss": 0.5186,
      "step": 21125
    },
    {
      "epoch": 17.684375653628948,
      "grad_norm": 21.08545684814453,
      "learning_rate": 3.614016736401674e-06,
      "loss": 0.5723,
      "step": 21150
    },
    {
      "epoch": 17.70529177996235,
      "grad_norm": 23.117658615112305,
      "learning_rate": 3.5813284518828454e-06,
      "loss": 0.5483,
      "step": 21175
    },
    {
      "epoch": 17.726207906295755,
      "grad_norm": 26.03366470336914,
      "learning_rate": 3.548640167364017e-06,
      "loss": 0.5363,
      "step": 21200
    },
    {
      "epoch": 17.747124032629156,
      "grad_norm": 18.901447296142578,
      "learning_rate": 3.515951882845188e-06,
      "loss": 0.5329,
      "step": 21225
    },
    {
      "epoch": 17.76804015896256,
      "grad_norm": 16.027189254760742,
      "learning_rate": 3.4832635983263597e-06,
      "loss": 0.6122,
      "step": 21250
    },
    {
      "epoch": 17.788956285295964,
      "grad_norm": 32.098167419433594,
      "learning_rate": 3.4505753138075313e-06,
      "loss": 0.6264,
      "step": 21275
    },
    {
      "epoch": 17.809872411629367,
      "grad_norm": 19.78704261779785,
      "learning_rate": 3.417887029288703e-06,
      "loss": 0.5063,
      "step": 21300
    },
    {
      "epoch": 17.830788537962768,
      "grad_norm": 25.945152282714844,
      "learning_rate": 3.3851987447698745e-06,
      "loss": 0.5015,
      "step": 21325
    },
    {
      "epoch": 17.85170466429617,
      "grad_norm": 25.29412269592285,
      "learning_rate": 3.352510460251046e-06,
      "loss": 0.5497,
      "step": 21350
    },
    {
      "epoch": 17.872620790629576,
      "grad_norm": 26.863969802856445,
      "learning_rate": 3.3198221757322177e-06,
      "loss": 0.5035,
      "step": 21375
    },
    {
      "epoch": 17.89353691696298,
      "grad_norm": 21.4409122467041,
      "learning_rate": 3.2871338912133893e-06,
      "loss": 0.5328,
      "step": 21400
    },
    {
      "epoch": 17.914453043296383,
      "grad_norm": 25.090879440307617,
      "learning_rate": 3.254445606694561e-06,
      "loss": 0.6012,
      "step": 21425
    },
    {
      "epoch": 17.935369169629784,
      "grad_norm": 20.238767623901367,
      "learning_rate": 3.2217573221757324e-06,
      "loss": 0.5722,
      "step": 21450
    },
    {
      "epoch": 17.956285295963188,
      "grad_norm": 25.537748336791992,
      "learning_rate": 3.189069037656904e-06,
      "loss": 0.5171,
      "step": 21475
    },
    {
      "epoch": 17.97720142229659,
      "grad_norm": 21.217021942138672,
      "learning_rate": 3.156380753138075e-06,
      "loss": 0.5223,
      "step": 21500
    },
    {
      "epoch": 17.998117548629995,
      "grad_norm": 35.13198471069336,
      "learning_rate": 3.1236924686192468e-06,
      "loss": 0.533,
      "step": 21525
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.46557825803756714,
      "eval_runtime": 8.4436,
      "eval_samples_per_second": 2013.348,
      "eval_steps_per_second": 63.006,
      "step": 21528
    },
    {
      "epoch": 18.018406191173394,
      "grad_norm": 28.321260452270508,
      "learning_rate": 3.0910041841004184e-06,
      "loss": 0.4626,
      "step": 21550
    },
    {
      "epoch": 18.039322317506798,
      "grad_norm": 33.36209487915039,
      "learning_rate": 3.05831589958159e-06,
      "loss": 0.5182,
      "step": 21575
    },
    {
      "epoch": 18.060238443840202,
      "grad_norm": 21.300575256347656,
      "learning_rate": 3.0256276150627615e-06,
      "loss": 0.4869,
      "step": 21600
    },
    {
      "epoch": 18.081154570173602,
      "grad_norm": 20.51805305480957,
      "learning_rate": 2.992939330543933e-06,
      "loss": 0.567,
      "step": 21625
    },
    {
      "epoch": 18.102070696507006,
      "grad_norm": 15.670023918151855,
      "learning_rate": 2.9602510460251043e-06,
      "loss": 0.4912,
      "step": 21650
    },
    {
      "epoch": 18.12298682284041,
      "grad_norm": 22.447750091552734,
      "learning_rate": 2.9275627615062763e-06,
      "loss": 0.51,
      "step": 21675
    },
    {
      "epoch": 18.143902949173814,
      "grad_norm": 24.604135513305664,
      "learning_rate": 2.894874476987448e-06,
      "loss": 0.5789,
      "step": 21700
    },
    {
      "epoch": 18.164819075507214,
      "grad_norm": 24.821521759033203,
      "learning_rate": 2.8621861924686195e-06,
      "loss": 0.5465,
      "step": 21725
    },
    {
      "epoch": 18.185735201840618,
      "grad_norm": 14.214305877685547,
      "learning_rate": 2.829497907949791e-06,
      "loss": 0.4983,
      "step": 21750
    },
    {
      "epoch": 18.206651328174022,
      "grad_norm": 19.267057418823242,
      "learning_rate": 2.7968096234309627e-06,
      "loss": 0.4962,
      "step": 21775
    },
    {
      "epoch": 18.227567454507426,
      "grad_norm": 26.617944717407227,
      "learning_rate": 2.764121338912134e-06,
      "loss": 0.5357,
      "step": 21800
    },
    {
      "epoch": 18.24848358084083,
      "grad_norm": 25.55438804626465,
      "learning_rate": 2.7314330543933054e-06,
      "loss": 0.4724,
      "step": 21825
    },
    {
      "epoch": 18.26939970717423,
      "grad_norm": 30.916135787963867,
      "learning_rate": 2.698744769874477e-06,
      "loss": 0.5552,
      "step": 21850
    },
    {
      "epoch": 18.290315833507634,
      "grad_norm": 33.19813537597656,
      "learning_rate": 2.6660564853556486e-06,
      "loss": 0.5224,
      "step": 21875
    },
    {
      "epoch": 18.311231959841038,
      "grad_norm": 27.954370498657227,
      "learning_rate": 2.63336820083682e-06,
      "loss": 0.5361,
      "step": 21900
    },
    {
      "epoch": 18.332148086174442,
      "grad_norm": 31.472339630126953,
      "learning_rate": 2.6006799163179918e-06,
      "loss": 0.5436,
      "step": 21925
    },
    {
      "epoch": 18.353064212507842,
      "grad_norm": 29.890962600708008,
      "learning_rate": 2.567991631799163e-06,
      "loss": 0.5906,
      "step": 21950
    },
    {
      "epoch": 18.373980338841246,
      "grad_norm": 17.688426971435547,
      "learning_rate": 2.5353033472803345e-06,
      "loss": 0.5707,
      "step": 21975
    },
    {
      "epoch": 18.39489646517465,
      "grad_norm": 22.148412704467773,
      "learning_rate": 2.502615062761506e-06,
      "loss": 0.5731,
      "step": 22000
    },
    {
      "epoch": 18.415812591508054,
      "grad_norm": 21.56778335571289,
      "learning_rate": 2.469926778242678e-06,
      "loss": 0.4826,
      "step": 22025
    },
    {
      "epoch": 18.436728717841454,
      "grad_norm": 27.493383407592773,
      "learning_rate": 2.4372384937238497e-06,
      "loss": 0.5351,
      "step": 22050
    },
    {
      "epoch": 18.457644844174858,
      "grad_norm": 20.65557289123535,
      "learning_rate": 2.4045502092050213e-06,
      "loss": 0.5536,
      "step": 22075
    },
    {
      "epoch": 18.478560970508262,
      "grad_norm": 15.197904586791992,
      "learning_rate": 2.3718619246861925e-06,
      "loss": 0.5785,
      "step": 22100
    },
    {
      "epoch": 18.499477096841666,
      "grad_norm": 29.250411987304688,
      "learning_rate": 2.339173640167364e-06,
      "loss": 0.4802,
      "step": 22125
    },
    {
      "epoch": 18.520393223175066,
      "grad_norm": 42.33552551269531,
      "learning_rate": 2.3064853556485356e-06,
      "loss": 0.5677,
      "step": 22150
    },
    {
      "epoch": 18.54130934950847,
      "grad_norm": 24.027341842651367,
      "learning_rate": 2.2737970711297072e-06,
      "loss": 0.4921,
      "step": 22175
    },
    {
      "epoch": 18.562225475841874,
      "grad_norm": 19.47201156616211,
      "learning_rate": 2.241108786610879e-06,
      "loss": 0.45,
      "step": 22200
    },
    {
      "epoch": 18.583141602175278,
      "grad_norm": 28.643985748291016,
      "learning_rate": 2.2084205020920504e-06,
      "loss": 0.5217,
      "step": 22225
    },
    {
      "epoch": 18.604057728508682,
      "grad_norm": 15.030765533447266,
      "learning_rate": 2.1757322175732216e-06,
      "loss": 0.5597,
      "step": 22250
    },
    {
      "epoch": 18.624973854842082,
      "grad_norm": 36.49744415283203,
      "learning_rate": 2.143043933054393e-06,
      "loss": 0.5281,
      "step": 22275
    },
    {
      "epoch": 18.645889981175486,
      "grad_norm": 28.316478729248047,
      "learning_rate": 2.1103556485355647e-06,
      "loss": 0.505,
      "step": 22300
    },
    {
      "epoch": 18.66680610750889,
      "grad_norm": 17.58763885498047,
      "learning_rate": 2.0776673640167363e-06,
      "loss": 0.4846,
      "step": 22325
    },
    {
      "epoch": 18.687722233842294,
      "grad_norm": 20.062408447265625,
      "learning_rate": 2.0449790794979083e-06,
      "loss": 0.4788,
      "step": 22350
    },
    {
      "epoch": 18.708638360175694,
      "grad_norm": 20.64087677001953,
      "learning_rate": 2.01229079497908e-06,
      "loss": 0.5556,
      "step": 22375
    },
    {
      "epoch": 18.729554486509098,
      "grad_norm": 27.992212295532227,
      "learning_rate": 1.979602510460251e-06,
      "loss": 0.4912,
      "step": 22400
    },
    {
      "epoch": 18.750470612842502,
      "grad_norm": 21.962934494018555,
      "learning_rate": 1.9469142259414227e-06,
      "loss": 0.5394,
      "step": 22425
    },
    {
      "epoch": 18.771386739175906,
      "grad_norm": 26.237836837768555,
      "learning_rate": 1.9142259414225943e-06,
      "loss": 0.5671,
      "step": 22450
    },
    {
      "epoch": 18.792302865509306,
      "grad_norm": 21.27305030822754,
      "learning_rate": 1.8815376569037659e-06,
      "loss": 0.5359,
      "step": 22475
    },
    {
      "epoch": 18.81321899184271,
      "grad_norm": 23.69489097595215,
      "learning_rate": 1.8488493723849372e-06,
      "loss": 0.5352,
      "step": 22500
    },
    {
      "epoch": 18.834135118176114,
      "grad_norm": 31.64430809020996,
      "learning_rate": 1.8161610878661088e-06,
      "loss": 0.4932,
      "step": 22525
    },
    {
      "epoch": 18.855051244509518,
      "grad_norm": 20.20417022705078,
      "learning_rate": 1.7834728033472804e-06,
      "loss": 0.4821,
      "step": 22550
    },
    {
      "epoch": 18.87596737084292,
      "grad_norm": 19.016571044921875,
      "learning_rate": 1.7507845188284518e-06,
      "loss": 0.5511,
      "step": 22575
    },
    {
      "epoch": 18.896883497176322,
      "grad_norm": 26.314260482788086,
      "learning_rate": 1.7180962343096236e-06,
      "loss": 0.5399,
      "step": 22600
    },
    {
      "epoch": 18.917799623509726,
      "grad_norm": 23.507892608642578,
      "learning_rate": 1.6854079497907952e-06,
      "loss": 0.4942,
      "step": 22625
    },
    {
      "epoch": 18.93871574984313,
      "grad_norm": 30.542518615722656,
      "learning_rate": 1.6527196652719666e-06,
      "loss": 0.6056,
      "step": 22650
    },
    {
      "epoch": 18.95963187617653,
      "grad_norm": 26.303239822387695,
      "learning_rate": 1.6200313807531381e-06,
      "loss": 0.5248,
      "step": 22675
    },
    {
      "epoch": 18.980548002509934,
      "grad_norm": 25.174015045166016,
      "learning_rate": 1.5873430962343097e-06,
      "loss": 0.5419,
      "step": 22700
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.45625895261764526,
      "eval_runtime": 8.5102,
      "eval_samples_per_second": 1997.609,
      "eval_steps_per_second": 62.513,
      "step": 22724
    },
    {
      "epoch": 19.000836645053337,
      "grad_norm": 28.614791870117188,
      "learning_rate": 1.554654811715481e-06,
      "loss": 0.5153,
      "step": 22725
    },
    {
      "epoch": 19.02175277138674,
      "grad_norm": 18.535499572753906,
      "learning_rate": 1.5219665271966527e-06,
      "loss": 0.5248,
      "step": 22750
    },
    {
      "epoch": 19.04266889772014,
      "grad_norm": 18.312984466552734,
      "learning_rate": 1.4892782426778245e-06,
      "loss": 0.5804,
      "step": 22775
    },
    {
      "epoch": 19.063585024053545,
      "grad_norm": 19.61512565612793,
      "learning_rate": 1.4565899581589959e-06,
      "loss": 0.4666,
      "step": 22800
    },
    {
      "epoch": 19.08450115038695,
      "grad_norm": 18.414592742919922,
      "learning_rate": 1.4239016736401675e-06,
      "loss": 0.5226,
      "step": 22825
    },
    {
      "epoch": 19.105417276720353,
      "grad_norm": 25.701038360595703,
      "learning_rate": 1.391213389121339e-06,
      "loss": 0.4777,
      "step": 22850
    },
    {
      "epoch": 19.126333403053753,
      "grad_norm": 14.75546932220459,
      "learning_rate": 1.3585251046025104e-06,
      "loss": 0.4965,
      "step": 22875
    },
    {
      "epoch": 19.147249529387157,
      "grad_norm": 21.449018478393555,
      "learning_rate": 1.325836820083682e-06,
      "loss": 0.5092,
      "step": 22900
    },
    {
      "epoch": 19.16816565572056,
      "grad_norm": 33.50394058227539,
      "learning_rate": 1.2931485355648536e-06,
      "loss": 0.4561,
      "step": 22925
    },
    {
      "epoch": 19.189081782053965,
      "grad_norm": 25.217628479003906,
      "learning_rate": 1.2604602510460252e-06,
      "loss": 0.5117,
      "step": 22950
    },
    {
      "epoch": 19.209997908387365,
      "grad_norm": 20.975492477416992,
      "learning_rate": 1.2277719665271968e-06,
      "loss": 0.5064,
      "step": 22975
    },
    {
      "epoch": 19.23091403472077,
      "grad_norm": 13.42758846282959,
      "learning_rate": 1.1950836820083684e-06,
      "loss": 0.5275,
      "step": 23000
    },
    {
      "epoch": 19.251830161054173,
      "grad_norm": 21.096017837524414,
      "learning_rate": 1.1623953974895397e-06,
      "loss": 0.5787,
      "step": 23025
    },
    {
      "epoch": 19.272746287387577,
      "grad_norm": 18.421899795532227,
      "learning_rate": 1.1297071129707113e-06,
      "loss": 0.4781,
      "step": 23050
    },
    {
      "epoch": 19.29366241372098,
      "grad_norm": 22.218894958496094,
      "learning_rate": 1.097018828451883e-06,
      "loss": 0.5018,
      "step": 23075
    },
    {
      "epoch": 19.31457854005438,
      "grad_norm": 24.585477828979492,
      "learning_rate": 1.0643305439330543e-06,
      "loss": 0.4826,
      "step": 23100
    },
    {
      "epoch": 19.335494666387785,
      "grad_norm": 22.779071807861328,
      "learning_rate": 1.031642259414226e-06,
      "loss": 0.5979,
      "step": 23125
    },
    {
      "epoch": 19.35641079272119,
      "grad_norm": 21.18541145324707,
      "learning_rate": 9.989539748953975e-07,
      "loss": 0.5208,
      "step": 23150
    },
    {
      "epoch": 19.377326919054592,
      "grad_norm": 22.23956871032715,
      "learning_rate": 9.66265690376569e-07,
      "loss": 0.5574,
      "step": 23175
    },
    {
      "epoch": 19.398243045387993,
      "grad_norm": 33.184844970703125,
      "learning_rate": 9.335774058577406e-07,
      "loss": 0.4696,
      "step": 23200
    },
    {
      "epoch": 19.419159171721397,
      "grad_norm": 25.459131240844727,
      "learning_rate": 9.008891213389121e-07,
      "loss": 0.4809,
      "step": 23225
    },
    {
      "epoch": 19.4400752980548,
      "grad_norm": 23.680206298828125,
      "learning_rate": 8.682008368200837e-07,
      "loss": 0.5068,
      "step": 23250
    },
    {
      "epoch": 19.460991424388205,
      "grad_norm": 25.565784454345703,
      "learning_rate": 8.355125523012553e-07,
      "loss": 0.5357,
      "step": 23275
    },
    {
      "epoch": 19.481907550721605,
      "grad_norm": 25.76517677307129,
      "learning_rate": 8.028242677824268e-07,
      "loss": 0.5165,
      "step": 23300
    },
    {
      "epoch": 19.50282367705501,
      "grad_norm": 30.048852920532227,
      "learning_rate": 7.701359832635984e-07,
      "loss": 0.5535,
      "step": 23325
    },
    {
      "epoch": 19.523739803388413,
      "grad_norm": 25.394712448120117,
      "learning_rate": 7.3744769874477e-07,
      "loss": 0.5568,
      "step": 23350
    },
    {
      "epoch": 19.544655929721817,
      "grad_norm": 36.20779800415039,
      "learning_rate": 7.047594142259414e-07,
      "loss": 0.531,
      "step": 23375
    },
    {
      "epoch": 19.565572056055217,
      "grad_norm": 18.781293869018555,
      "learning_rate": 6.720711297071129e-07,
      "loss": 0.4597,
      "step": 23400
    },
    {
      "epoch": 19.58648818238862,
      "grad_norm": 22.677976608276367,
      "learning_rate": 6.393828451882845e-07,
      "loss": 0.5423,
      "step": 23425
    },
    {
      "epoch": 19.607404308722025,
      "grad_norm": 20.418485641479492,
      "learning_rate": 6.066945606694561e-07,
      "loss": 0.5408,
      "step": 23450
    },
    {
      "epoch": 19.62832043505543,
      "grad_norm": 26.792762756347656,
      "learning_rate": 5.740062761506276e-07,
      "loss": 0.532,
      "step": 23475
    },
    {
      "epoch": 19.649236561388832,
      "grad_norm": 28.78705406188965,
      "learning_rate": 5.413179916317992e-07,
      "loss": 0.5067,
      "step": 23500
    },
    {
      "epoch": 19.670152687722233,
      "grad_norm": 26.07322120666504,
      "learning_rate": 5.086297071129708e-07,
      "loss": 0.5743,
      "step": 23525
    },
    {
      "epoch": 19.691068814055637,
      "grad_norm": 22.6095027923584,
      "learning_rate": 4.7594142259414224e-07,
      "loss": 0.5698,
      "step": 23550
    },
    {
      "epoch": 19.71198494038904,
      "grad_norm": 21.89048957824707,
      "learning_rate": 4.4325313807531383e-07,
      "loss": 0.4768,
      "step": 23575
    },
    {
      "epoch": 19.732901066722444,
      "grad_norm": 22.7934627532959,
      "learning_rate": 4.1056485355648537e-07,
      "loss": 0.4896,
      "step": 23600
    },
    {
      "epoch": 19.753817193055845,
      "grad_norm": 26.940410614013672,
      "learning_rate": 3.778765690376569e-07,
      "loss": 0.5015,
      "step": 23625
    },
    {
      "epoch": 19.77473331938925,
      "grad_norm": 24.33913803100586,
      "learning_rate": 3.451882845188285e-07,
      "loss": 0.4974,
      "step": 23650
    },
    {
      "epoch": 19.795649445722653,
      "grad_norm": 21.208017349243164,
      "learning_rate": 3.1249999999999997e-07,
      "loss": 0.5141,
      "step": 23675
    },
    {
      "epoch": 19.816565572056057,
      "grad_norm": 25.184255599975586,
      "learning_rate": 2.7981171548117156e-07,
      "loss": 0.4839,
      "step": 23700
    },
    {
      "epoch": 19.837481698389457,
      "grad_norm": 18.547147750854492,
      "learning_rate": 2.4712343096234315e-07,
      "loss": 0.4741,
      "step": 23725
    },
    {
      "epoch": 19.85839782472286,
      "grad_norm": 24.926773071289062,
      "learning_rate": 2.1443514644351463e-07,
      "loss": 0.5635,
      "step": 23750
    },
    {
      "epoch": 19.879313951056265,
      "grad_norm": 20.497987747192383,
      "learning_rate": 1.817468619246862e-07,
      "loss": 0.5305,
      "step": 23775
    },
    {
      "epoch": 19.90023007738967,
      "grad_norm": 20.378459930419922,
      "learning_rate": 1.503661087866109e-07,
      "loss": 0.5663,
      "step": 23800
    },
    {
      "epoch": 19.92114620372307,
      "grad_norm": 14.540600776672363,
      "learning_rate": 1.1767782426778243e-07,
      "loss": 0.5065,
      "step": 23825
    },
    {
      "epoch": 19.942062330056473,
      "grad_norm": 22.42496681213379,
      "learning_rate": 8.498953974895397e-08,
      "loss": 0.4644,
      "step": 23850
    },
    {
      "epoch": 19.962978456389877,
      "grad_norm": 24.50031089782715,
      "learning_rate": 5.230125523012552e-08,
      "loss": 0.4833,
      "step": 23875
    },
    {
      "epoch": 19.98389458272328,
      "grad_norm": 18.220788955688477,
      "learning_rate": 1.961297071129707e-08,
      "loss": 0.4515,
      "step": 23900
    }
  ],
  "logging_steps": 25,
  "max_steps": 23900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
