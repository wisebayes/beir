{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.992261033256641,
  "eval_steps": 500,
  "global_step": 11950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020916126333403055,
      "grad_norm": 67741.1015625,
      "learning_rate": 3.451882845188285e-07,
      "loss": 7287.04,
      "step": 25
    },
    {
      "epoch": 0.04183225266680611,
      "grad_norm": 72766.515625,
      "learning_rate": 1.1297071129707113e-06,
      "loss": 6615.5337,
      "step": 50
    },
    {
      "epoch": 0.06274837900020916,
      "grad_norm": 49765.94921875,
      "learning_rate": 1.9142259414225943e-06,
      "loss": 5165.5994,
      "step": 75
    },
    {
      "epoch": 0.08366450533361222,
      "grad_norm": 26185.6015625,
      "learning_rate": 2.698744769874477e-06,
      "loss": 3156.7488,
      "step": 100
    },
    {
      "epoch": 0.10458063166701527,
      "grad_norm": 9233.1181640625,
      "learning_rate": 3.4832635983263597e-06,
      "loss": 1752.7506,
      "step": 125
    },
    {
      "epoch": 0.12549675800041832,
      "grad_norm": 5158.65673828125,
      "learning_rate": 4.267782426778243e-06,
      "loss": 922.3852,
      "step": 150
    },
    {
      "epoch": 0.14641288433382138,
      "grad_norm": 2747.746826171875,
      "learning_rate": 5.052301255230126e-06,
      "loss": 468.4091,
      "step": 175
    },
    {
      "epoch": 0.16732901066722444,
      "grad_norm": 1984.7379150390625,
      "learning_rate": 5.836820083682009e-06,
      "loss": 252.4444,
      "step": 200
    },
    {
      "epoch": 0.18824513700062748,
      "grad_norm": 1403.66552734375,
      "learning_rate": 6.6213389121338915e-06,
      "loss": 144.2602,
      "step": 225
    },
    {
      "epoch": 0.20916126333403054,
      "grad_norm": 2478.125,
      "learning_rate": 7.405857740585774e-06,
      "loss": 82.9497,
      "step": 250
    },
    {
      "epoch": 0.2300773896674336,
      "grad_norm": 838.6129760742188,
      "learning_rate": 8.190376569037657e-06,
      "loss": 55.2185,
      "step": 275
    },
    {
      "epoch": 0.25099351600083664,
      "grad_norm": 994.402099609375,
      "learning_rate": 8.974895397489539e-06,
      "loss": 42.692,
      "step": 300
    },
    {
      "epoch": 0.2719096423342397,
      "grad_norm": 913.2096557617188,
      "learning_rate": 9.759414225941422e-06,
      "loss": 34.3373,
      "step": 325
    },
    {
      "epoch": 0.29282576866764276,
      "grad_norm": 545.9105224609375,
      "learning_rate": 1.0543933054393304e-05,
      "loss": 30.8981,
      "step": 350
    },
    {
      "epoch": 0.3137418950010458,
      "grad_norm": 677.25537109375,
      "learning_rate": 1.132845188284519e-05,
      "loss": 26.751,
      "step": 375
    },
    {
      "epoch": 0.3346580213344489,
      "grad_norm": 366.7425537109375,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 23.1121,
      "step": 400
    },
    {
      "epoch": 0.3555741476678519,
      "grad_norm": 329.7328796386719,
      "learning_rate": 1.2897489539748955e-05,
      "loss": 20.071,
      "step": 425
    },
    {
      "epoch": 0.37649027400125495,
      "grad_norm": 502.0028381347656,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 18.7708,
      "step": 450
    },
    {
      "epoch": 0.397406400334658,
      "grad_norm": 330.0120849609375,
      "learning_rate": 1.446652719665272e-05,
      "loss": 17.1152,
      "step": 475
    },
    {
      "epoch": 0.4183225266680611,
      "grad_norm": 283.4311218261719,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 15.2941,
      "step": 500
    },
    {
      "epoch": 0.43923865300146414,
      "grad_norm": 268.18280029296875,
      "learning_rate": 1.6035564853556484e-05,
      "loss": 14.1917,
      "step": 525
    },
    {
      "epoch": 0.4601547793348672,
      "grad_norm": 208.31900024414062,
      "learning_rate": 1.682008368200837e-05,
      "loss": 13.2238,
      "step": 550
    },
    {
      "epoch": 0.4810709056682702,
      "grad_norm": 190.92054748535156,
      "learning_rate": 1.760460251046025e-05,
      "loss": 12.623,
      "step": 575
    },
    {
      "epoch": 0.5019870320016733,
      "grad_norm": 192.88467407226562,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 12.3279,
      "step": 600
    },
    {
      "epoch": 0.5229031583350764,
      "grad_norm": 168.76426696777344,
      "learning_rate": 1.9173640167364015e-05,
      "loss": 11.87,
      "step": 625
    },
    {
      "epoch": 0.5438192846684794,
      "grad_norm": 155.06643676757812,
      "learning_rate": 1.99581589958159e-05,
      "loss": 11.7334,
      "step": 650
    },
    {
      "epoch": 0.5647354110018824,
      "grad_norm": 151.23667907714844,
      "learning_rate": 2.0742677824267782e-05,
      "loss": 11.3767,
      "step": 675
    },
    {
      "epoch": 0.5856515373352855,
      "grad_norm": 135.9052276611328,
      "learning_rate": 2.1527196652719668e-05,
      "loss": 10.7534,
      "step": 700
    },
    {
      "epoch": 0.6065676636686885,
      "grad_norm": 141.75498962402344,
      "learning_rate": 2.2311715481171546e-05,
      "loss": 10.8914,
      "step": 725
    },
    {
      "epoch": 0.6274837900020916,
      "grad_norm": 130.6064910888672,
      "learning_rate": 2.309623430962343e-05,
      "loss": 10.4907,
      "step": 750
    },
    {
      "epoch": 0.6483999163354947,
      "grad_norm": 127.423095703125,
      "learning_rate": 2.3880753138075313e-05,
      "loss": 10.3467,
      "step": 775
    },
    {
      "epoch": 0.6693160426688978,
      "grad_norm": 131.9757843017578,
      "learning_rate": 2.46652719665272e-05,
      "loss": 9.7345,
      "step": 800
    },
    {
      "epoch": 0.6902321690023008,
      "grad_norm": 135.89486694335938,
      "learning_rate": 2.5449790794979077e-05,
      "loss": 9.6149,
      "step": 825
    },
    {
      "epoch": 0.7111482953357038,
      "grad_norm": 114.55039978027344,
      "learning_rate": 2.6234309623430962e-05,
      "loss": 9.3884,
      "step": 850
    },
    {
      "epoch": 0.7320644216691069,
      "grad_norm": 118.17861938476562,
      "learning_rate": 2.7018828451882844e-05,
      "loss": 12.5412,
      "step": 875
    },
    {
      "epoch": 0.7529805480025099,
      "grad_norm": 115.91527557373047,
      "learning_rate": 2.780334728033473e-05,
      "loss": 8.4376,
      "step": 900
    },
    {
      "epoch": 0.773896674335913,
      "grad_norm": 134.81976318359375,
      "learning_rate": 2.858786610878661e-05,
      "loss": 8.1836,
      "step": 925
    },
    {
      "epoch": 0.794812800669316,
      "grad_norm": 113.04315948486328,
      "learning_rate": 2.9372384937238493e-05,
      "loss": 7.7423,
      "step": 950
    },
    {
      "epoch": 0.8157289270027192,
      "grad_norm": 113.45817565917969,
      "learning_rate": 2.9986356194287794e-05,
      "loss": 7.6154,
      "step": 975
    },
    {
      "epoch": 0.8366450533361222,
      "grad_norm": 100.21216583251953,
      "learning_rate": 2.991813716572676e-05,
      "loss": 7.2809,
      "step": 1000
    },
    {
      "epoch": 0.8575611796695252,
      "grad_norm": 117.78143310546875,
      "learning_rate": 2.9849918137165727e-05,
      "loss": 7.0679,
      "step": 1025
    },
    {
      "epoch": 0.8784773060029283,
      "grad_norm": 142.83245849609375,
      "learning_rate": 2.9781699108604695e-05,
      "loss": 6.5942,
      "step": 1050
    },
    {
      "epoch": 0.8993934323363313,
      "grad_norm": 98.55473327636719,
      "learning_rate": 2.9713480080043663e-05,
      "loss": 6.6448,
      "step": 1075
    },
    {
      "epoch": 0.9203095586697344,
      "grad_norm": 123.25533294677734,
      "learning_rate": 2.9645261051482628e-05,
      "loss": 6.4179,
      "step": 1100
    },
    {
      "epoch": 0.9412256850031374,
      "grad_norm": 106.11102294921875,
      "learning_rate": 2.9577042022921592e-05,
      "loss": 6.353,
      "step": 1125
    },
    {
      "epoch": 0.9621418113365404,
      "grad_norm": 105.7027816772461,
      "learning_rate": 2.9508822994360564e-05,
      "loss": 6.1897,
      "step": 1150
    },
    {
      "epoch": 0.9830579376699435,
      "grad_norm": 119.72208404541016,
      "learning_rate": 2.944060396579953e-05,
      "loss": 5.9103,
      "step": 1175
    },
    {
      "epoch": 1.0033465802133446,
      "grad_norm": 91.86458587646484,
      "learning_rate": 2.9372384937238493e-05,
      "loss": 5.7641,
      "step": 1200
    },
    {
      "epoch": 1.0242627065467476,
      "grad_norm": 97.07525634765625,
      "learning_rate": 2.930416590867746e-05,
      "loss": 5.7074,
      "step": 1225
    },
    {
      "epoch": 1.0451788328801506,
      "grad_norm": 352.5340270996094,
      "learning_rate": 2.9235946880116426e-05,
      "loss": 5.6942,
      "step": 1250
    },
    {
      "epoch": 1.0660949592135536,
      "grad_norm": 102.24018096923828,
      "learning_rate": 2.9167727851555394e-05,
      "loss": 7.0173,
      "step": 1275
    },
    {
      "epoch": 1.0870110855469568,
      "grad_norm": 92.02713775634766,
      "learning_rate": 2.9099508822994362e-05,
      "loss": 5.5911,
      "step": 1300
    },
    {
      "epoch": 1.1079272118803598,
      "grad_norm": 84.28057861328125,
      "learning_rate": 2.9031289794433327e-05,
      "loss": 5.2273,
      "step": 1325
    },
    {
      "epoch": 1.1288433382137628,
      "grad_norm": 85.88536834716797,
      "learning_rate": 2.8963070765872295e-05,
      "loss": 5.3551,
      "step": 1350
    },
    {
      "epoch": 1.1497594645471658,
      "grad_norm": 100.56289672851562,
      "learning_rate": 2.8894851737311263e-05,
      "loss": 4.8977,
      "step": 1375
    },
    {
      "epoch": 1.1706755908805688,
      "grad_norm": 78.84574127197266,
      "learning_rate": 2.8826632708750228e-05,
      "loss": 4.8689,
      "step": 1400
    },
    {
      "epoch": 1.191591717213972,
      "grad_norm": 96.45179748535156,
      "learning_rate": 2.8758413680189196e-05,
      "loss": 5.015,
      "step": 1425
    },
    {
      "epoch": 1.212507843547375,
      "grad_norm": 88.38914489746094,
      "learning_rate": 2.869019465162816e-05,
      "loss": 4.7961,
      "step": 1450
    },
    {
      "epoch": 1.233423969880778,
      "grad_norm": 88.90882873535156,
      "learning_rate": 2.8621975623067126e-05,
      "loss": 4.6335,
      "step": 1475
    },
    {
      "epoch": 1.254340096214181,
      "grad_norm": 85.75153350830078,
      "learning_rate": 2.8553756594506097e-05,
      "loss": 4.667,
      "step": 1500
    },
    {
      "epoch": 1.275256222547584,
      "grad_norm": 78.6447982788086,
      "learning_rate": 2.8485537565945062e-05,
      "loss": 4.5171,
      "step": 1525
    },
    {
      "epoch": 1.2961723488809873,
      "grad_norm": 75.14204406738281,
      "learning_rate": 2.841731853738403e-05,
      "loss": 4.4934,
      "step": 1550
    },
    {
      "epoch": 1.3170884752143903,
      "grad_norm": 83.52923583984375,
      "learning_rate": 2.8349099508822995e-05,
      "loss": 4.1949,
      "step": 1575
    },
    {
      "epoch": 1.3380046015477933,
      "grad_norm": 76.23849487304688,
      "learning_rate": 2.828088048026196e-05,
      "loss": 3.9994,
      "step": 1600
    },
    {
      "epoch": 1.3589207278811963,
      "grad_norm": 72.85297393798828,
      "learning_rate": 2.821266145170093e-05,
      "loss": 4.2289,
      "step": 1625
    },
    {
      "epoch": 1.3798368542145996,
      "grad_norm": 84.07281494140625,
      "learning_rate": 2.8144442423139896e-05,
      "loss": 4.0993,
      "step": 1650
    },
    {
      "epoch": 1.4007529805480026,
      "grad_norm": 84.71366119384766,
      "learning_rate": 2.807622339457886e-05,
      "loss": 4.2167,
      "step": 1675
    },
    {
      "epoch": 1.4216691068814056,
      "grad_norm": 85.2173080444336,
      "learning_rate": 2.800800436601783e-05,
      "loss": 4.184,
      "step": 1700
    },
    {
      "epoch": 1.4425852332148086,
      "grad_norm": 84.28860473632812,
      "learning_rate": 2.7939785337456797e-05,
      "loss": 3.9625,
      "step": 1725
    },
    {
      "epoch": 1.4635013595482116,
      "grad_norm": 83.908935546875,
      "learning_rate": 2.787156630889576e-05,
      "loss": 3.8333,
      "step": 1750
    },
    {
      "epoch": 1.4844174858816146,
      "grad_norm": 73.52376556396484,
      "learning_rate": 2.780334728033473e-05,
      "loss": 3.6684,
      "step": 1775
    },
    {
      "epoch": 1.5053336122150178,
      "grad_norm": 80.53265380859375,
      "learning_rate": 2.7735128251773694e-05,
      "loss": 3.9582,
      "step": 1800
    },
    {
      "epoch": 1.5262497385484208,
      "grad_norm": 69.26212310791016,
      "learning_rate": 2.7666909223212662e-05,
      "loss": 4.2164,
      "step": 1825
    },
    {
      "epoch": 1.5471658648818238,
      "grad_norm": 81.6836166381836,
      "learning_rate": 2.759869019465163e-05,
      "loss": 3.8607,
      "step": 1850
    },
    {
      "epoch": 1.568081991215227,
      "grad_norm": 93.35623931884766,
      "learning_rate": 2.7530471166090595e-05,
      "loss": 4.2899,
      "step": 1875
    },
    {
      "epoch": 1.58899811754863,
      "grad_norm": 68.94779205322266,
      "learning_rate": 2.7462252137529563e-05,
      "loss": 3.7762,
      "step": 1900
    },
    {
      "epoch": 1.609914243882033,
      "grad_norm": 84.46380615234375,
      "learning_rate": 2.7394033108968528e-05,
      "loss": 3.6036,
      "step": 1925
    },
    {
      "epoch": 1.630830370215436,
      "grad_norm": 85.85205841064453,
      "learning_rate": 2.7325814080407493e-05,
      "loss": 3.7383,
      "step": 1950
    },
    {
      "epoch": 1.651746496548839,
      "grad_norm": 74.88214874267578,
      "learning_rate": 2.7257595051846464e-05,
      "loss": 3.7085,
      "step": 1975
    },
    {
      "epoch": 1.672662622882242,
      "grad_norm": 79.33343505859375,
      "learning_rate": 2.718937602328543e-05,
      "loss": 3.6817,
      "step": 2000
    },
    {
      "epoch": 1.693578749215645,
      "grad_norm": 70.41914367675781,
      "learning_rate": 2.7121156994724394e-05,
      "loss": 3.3364,
      "step": 2025
    },
    {
      "epoch": 1.7144948755490483,
      "grad_norm": 71.71107482910156,
      "learning_rate": 2.7052937966163362e-05,
      "loss": 3.6588,
      "step": 2050
    },
    {
      "epoch": 1.7354110018824513,
      "grad_norm": 63.416664123535156,
      "learning_rate": 2.698471893760233e-05,
      "loss": 3.6554,
      "step": 2075
    },
    {
      "epoch": 1.7563271282158546,
      "grad_norm": 65.3271484375,
      "learning_rate": 2.6916499909041298e-05,
      "loss": 3.4816,
      "step": 2100
    },
    {
      "epoch": 1.7772432545492576,
      "grad_norm": 75.436767578125,
      "learning_rate": 2.6848280880480263e-05,
      "loss": 3.3675,
      "step": 2125
    },
    {
      "epoch": 1.7981593808826606,
      "grad_norm": 73.21405029296875,
      "learning_rate": 2.6780061851919228e-05,
      "loss": 3.4066,
      "step": 2150
    },
    {
      "epoch": 1.8190755072160636,
      "grad_norm": 85.31965637207031,
      "learning_rate": 2.6711842823358196e-05,
      "loss": 3.3328,
      "step": 2175
    },
    {
      "epoch": 1.8399916335494666,
      "grad_norm": 60.5441780090332,
      "learning_rate": 2.6643623794797164e-05,
      "loss": 3.5218,
      "step": 2200
    },
    {
      "epoch": 1.8609077598828696,
      "grad_norm": 64.27903747558594,
      "learning_rate": 2.657540476623613e-05,
      "loss": 3.3473,
      "step": 2225
    },
    {
      "epoch": 1.8818238862162726,
      "grad_norm": 61.623268127441406,
      "learning_rate": 2.6507185737675097e-05,
      "loss": 3.5968,
      "step": 2250
    },
    {
      "epoch": 1.9027400125496758,
      "grad_norm": 71.64888000488281,
      "learning_rate": 2.643896670911406e-05,
      "loss": 3.2225,
      "step": 2275
    },
    {
      "epoch": 1.9236561388830788,
      "grad_norm": 70.23246765136719,
      "learning_rate": 2.6370747680553033e-05,
      "loss": 3.3081,
      "step": 2300
    },
    {
      "epoch": 1.9445722652164819,
      "grad_norm": 63.2347412109375,
      "learning_rate": 2.6302528651991998e-05,
      "loss": 3.4868,
      "step": 2325
    },
    {
      "epoch": 1.965488391549885,
      "grad_norm": 70.06305694580078,
      "learning_rate": 2.6234309623430962e-05,
      "loss": 3.2818,
      "step": 2350
    },
    {
      "epoch": 1.986404517883288,
      "grad_norm": 71.54434204101562,
      "learning_rate": 2.616609059486993e-05,
      "loss": 3.2902,
      "step": 2375
    },
    {
      "epoch": 2.006693160426689,
      "grad_norm": 66.96988677978516,
      "learning_rate": 2.6097871566308895e-05,
      "loss": 5.2864,
      "step": 2400
    },
    {
      "epoch": 2.027609286760092,
      "grad_norm": 79.91316986083984,
      "learning_rate": 2.6029652537747863e-05,
      "loss": 2.927,
      "step": 2425
    },
    {
      "epoch": 2.048525413093495,
      "grad_norm": 72.91102600097656,
      "learning_rate": 2.596143350918683e-05,
      "loss": 2.9363,
      "step": 2450
    },
    {
      "epoch": 2.069441539426898,
      "grad_norm": 60.375064849853516,
      "learning_rate": 2.5893214480625796e-05,
      "loss": 3.0663,
      "step": 2475
    },
    {
      "epoch": 2.090357665760301,
      "grad_norm": 98.46151733398438,
      "learning_rate": 2.582499545206476e-05,
      "loss": 3.269,
      "step": 2500
    },
    {
      "epoch": 2.111273792093704,
      "grad_norm": 72.8976821899414,
      "learning_rate": 2.575677642350373e-05,
      "loss": 3.024,
      "step": 2525
    },
    {
      "epoch": 2.132189918427107,
      "grad_norm": 58.472206115722656,
      "learning_rate": 2.5688557394942697e-05,
      "loss": 2.9208,
      "step": 2550
    },
    {
      "epoch": 2.15310604476051,
      "grad_norm": 62.195396423339844,
      "learning_rate": 2.5620338366381665e-05,
      "loss": 3.1275,
      "step": 2575
    },
    {
      "epoch": 2.1740221710939136,
      "grad_norm": 66.55767059326172,
      "learning_rate": 2.555211933782063e-05,
      "loss": 2.9161,
      "step": 2600
    },
    {
      "epoch": 2.1949382974273166,
      "grad_norm": 63.188926696777344,
      "learning_rate": 2.5483900309259595e-05,
      "loss": 2.8242,
      "step": 2625
    },
    {
      "epoch": 2.2158544237607196,
      "grad_norm": 67.15054321289062,
      "learning_rate": 2.5415681280698566e-05,
      "loss": 2.8874,
      "step": 2650
    },
    {
      "epoch": 2.2367705500941226,
      "grad_norm": 57.62037658691406,
      "learning_rate": 2.534746225213753e-05,
      "loss": 3.0466,
      "step": 2675
    },
    {
      "epoch": 2.2576866764275256,
      "grad_norm": 62.377071380615234,
      "learning_rate": 2.5279243223576496e-05,
      "loss": 2.6925,
      "step": 2700
    },
    {
      "epoch": 2.2786028027609286,
      "grad_norm": 69.86072540283203,
      "learning_rate": 2.5211024195015464e-05,
      "loss": 2.8583,
      "step": 2725
    },
    {
      "epoch": 2.2995189290943316,
      "grad_norm": 55.09020233154297,
      "learning_rate": 2.514280516645443e-05,
      "loss": 2.7241,
      "step": 2750
    },
    {
      "epoch": 2.3204350554277346,
      "grad_norm": 75.33358764648438,
      "learning_rate": 2.5074586137893397e-05,
      "loss": 2.6911,
      "step": 2775
    },
    {
      "epoch": 2.3413511817611377,
      "grad_norm": 49.993587493896484,
      "learning_rate": 2.5006367109332365e-05,
      "loss": 2.7545,
      "step": 2800
    },
    {
      "epoch": 2.362267308094541,
      "grad_norm": 68.30598449707031,
      "learning_rate": 2.493814808077133e-05,
      "loss": 2.7682,
      "step": 2825
    },
    {
      "epoch": 2.383183434427944,
      "grad_norm": 57.63652420043945,
      "learning_rate": 2.4869929052210298e-05,
      "loss": 2.7545,
      "step": 2850
    },
    {
      "epoch": 2.404099560761347,
      "grad_norm": 65.20864868164062,
      "learning_rate": 2.4801710023649262e-05,
      "loss": 2.5287,
      "step": 2875
    },
    {
      "epoch": 2.42501568709475,
      "grad_norm": 66.06903839111328,
      "learning_rate": 2.473349099508823e-05,
      "loss": 2.6729,
      "step": 2900
    },
    {
      "epoch": 2.445931813428153,
      "grad_norm": 66.5090103149414,
      "learning_rate": 2.46652719665272e-05,
      "loss": 2.7219,
      "step": 2925
    },
    {
      "epoch": 2.466847939761556,
      "grad_norm": 66.71720123291016,
      "learning_rate": 2.4597052937966163e-05,
      "loss": 2.6623,
      "step": 2950
    },
    {
      "epoch": 2.487764066094959,
      "grad_norm": 61.08208084106445,
      "learning_rate": 2.4528833909405128e-05,
      "loss": 2.6815,
      "step": 2975
    },
    {
      "epoch": 2.508680192428362,
      "grad_norm": 69.13338470458984,
      "learning_rate": 2.44606148808441e-05,
      "loss": 2.7268,
      "step": 3000
    },
    {
      "epoch": 2.529596318761765,
      "grad_norm": 61.65825271606445,
      "learning_rate": 2.4392395852283064e-05,
      "loss": 2.7212,
      "step": 3025
    },
    {
      "epoch": 2.550512445095168,
      "grad_norm": 66.42776489257812,
      "learning_rate": 2.4324176823722032e-05,
      "loss": 2.7546,
      "step": 3050
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 67.78575897216797,
      "learning_rate": 2.4255957795160997e-05,
      "loss": 2.5527,
      "step": 3075
    },
    {
      "epoch": 2.5923446977619746,
      "grad_norm": 65.08406829833984,
      "learning_rate": 2.4187738766599962e-05,
      "loss": 2.6804,
      "step": 3100
    },
    {
      "epoch": 2.6132608240953776,
      "grad_norm": 63.796688079833984,
      "learning_rate": 2.4119519738038933e-05,
      "loss": 2.7775,
      "step": 3125
    },
    {
      "epoch": 2.6341769504287806,
      "grad_norm": 71.74921417236328,
      "learning_rate": 2.4051300709477898e-05,
      "loss": 2.6498,
      "step": 3150
    },
    {
      "epoch": 2.6550930767621836,
      "grad_norm": 66.76114654541016,
      "learning_rate": 2.3983081680916863e-05,
      "loss": 2.5718,
      "step": 3175
    },
    {
      "epoch": 2.6760092030955867,
      "grad_norm": 73.06959533691406,
      "learning_rate": 2.391486265235583e-05,
      "loss": 2.6391,
      "step": 3200
    },
    {
      "epoch": 2.6969253294289897,
      "grad_norm": 62.74623489379883,
      "learning_rate": 2.38466436237948e-05,
      "loss": 2.7538,
      "step": 3225
    },
    {
      "epoch": 2.7178414557623927,
      "grad_norm": 65.4990005493164,
      "learning_rate": 2.3778424595233764e-05,
      "loss": 2.6966,
      "step": 3250
    },
    {
      "epoch": 2.738757582095796,
      "grad_norm": 70.44099426269531,
      "learning_rate": 2.3710205566672732e-05,
      "loss": 2.4594,
      "step": 3275
    },
    {
      "epoch": 2.759673708429199,
      "grad_norm": 59.5145263671875,
      "learning_rate": 2.3641986538111697e-05,
      "loss": 2.6976,
      "step": 3300
    },
    {
      "epoch": 2.780589834762602,
      "grad_norm": 61.16000747680664,
      "learning_rate": 2.3573767509550665e-05,
      "loss": 2.486,
      "step": 3325
    },
    {
      "epoch": 2.801505961096005,
      "grad_norm": 59.61339569091797,
      "learning_rate": 2.3505548480989633e-05,
      "loss": 2.6426,
      "step": 3350
    },
    {
      "epoch": 2.822422087429408,
      "grad_norm": 62.670963287353516,
      "learning_rate": 2.3437329452428598e-05,
      "loss": 2.5044,
      "step": 3375
    },
    {
      "epoch": 2.843338213762811,
      "grad_norm": 61.46115493774414,
      "learning_rate": 2.3369110423867566e-05,
      "loss": 2.6027,
      "step": 3400
    },
    {
      "epoch": 2.864254340096214,
      "grad_norm": 54.16838836669922,
      "learning_rate": 2.330089139530653e-05,
      "loss": 2.4501,
      "step": 3425
    },
    {
      "epoch": 2.885170466429617,
      "grad_norm": 53.886959075927734,
      "learning_rate": 2.3232672366745495e-05,
      "loss": 2.47,
      "step": 3450
    },
    {
      "epoch": 2.90608659276302,
      "grad_norm": 68.81365203857422,
      "learning_rate": 2.3164453338184467e-05,
      "loss": 2.5113,
      "step": 3475
    },
    {
      "epoch": 2.927002719096423,
      "grad_norm": 51.649085998535156,
      "learning_rate": 2.309623430962343e-05,
      "loss": 3.4089,
      "step": 3500
    },
    {
      "epoch": 2.947918845429826,
      "grad_norm": 49.82494354248047,
      "learning_rate": 2.30280152810624e-05,
      "loss": 2.5679,
      "step": 3525
    },
    {
      "epoch": 2.968834971763229,
      "grad_norm": 61.10314178466797,
      "learning_rate": 2.2959796252501364e-05,
      "loss": 2.585,
      "step": 3550
    },
    {
      "epoch": 2.9897510980966326,
      "grad_norm": 66.60240173339844,
      "learning_rate": 2.2891577223940332e-05,
      "loss": 2.3694,
      "step": 3575
    },
    {
      "epoch": 3.0100397406400337,
      "grad_norm": 54.73857498168945,
      "learning_rate": 2.28233581953793e-05,
      "loss": 2.2647,
      "step": 3600
    },
    {
      "epoch": 3.0309558669734367,
      "grad_norm": 55.224609375,
      "learning_rate": 2.2755139166818265e-05,
      "loss": 2.2014,
      "step": 3625
    },
    {
      "epoch": 3.0518719933068397,
      "grad_norm": 58.642539978027344,
      "learning_rate": 2.268692013825723e-05,
      "loss": 2.3167,
      "step": 3650
    },
    {
      "epoch": 3.0727881196402427,
      "grad_norm": 60.71620559692383,
      "learning_rate": 2.2618701109696198e-05,
      "loss": 2.4738,
      "step": 3675
    },
    {
      "epoch": 3.0937042459736457,
      "grad_norm": 59.49321746826172,
      "learning_rate": 2.2550482081135166e-05,
      "loss": 2.4334,
      "step": 3700
    },
    {
      "epoch": 3.1146203723070487,
      "grad_norm": 57.208518981933594,
      "learning_rate": 2.248226305257413e-05,
      "loss": 2.3451,
      "step": 3725
    },
    {
      "epoch": 3.1355364986404517,
      "grad_norm": 56.84392547607422,
      "learning_rate": 2.24140440240131e-05,
      "loss": 2.2832,
      "step": 3750
    },
    {
      "epoch": 3.1564526249738547,
      "grad_norm": 50.91781234741211,
      "learning_rate": 2.2345824995452064e-05,
      "loss": 2.1955,
      "step": 3775
    },
    {
      "epoch": 3.1773687513072577,
      "grad_norm": 59.30448532104492,
      "learning_rate": 2.2277605966891035e-05,
      "loss": 2.1188,
      "step": 3800
    },
    {
      "epoch": 3.1982848776406607,
      "grad_norm": 56.927818298339844,
      "learning_rate": 2.220938693833e-05,
      "loss": 2.3287,
      "step": 3825
    },
    {
      "epoch": 3.219201003974064,
      "grad_norm": 62.37499237060547,
      "learning_rate": 2.2141167909768965e-05,
      "loss": 2.4039,
      "step": 3850
    },
    {
      "epoch": 3.240117130307467,
      "grad_norm": 59.23997497558594,
      "learning_rate": 2.2072948881207933e-05,
      "loss": 2.3324,
      "step": 3875
    },
    {
      "epoch": 3.26103325664087,
      "grad_norm": 67.33534240722656,
      "learning_rate": 2.2004729852646898e-05,
      "loss": 5.3639,
      "step": 3900
    },
    {
      "epoch": 3.281949382974273,
      "grad_norm": 52.23113250732422,
      "learning_rate": 2.1936510824085866e-05,
      "loss": 2.294,
      "step": 3925
    },
    {
      "epoch": 3.302865509307676,
      "grad_norm": 67.4162826538086,
      "learning_rate": 2.1868291795524834e-05,
      "loss": 2.2803,
      "step": 3950
    },
    {
      "epoch": 3.323781635641079,
      "grad_norm": 57.0797233581543,
      "learning_rate": 2.18000727669638e-05,
      "loss": 2.3673,
      "step": 3975
    },
    {
      "epoch": 3.344697761974482,
      "grad_norm": 58.53456115722656,
      "learning_rate": 2.1731853738402763e-05,
      "loss": 2.367,
      "step": 4000
    },
    {
      "epoch": 3.365613888307885,
      "grad_norm": 49.73225021362305,
      "learning_rate": 2.166363470984173e-05,
      "loss": 2.2887,
      "step": 4025
    },
    {
      "epoch": 3.3865300146412887,
      "grad_norm": 57.04621887207031,
      "learning_rate": 2.15954156812807e-05,
      "loss": 2.1779,
      "step": 4050
    },
    {
      "epoch": 3.4074461409746917,
      "grad_norm": 62.05523681640625,
      "learning_rate": 2.1527196652719668e-05,
      "loss": 2.1668,
      "step": 4075
    },
    {
      "epoch": 3.4283622673080947,
      "grad_norm": 51.72616195678711,
      "learning_rate": 2.1458977624158632e-05,
      "loss": 2.2043,
      "step": 4100
    },
    {
      "epoch": 3.4492783936414977,
      "grad_norm": 60.79533767700195,
      "learning_rate": 2.1390758595597597e-05,
      "loss": 2.2458,
      "step": 4125
    },
    {
      "epoch": 3.4701945199749007,
      "grad_norm": 61.56719970703125,
      "learning_rate": 2.132253956703657e-05,
      "loss": 2.0272,
      "step": 4150
    },
    {
      "epoch": 3.4911106463083037,
      "grad_norm": 49.4012336730957,
      "learning_rate": 2.1254320538475533e-05,
      "loss": 2.2069,
      "step": 4175
    },
    {
      "epoch": 3.5120267726417067,
      "grad_norm": 55.048336029052734,
      "learning_rate": 2.1186101509914498e-05,
      "loss": 2.1971,
      "step": 4200
    },
    {
      "epoch": 3.5329428989751097,
      "grad_norm": 61.69633865356445,
      "learning_rate": 2.1117882481353466e-05,
      "loss": 2.1678,
      "step": 4225
    },
    {
      "epoch": 3.5538590253085127,
      "grad_norm": 64.47138214111328,
      "learning_rate": 2.104966345279243e-05,
      "loss": 2.2503,
      "step": 4250
    },
    {
      "epoch": 3.5747751516419157,
      "grad_norm": 64.2502212524414,
      "learning_rate": 2.0981444424231402e-05,
      "loss": 2.1468,
      "step": 4275
    },
    {
      "epoch": 3.5956912779753187,
      "grad_norm": 50.064918518066406,
      "learning_rate": 2.0913225395670367e-05,
      "loss": 2.1491,
      "step": 4300
    },
    {
      "epoch": 3.616607404308722,
      "grad_norm": 65.37407684326172,
      "learning_rate": 2.0845006367109332e-05,
      "loss": 2.4192,
      "step": 4325
    },
    {
      "epoch": 3.637523530642125,
      "grad_norm": 58.046852111816406,
      "learning_rate": 2.07767873385483e-05,
      "loss": 2.0993,
      "step": 4350
    },
    {
      "epoch": 3.658439656975528,
      "grad_norm": 55.587520599365234,
      "learning_rate": 2.0708568309987265e-05,
      "loss": 2.2519,
      "step": 4375
    },
    {
      "epoch": 3.679355783308931,
      "grad_norm": 54.51913070678711,
      "learning_rate": 2.0640349281426233e-05,
      "loss": 2.2806,
      "step": 4400
    },
    {
      "epoch": 3.700271909642334,
      "grad_norm": 58.59818649291992,
      "learning_rate": 2.05721302528652e-05,
      "loss": 2.0965,
      "step": 4425
    },
    {
      "epoch": 3.721188035975737,
      "grad_norm": 66.50033569335938,
      "learning_rate": 2.0503911224304166e-05,
      "loss": 2.2136,
      "step": 4450
    },
    {
      "epoch": 3.7421041623091402,
      "grad_norm": 67.06180572509766,
      "learning_rate": 2.043569219574313e-05,
      "loss": 2.1663,
      "step": 4475
    },
    {
      "epoch": 3.7630202886425437,
      "grad_norm": 56.13823699951172,
      "learning_rate": 2.0367473167182102e-05,
      "loss": 2.0991,
      "step": 4500
    },
    {
      "epoch": 3.7839364149759467,
      "grad_norm": 60.737579345703125,
      "learning_rate": 2.0299254138621067e-05,
      "loss": 2.2654,
      "step": 4525
    },
    {
      "epoch": 3.8048525413093497,
      "grad_norm": 61.34425354003906,
      "learning_rate": 2.0231035110060035e-05,
      "loss": 2.3528,
      "step": 4550
    },
    {
      "epoch": 3.8257686676427527,
      "grad_norm": 56.584991455078125,
      "learning_rate": 2.0162816081499e-05,
      "loss": 2.0346,
      "step": 4575
    },
    {
      "epoch": 3.8466847939761557,
      "grad_norm": 51.0062141418457,
      "learning_rate": 2.0094597052937964e-05,
      "loss": 2.0205,
      "step": 4600
    },
    {
      "epoch": 3.8676009203095587,
      "grad_norm": 54.52594757080078,
      "learning_rate": 2.0026378024376936e-05,
      "loss": 2.0359,
      "step": 4625
    },
    {
      "epoch": 3.8885170466429617,
      "grad_norm": 58.97701644897461,
      "learning_rate": 1.99581589958159e-05,
      "loss": 2.0412,
      "step": 4650
    },
    {
      "epoch": 3.9094331729763647,
      "grad_norm": 60.79568862915039,
      "learning_rate": 1.9889939967254865e-05,
      "loss": 2.0971,
      "step": 4675
    },
    {
      "epoch": 3.9303492993097677,
      "grad_norm": 54.012001037597656,
      "learning_rate": 1.9821720938693833e-05,
      "loss": 2.0284,
      "step": 4700
    },
    {
      "epoch": 3.9512654256431707,
      "grad_norm": 58.06962585449219,
      "learning_rate": 1.97535019101328e-05,
      "loss": 2.1545,
      "step": 4725
    },
    {
      "epoch": 3.9721815519765737,
      "grad_norm": 50.77542495727539,
      "learning_rate": 1.9685282881571766e-05,
      "loss": 2.247,
      "step": 4750
    },
    {
      "epoch": 3.9930976783099768,
      "grad_norm": 72.9944839477539,
      "learning_rate": 1.9617063853010734e-05,
      "loss": 2.0696,
      "step": 4775
    },
    {
      "epoch": 4.013386320853378,
      "grad_norm": 51.878108978271484,
      "learning_rate": 1.95488448244497e-05,
      "loss": 2.0553,
      "step": 4800
    },
    {
      "epoch": 4.034302447186781,
      "grad_norm": 57.73035430908203,
      "learning_rate": 1.9480625795888667e-05,
      "loss": 1.847,
      "step": 4825
    },
    {
      "epoch": 4.055218573520184,
      "grad_norm": 72.26432037353516,
      "learning_rate": 1.9412406767327635e-05,
      "loss": 2.1437,
      "step": 4850
    },
    {
      "epoch": 4.076134699853587,
      "grad_norm": 56.7169303894043,
      "learning_rate": 1.93441877387666e-05,
      "loss": 1.9105,
      "step": 4875
    },
    {
      "epoch": 4.09705082618699,
      "grad_norm": 61.49849319458008,
      "learning_rate": 1.9275968710205568e-05,
      "loss": 1.9564,
      "step": 4900
    },
    {
      "epoch": 4.117966952520393,
      "grad_norm": 57.233551025390625,
      "learning_rate": 1.9207749681644533e-05,
      "loss": 1.8703,
      "step": 4925
    },
    {
      "epoch": 4.138883078853796,
      "grad_norm": 43.54296875,
      "learning_rate": 1.9139530653083498e-05,
      "loss": 1.7799,
      "step": 4950
    },
    {
      "epoch": 4.159799205187199,
      "grad_norm": 58.77262878417969,
      "learning_rate": 1.907131162452247e-05,
      "loss": 1.8773,
      "step": 4975
    },
    {
      "epoch": 4.180715331520602,
      "grad_norm": 42.399993896484375,
      "learning_rate": 1.9003092595961434e-05,
      "loss": 2.0439,
      "step": 5000
    },
    {
      "epoch": 4.201631457854005,
      "grad_norm": 49.82337951660156,
      "learning_rate": 1.8934873567400402e-05,
      "loss": 1.7843,
      "step": 5025
    },
    {
      "epoch": 4.222547584187408,
      "grad_norm": 48.21440887451172,
      "learning_rate": 1.8866654538839367e-05,
      "loss": 1.8648,
      "step": 5050
    },
    {
      "epoch": 4.243463710520811,
      "grad_norm": 63.015281677246094,
      "learning_rate": 1.8798435510278335e-05,
      "loss": 1.8777,
      "step": 5075
    },
    {
      "epoch": 4.264379836854214,
      "grad_norm": 67.22020721435547,
      "learning_rate": 1.8730216481717303e-05,
      "loss": 1.9631,
      "step": 5100
    },
    {
      "epoch": 4.285295963187617,
      "grad_norm": 54.28640365600586,
      "learning_rate": 1.8661997453156268e-05,
      "loss": 1.9279,
      "step": 5125
    },
    {
      "epoch": 4.30621208952102,
      "grad_norm": 43.87191390991211,
      "learning_rate": 1.8593778424595232e-05,
      "loss": 1.9024,
      "step": 5150
    },
    {
      "epoch": 4.327128215854424,
      "grad_norm": 61.71999740600586,
      "learning_rate": 1.85255593960342e-05,
      "loss": 1.9858,
      "step": 5175
    },
    {
      "epoch": 4.348044342187827,
      "grad_norm": 60.14115524291992,
      "learning_rate": 1.845734036747317e-05,
      "loss": 1.9682,
      "step": 5200
    },
    {
      "epoch": 4.36896046852123,
      "grad_norm": 65.1064224243164,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 1.932,
      "step": 5225
    },
    {
      "epoch": 4.389876594854633,
      "grad_norm": 50.931007385253906,
      "learning_rate": 1.83209023103511e-05,
      "loss": 1.8638,
      "step": 5250
    },
    {
      "epoch": 4.410792721188036,
      "grad_norm": 50.18727493286133,
      "learning_rate": 1.8252683281790066e-05,
      "loss": 1.9223,
      "step": 5275
    },
    {
      "epoch": 4.431708847521439,
      "grad_norm": 47.49169921875,
      "learning_rate": 1.8184464253229034e-05,
      "loss": 1.8602,
      "step": 5300
    },
    {
      "epoch": 4.452624973854842,
      "grad_norm": 51.037166595458984,
      "learning_rate": 1.8116245224668002e-05,
      "loss": 2.0198,
      "step": 5325
    },
    {
      "epoch": 4.473541100188245,
      "grad_norm": 59.33018493652344,
      "learning_rate": 1.8048026196106967e-05,
      "loss": 1.866,
      "step": 5350
    },
    {
      "epoch": 4.494457226521648,
      "grad_norm": 61.92234802246094,
      "learning_rate": 1.7979807167545935e-05,
      "loss": 1.944,
      "step": 5375
    },
    {
      "epoch": 4.515373352855051,
      "grad_norm": 52.01081466674805,
      "learning_rate": 1.79115881389849e-05,
      "loss": 2.0164,
      "step": 5400
    },
    {
      "epoch": 4.536289479188454,
      "grad_norm": 48.65277862548828,
      "learning_rate": 1.7843369110423868e-05,
      "loss": 1.8726,
      "step": 5425
    },
    {
      "epoch": 4.557205605521857,
      "grad_norm": 56.80657196044922,
      "learning_rate": 1.7775150081862836e-05,
      "loss": 1.9949,
      "step": 5450
    },
    {
      "epoch": 4.57812173185526,
      "grad_norm": 46.5777473449707,
      "learning_rate": 1.77069310533018e-05,
      "loss": 1.8207,
      "step": 5475
    },
    {
      "epoch": 4.599037858188663,
      "grad_norm": 53.037628173828125,
      "learning_rate": 1.763871202474077e-05,
      "loss": 1.8883,
      "step": 5500
    },
    {
      "epoch": 4.619953984522066,
      "grad_norm": 56.12369918823242,
      "learning_rate": 1.7570492996179734e-05,
      "loss": 1.9136,
      "step": 5525
    },
    {
      "epoch": 4.640870110855469,
      "grad_norm": 59.25252914428711,
      "learning_rate": 1.7502273967618702e-05,
      "loss": 1.7779,
      "step": 5550
    },
    {
      "epoch": 4.661786237188872,
      "grad_norm": 54.98756408691406,
      "learning_rate": 1.743405493905767e-05,
      "loss": 1.833,
      "step": 5575
    },
    {
      "epoch": 4.682702363522275,
      "grad_norm": 55.87040328979492,
      "learning_rate": 1.7365835910496635e-05,
      "loss": 1.8151,
      "step": 5600
    },
    {
      "epoch": 4.703618489855678,
      "grad_norm": 54.41866683959961,
      "learning_rate": 1.72976168819356e-05,
      "loss": 1.646,
      "step": 5625
    },
    {
      "epoch": 4.724534616189082,
      "grad_norm": 60.57396697998047,
      "learning_rate": 1.722939785337457e-05,
      "loss": 1.8242,
      "step": 5650
    },
    {
      "epoch": 4.745450742522485,
      "grad_norm": 64.13805389404297,
      "learning_rate": 1.7161178824813536e-05,
      "loss": 1.7112,
      "step": 5675
    },
    {
      "epoch": 4.766366868855888,
      "grad_norm": 54.9988899230957,
      "learning_rate": 1.70929597962525e-05,
      "loss": 1.894,
      "step": 5700
    },
    {
      "epoch": 4.787282995189291,
      "grad_norm": 60.30537033081055,
      "learning_rate": 1.702474076769147e-05,
      "loss": 1.9538,
      "step": 5725
    },
    {
      "epoch": 4.808199121522694,
      "grad_norm": 46.18692398071289,
      "learning_rate": 1.6956521739130433e-05,
      "loss": 1.8436,
      "step": 5750
    },
    {
      "epoch": 4.829115247856097,
      "grad_norm": 65.3480224609375,
      "learning_rate": 1.6888302710569405e-05,
      "loss": 1.7558,
      "step": 5775
    },
    {
      "epoch": 4.8500313741895,
      "grad_norm": 55.944637298583984,
      "learning_rate": 1.682008368200837e-05,
      "loss": 1.7309,
      "step": 5800
    },
    {
      "epoch": 4.870947500522903,
      "grad_norm": 57.063636779785156,
      "learning_rate": 1.6751864653447334e-05,
      "loss": 1.8367,
      "step": 5825
    },
    {
      "epoch": 4.891863626856306,
      "grad_norm": 55.4058952331543,
      "learning_rate": 1.6683645624886302e-05,
      "loss": 1.7867,
      "step": 5850
    },
    {
      "epoch": 4.912779753189709,
      "grad_norm": 54.63274002075195,
      "learning_rate": 1.6615426596325267e-05,
      "loss": 1.7639,
      "step": 5875
    },
    {
      "epoch": 4.933695879523112,
      "grad_norm": 49.53061294555664,
      "learning_rate": 1.6547207567764235e-05,
      "loss": 1.7479,
      "step": 5900
    },
    {
      "epoch": 4.954612005856515,
      "grad_norm": 46.16130447387695,
      "learning_rate": 1.6478988539203203e-05,
      "loss": 1.796,
      "step": 5925
    },
    {
      "epoch": 4.975528132189918,
      "grad_norm": 46.512535095214844,
      "learning_rate": 1.6410769510642168e-05,
      "loss": 1.7172,
      "step": 5950
    },
    {
      "epoch": 4.996444258523321,
      "grad_norm": 45.71543884277344,
      "learning_rate": 1.6342550482081133e-05,
      "loss": 1.8327,
      "step": 5975
    },
    {
      "epoch": 5.016732901066723,
      "grad_norm": 51.162227630615234,
      "learning_rate": 1.6274331453520104e-05,
      "loss": 1.5956,
      "step": 6000
    },
    {
      "epoch": 5.037649027400126,
      "grad_norm": 68.91793823242188,
      "learning_rate": 1.620611242495907e-05,
      "loss": 1.6949,
      "step": 6025
    },
    {
      "epoch": 5.058565153733529,
      "grad_norm": 54.8924446105957,
      "learning_rate": 1.6137893396398037e-05,
      "loss": 1.6426,
      "step": 6050
    },
    {
      "epoch": 5.079481280066932,
      "grad_norm": 49.65742492675781,
      "learning_rate": 1.6069674367837002e-05,
      "loss": 1.7135,
      "step": 6075
    },
    {
      "epoch": 5.100397406400335,
      "grad_norm": 48.75725173950195,
      "learning_rate": 1.6001455339275967e-05,
      "loss": 1.778,
      "step": 6100
    },
    {
      "epoch": 5.121313532733738,
      "grad_norm": 49.9706916809082,
      "learning_rate": 1.5933236310714938e-05,
      "loss": 1.7899,
      "step": 6125
    },
    {
      "epoch": 5.142229659067141,
      "grad_norm": 50.797882080078125,
      "learning_rate": 1.5865017282153903e-05,
      "loss": 1.5925,
      "step": 6150
    },
    {
      "epoch": 5.163145785400544,
      "grad_norm": 79.15203094482422,
      "learning_rate": 1.5796798253592868e-05,
      "loss": 1.6831,
      "step": 6175
    },
    {
      "epoch": 5.184061911733947,
      "grad_norm": 59.177940368652344,
      "learning_rate": 1.5728579225031836e-05,
      "loss": 1.7188,
      "step": 6200
    },
    {
      "epoch": 5.20497803806735,
      "grad_norm": 49.145267486572266,
      "learning_rate": 1.5660360196470804e-05,
      "loss": 1.5955,
      "step": 6225
    },
    {
      "epoch": 5.225894164400753,
      "grad_norm": 45.75862121582031,
      "learning_rate": 1.5592141167909772e-05,
      "loss": 1.7045,
      "step": 6250
    },
    {
      "epoch": 5.246810290734156,
      "grad_norm": 33.48221206665039,
      "learning_rate": 1.5523922139348737e-05,
      "loss": 1.6516,
      "step": 6275
    },
    {
      "epoch": 5.267726417067559,
      "grad_norm": 45.718711853027344,
      "learning_rate": 1.54557031107877e-05,
      "loss": 1.6744,
      "step": 6300
    },
    {
      "epoch": 5.288642543400962,
      "grad_norm": 40.342529296875,
      "learning_rate": 1.538748408222667e-05,
      "loss": 1.6891,
      "step": 6325
    },
    {
      "epoch": 5.309558669734365,
      "grad_norm": 50.94070816040039,
      "learning_rate": 1.5319265053665638e-05,
      "loss": 1.6751,
      "step": 6350
    },
    {
      "epoch": 5.330474796067768,
      "grad_norm": 45.79512405395508,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 1.5709,
      "step": 6375
    },
    {
      "epoch": 5.351390922401171,
      "grad_norm": 50.992008209228516,
      "learning_rate": 1.5182826996543569e-05,
      "loss": 1.6383,
      "step": 6400
    },
    {
      "epoch": 5.372307048734575,
      "grad_norm": 41.144351959228516,
      "learning_rate": 1.5114607967982535e-05,
      "loss": 1.6177,
      "step": 6425
    },
    {
      "epoch": 5.393223175067978,
      "grad_norm": 45.4742546081543,
      "learning_rate": 1.5046388939421502e-05,
      "loss": 1.6315,
      "step": 6450
    },
    {
      "epoch": 5.414139301401381,
      "grad_norm": 47.33210372924805,
      "learning_rate": 1.497816991086047e-05,
      "loss": 1.6348,
      "step": 6475
    },
    {
      "epoch": 5.435055427734784,
      "grad_norm": 44.551902770996094,
      "learning_rate": 1.4909950882299436e-05,
      "loss": 1.6277,
      "step": 6500
    },
    {
      "epoch": 5.455971554068187,
      "grad_norm": 38.738189697265625,
      "learning_rate": 1.4841731853738403e-05,
      "loss": 1.5968,
      "step": 6525
    },
    {
      "epoch": 5.47688768040159,
      "grad_norm": 54.40837097167969,
      "learning_rate": 1.477351282517737e-05,
      "loss": 1.7194,
      "step": 6550
    },
    {
      "epoch": 5.497803806734993,
      "grad_norm": 56.851829528808594,
      "learning_rate": 1.4705293796616337e-05,
      "loss": 1.6061,
      "step": 6575
    },
    {
      "epoch": 5.518719933068396,
      "grad_norm": 48.022701263427734,
      "learning_rate": 1.4637074768055304e-05,
      "loss": 1.635,
      "step": 6600
    },
    {
      "epoch": 5.539636059401799,
      "grad_norm": 44.06343460083008,
      "learning_rate": 1.456885573949427e-05,
      "loss": 1.5818,
      "step": 6625
    },
    {
      "epoch": 5.560552185735202,
      "grad_norm": 57.97056198120117,
      "learning_rate": 1.4500636710933236e-05,
      "loss": 1.7574,
      "step": 6650
    },
    {
      "epoch": 5.581468312068605,
      "grad_norm": 55.38942337036133,
      "learning_rate": 1.4432417682372205e-05,
      "loss": 1.6267,
      "step": 6675
    },
    {
      "epoch": 5.602384438402008,
      "grad_norm": 48.729557037353516,
      "learning_rate": 1.436419865381117e-05,
      "loss": 1.5892,
      "step": 6700
    },
    {
      "epoch": 5.623300564735411,
      "grad_norm": 42.036502838134766,
      "learning_rate": 1.4295979625250137e-05,
      "loss": 1.5404,
      "step": 6725
    },
    {
      "epoch": 5.644216691068814,
      "grad_norm": 45.44390869140625,
      "learning_rate": 1.4227760596689104e-05,
      "loss": 1.5757,
      "step": 6750
    },
    {
      "epoch": 5.665132817402217,
      "grad_norm": 44.4267578125,
      "learning_rate": 1.415954156812807e-05,
      "loss": 1.6779,
      "step": 6775
    },
    {
      "epoch": 5.68604894373562,
      "grad_norm": 51.84392547607422,
      "learning_rate": 1.4091322539567037e-05,
      "loss": 1.6268,
      "step": 6800
    },
    {
      "epoch": 5.706965070069023,
      "grad_norm": 57.780914306640625,
      "learning_rate": 1.4023103511006003e-05,
      "loss": 1.4921,
      "step": 6825
    },
    {
      "epoch": 5.727881196402427,
      "grad_norm": 52.501182556152344,
      "learning_rate": 1.3954884482444971e-05,
      "loss": 1.4954,
      "step": 6850
    },
    {
      "epoch": 5.748797322735829,
      "grad_norm": 54.5611572265625,
      "learning_rate": 1.3886665453883936e-05,
      "loss": 1.6741,
      "step": 6875
    },
    {
      "epoch": 5.769713449069233,
      "grad_norm": 39.347740173339844,
      "learning_rate": 1.3818446425322904e-05,
      "loss": 1.5385,
      "step": 6900
    },
    {
      "epoch": 5.790629575402636,
      "grad_norm": 54.25774383544922,
      "learning_rate": 1.375022739676187e-05,
      "loss": 1.6107,
      "step": 6925
    },
    {
      "epoch": 5.811545701736039,
      "grad_norm": 60.66078186035156,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 1.6327,
      "step": 6950
    },
    {
      "epoch": 5.832461828069442,
      "grad_norm": 52.629913330078125,
      "learning_rate": 1.3613789339639803e-05,
      "loss": 1.7204,
      "step": 6975
    },
    {
      "epoch": 5.853377954402845,
      "grad_norm": 47.28108215332031,
      "learning_rate": 1.354557031107877e-05,
      "loss": 1.5499,
      "step": 7000
    },
    {
      "epoch": 5.874294080736248,
      "grad_norm": 46.41569900512695,
      "learning_rate": 1.3477351282517738e-05,
      "loss": 1.5566,
      "step": 7025
    },
    {
      "epoch": 5.895210207069651,
      "grad_norm": 45.06227493286133,
      "learning_rate": 1.3409132253956704e-05,
      "loss": 1.6524,
      "step": 7050
    },
    {
      "epoch": 5.916126333403054,
      "grad_norm": 55.87266540527344,
      "learning_rate": 1.334091322539567e-05,
      "loss": 1.5043,
      "step": 7075
    },
    {
      "epoch": 5.937042459736457,
      "grad_norm": 52.98398971557617,
      "learning_rate": 1.3272694196834637e-05,
      "loss": 1.6936,
      "step": 7100
    },
    {
      "epoch": 5.95795858606986,
      "grad_norm": 60.69059371948242,
      "learning_rate": 1.3204475168273605e-05,
      "loss": 1.6063,
      "step": 7125
    },
    {
      "epoch": 5.978874712403263,
      "grad_norm": 46.77740478515625,
      "learning_rate": 1.313625613971257e-05,
      "loss": 1.5914,
      "step": 7150
    },
    {
      "epoch": 5.999790838736666,
      "grad_norm": 54.20994567871094,
      "learning_rate": 1.3068037111151536e-05,
      "loss": 1.5116,
      "step": 7175
    },
    {
      "epoch": 6.020079481280067,
      "grad_norm": 47.62923812866211,
      "learning_rate": 1.2999818082590505e-05,
      "loss": 1.3606,
      "step": 7200
    },
    {
      "epoch": 6.04099560761347,
      "grad_norm": 45.61824035644531,
      "learning_rate": 1.2931599054029471e-05,
      "loss": 1.3704,
      "step": 7225
    },
    {
      "epoch": 6.061911733946873,
      "grad_norm": 188.40133666992188,
      "learning_rate": 1.2863380025468437e-05,
      "loss": 1.467,
      "step": 7250
    },
    {
      "epoch": 6.082827860280276,
      "grad_norm": 56.70905303955078,
      "learning_rate": 1.2795160996907404e-05,
      "loss": 1.4476,
      "step": 7275
    },
    {
      "epoch": 6.103743986613679,
      "grad_norm": 48.44866180419922,
      "learning_rate": 1.2726941968346372e-05,
      "loss": 1.4177,
      "step": 7300
    },
    {
      "epoch": 6.124660112947082,
      "grad_norm": 38.675113677978516,
      "learning_rate": 1.2658722939785338e-05,
      "loss": 1.4575,
      "step": 7325
    },
    {
      "epoch": 6.145576239280485,
      "grad_norm": 47.29785919189453,
      "learning_rate": 1.2590503911224303e-05,
      "loss": 1.4257,
      "step": 7350
    },
    {
      "epoch": 6.166492365613888,
      "grad_norm": 47.30064392089844,
      "learning_rate": 1.2522284882663271e-05,
      "loss": 1.4598,
      "step": 7375
    },
    {
      "epoch": 6.187408491947291,
      "grad_norm": 49.54374694824219,
      "learning_rate": 1.2454065854102238e-05,
      "loss": 1.4693,
      "step": 7400
    },
    {
      "epoch": 6.208324618280694,
      "grad_norm": 53.0892219543457,
      "learning_rate": 1.2385846825541206e-05,
      "loss": 1.4532,
      "step": 7425
    },
    {
      "epoch": 6.229240744614097,
      "grad_norm": 57.32379913330078,
      "learning_rate": 1.231762779698017e-05,
      "loss": 1.5192,
      "step": 7450
    },
    {
      "epoch": 6.2501568709475,
      "grad_norm": 39.37397384643555,
      "learning_rate": 1.2249408768419139e-05,
      "loss": 1.4303,
      "step": 7475
    },
    {
      "epoch": 6.271072997280903,
      "grad_norm": 37.6157341003418,
      "learning_rate": 1.2181189739858105e-05,
      "loss": 1.5257,
      "step": 7500
    },
    {
      "epoch": 6.291989123614306,
      "grad_norm": 47.80873489379883,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 1.4049,
      "step": 7525
    },
    {
      "epoch": 6.312905249947709,
      "grad_norm": 51.43169021606445,
      "learning_rate": 1.2044751682736038e-05,
      "loss": 1.3723,
      "step": 7550
    },
    {
      "epoch": 6.333821376281112,
      "grad_norm": 47.8204460144043,
      "learning_rate": 1.1976532654175004e-05,
      "loss": 1.5183,
      "step": 7575
    },
    {
      "epoch": 6.354737502614515,
      "grad_norm": 52.32658386230469,
      "learning_rate": 1.1908313625613972e-05,
      "loss": 1.3412,
      "step": 7600
    },
    {
      "epoch": 6.375653628947919,
      "grad_norm": 52.503883361816406,
      "learning_rate": 1.1840094597052937e-05,
      "loss": 1.3675,
      "step": 7625
    },
    {
      "epoch": 6.396569755281321,
      "grad_norm": 38.3068962097168,
      "learning_rate": 1.1771875568491905e-05,
      "loss": 1.2623,
      "step": 7650
    },
    {
      "epoch": 6.417485881614725,
      "grad_norm": 57.984214782714844,
      "learning_rate": 1.1703656539930872e-05,
      "loss": 1.4986,
      "step": 7675
    },
    {
      "epoch": 6.438402007948128,
      "grad_norm": 47.56784439086914,
      "learning_rate": 1.163543751136984e-05,
      "loss": 1.4454,
      "step": 7700
    },
    {
      "epoch": 6.459318134281531,
      "grad_norm": 50.82830047607422,
      "learning_rate": 1.1567218482808805e-05,
      "loss": 1.4244,
      "step": 7725
    },
    {
      "epoch": 6.480234260614934,
      "grad_norm": 48.76741409301758,
      "learning_rate": 1.1498999454247771e-05,
      "loss": 1.3917,
      "step": 7750
    },
    {
      "epoch": 6.501150386948337,
      "grad_norm": 50.40700912475586,
      "learning_rate": 1.1430780425686739e-05,
      "loss": 1.4485,
      "step": 7775
    },
    {
      "epoch": 6.52206651328174,
      "grad_norm": 49.41780090332031,
      "learning_rate": 1.1362561397125706e-05,
      "loss": 1.476,
      "step": 7800
    },
    {
      "epoch": 6.542982639615143,
      "grad_norm": 42.989219665527344,
      "learning_rate": 1.1294342368564672e-05,
      "loss": 1.493,
      "step": 7825
    },
    {
      "epoch": 6.563898765948546,
      "grad_norm": 52.32772445678711,
      "learning_rate": 1.1226123340003638e-05,
      "loss": 1.4509,
      "step": 7850
    },
    {
      "epoch": 6.584814892281949,
      "grad_norm": 55.28146743774414,
      "learning_rate": 1.1157904311442606e-05,
      "loss": 1.4726,
      "step": 7875
    },
    {
      "epoch": 6.605731018615352,
      "grad_norm": 42.76512145996094,
      "learning_rate": 1.1089685282881571e-05,
      "loss": 1.446,
      "step": 7900
    },
    {
      "epoch": 6.626647144948755,
      "grad_norm": 42.66310119628906,
      "learning_rate": 1.1021466254320538e-05,
      "loss": 1.4273,
      "step": 7925
    },
    {
      "epoch": 6.647563271282158,
      "grad_norm": 50.09611511230469,
      "learning_rate": 1.0953247225759506e-05,
      "loss": 1.3591,
      "step": 7950
    },
    {
      "epoch": 6.668479397615561,
      "grad_norm": 44.8928108215332,
      "learning_rate": 1.0885028197198472e-05,
      "loss": 1.4276,
      "step": 7975
    },
    {
      "epoch": 6.689395523948964,
      "grad_norm": 50.87517166137695,
      "learning_rate": 1.0816809168637439e-05,
      "loss": 1.4159,
      "step": 8000
    },
    {
      "epoch": 6.710311650282367,
      "grad_norm": 48.281044006347656,
      "learning_rate": 1.0748590140076405e-05,
      "loss": 1.5546,
      "step": 8025
    },
    {
      "epoch": 6.73122777661577,
      "grad_norm": 51.2491455078125,
      "learning_rate": 1.0680371111515373e-05,
      "loss": 1.4129,
      "step": 8050
    },
    {
      "epoch": 6.752143902949173,
      "grad_norm": 48.28797912597656,
      "learning_rate": 1.061215208295434e-05,
      "loss": 1.3446,
      "step": 8075
    },
    {
      "epoch": 6.773060029282577,
      "grad_norm": 52.0809211730957,
      "learning_rate": 1.0543933054393304e-05,
      "loss": 1.3257,
      "step": 8100
    },
    {
      "epoch": 6.7939761556159795,
      "grad_norm": 46.41211700439453,
      "learning_rate": 1.0475714025832272e-05,
      "loss": 1.5271,
      "step": 8125
    },
    {
      "epoch": 6.814892281949383,
      "grad_norm": 46.219791412353516,
      "learning_rate": 1.0407494997271239e-05,
      "loss": 1.4054,
      "step": 8150
    },
    {
      "epoch": 6.835808408282786,
      "grad_norm": 51.6476936340332,
      "learning_rate": 1.0339275968710207e-05,
      "loss": 1.3635,
      "step": 8175
    },
    {
      "epoch": 6.856724534616189,
      "grad_norm": 45.50102996826172,
      "learning_rate": 1.0271056940149172e-05,
      "loss": 1.4694,
      "step": 8200
    },
    {
      "epoch": 6.877640660949592,
      "grad_norm": 44.4399299621582,
      "learning_rate": 1.020283791158814e-05,
      "loss": 1.4291,
      "step": 8225
    },
    {
      "epoch": 6.898556787282995,
      "grad_norm": 42.24668502807617,
      "learning_rate": 1.0134618883027106e-05,
      "loss": 1.4389,
      "step": 8250
    },
    {
      "epoch": 6.919472913616398,
      "grad_norm": 50.35264205932617,
      "learning_rate": 1.0066399854466073e-05,
      "loss": 1.4349,
      "step": 8275
    },
    {
      "epoch": 6.940389039949801,
      "grad_norm": 43.44196319580078,
      "learning_rate": 9.998180825905039e-06,
      "loss": 1.388,
      "step": 8300
    },
    {
      "epoch": 6.961305166283204,
      "grad_norm": 38.04987335205078,
      "learning_rate": 9.929961797344006e-06,
      "loss": 1.3483,
      "step": 8325
    },
    {
      "epoch": 6.982221292616607,
      "grad_norm": 50.19105529785156,
      "learning_rate": 9.861742768782974e-06,
      "loss": 1.434,
      "step": 8350
    },
    {
      "epoch": 7.002509935160008,
      "grad_norm": 55.526126861572266,
      "learning_rate": 9.793523740221938e-06,
      "loss": 1.3971,
      "step": 8375
    },
    {
      "epoch": 7.023426061493412,
      "grad_norm": 46.299415588378906,
      "learning_rate": 9.725304711660906e-06,
      "loss": 1.3085,
      "step": 8400
    },
    {
      "epoch": 7.044342187826815,
      "grad_norm": 56.49745559692383,
      "learning_rate": 9.657085683099873e-06,
      "loss": 1.316,
      "step": 8425
    },
    {
      "epoch": 7.065258314160218,
      "grad_norm": 54.226600646972656,
      "learning_rate": 9.588866654538841e-06,
      "loss": 1.3715,
      "step": 8450
    },
    {
      "epoch": 7.086174440493621,
      "grad_norm": 37.56969451904297,
      "learning_rate": 9.520647625977806e-06,
      "loss": 1.2874,
      "step": 8475
    },
    {
      "epoch": 7.107090566827024,
      "grad_norm": 52.718807220458984,
      "learning_rate": 9.452428597416772e-06,
      "loss": 1.2108,
      "step": 8500
    },
    {
      "epoch": 7.128006693160427,
      "grad_norm": 45.51591873168945,
      "learning_rate": 9.38420956885574e-06,
      "loss": 1.3213,
      "step": 8525
    },
    {
      "epoch": 7.14892281949383,
      "grad_norm": 56.8362922668457,
      "learning_rate": 9.315990540294707e-06,
      "loss": 1.3196,
      "step": 8550
    },
    {
      "epoch": 7.169838945827233,
      "grad_norm": 44.81874084472656,
      "learning_rate": 9.247771511733673e-06,
      "loss": 1.3462,
      "step": 8575
    },
    {
      "epoch": 7.190755072160636,
      "grad_norm": 56.235172271728516,
      "learning_rate": 9.17955248317264e-06,
      "loss": 1.1862,
      "step": 8600
    },
    {
      "epoch": 7.211671198494039,
      "grad_norm": 46.458168029785156,
      "learning_rate": 9.111333454611608e-06,
      "loss": 1.4343,
      "step": 8625
    },
    {
      "epoch": 7.232587324827442,
      "grad_norm": 52.69073486328125,
      "learning_rate": 9.043114426050572e-06,
      "loss": 1.2938,
      "step": 8650
    },
    {
      "epoch": 7.253503451160845,
      "grad_norm": 48.153541564941406,
      "learning_rate": 8.974895397489539e-06,
      "loss": 1.2226,
      "step": 8675
    },
    {
      "epoch": 7.274419577494248,
      "grad_norm": 45.40321350097656,
      "learning_rate": 8.906676368928507e-06,
      "loss": 1.3444,
      "step": 8700
    },
    {
      "epoch": 7.295335703827651,
      "grad_norm": 41.40690231323242,
      "learning_rate": 8.838457340367473e-06,
      "loss": 1.2028,
      "step": 8725
    },
    {
      "epoch": 7.316251830161054,
      "grad_norm": 44.97480773925781,
      "learning_rate": 8.77023831180644e-06,
      "loss": 1.2297,
      "step": 8750
    },
    {
      "epoch": 7.337167956494457,
      "grad_norm": 56.64478302001953,
      "learning_rate": 8.702019283245406e-06,
      "loss": 1.3571,
      "step": 8775
    },
    {
      "epoch": 7.35808408282786,
      "grad_norm": 44.72664260864258,
      "learning_rate": 8.633800254684374e-06,
      "loss": 1.2545,
      "step": 8800
    },
    {
      "epoch": 7.379000209161263,
      "grad_norm": 53.319488525390625,
      "learning_rate": 8.56558122612334e-06,
      "loss": 1.2806,
      "step": 8825
    },
    {
      "epoch": 7.399916335494666,
      "grad_norm": 59.047794342041016,
      "learning_rate": 8.497362197562306e-06,
      "loss": 1.3103,
      "step": 8850
    },
    {
      "epoch": 7.42083246182807,
      "grad_norm": 59.36393737792969,
      "learning_rate": 8.429143169001274e-06,
      "loss": 1.4025,
      "step": 8875
    },
    {
      "epoch": 7.441748588161473,
      "grad_norm": 53.7931022644043,
      "learning_rate": 8.36092414044024e-06,
      "loss": 1.2706,
      "step": 8900
    },
    {
      "epoch": 7.462664714494876,
      "grad_norm": 49.930049896240234,
      "learning_rate": 8.292705111879208e-06,
      "loss": 1.2854,
      "step": 8925
    },
    {
      "epoch": 7.483580840828279,
      "grad_norm": 49.654136657714844,
      "learning_rate": 8.224486083318173e-06,
      "loss": 1.3214,
      "step": 8950
    },
    {
      "epoch": 7.504496967161682,
      "grad_norm": 45.24076461791992,
      "learning_rate": 8.156267054757141e-06,
      "loss": 1.3055,
      "step": 8975
    },
    {
      "epoch": 7.525413093495085,
      "grad_norm": 45.18111038208008,
      "learning_rate": 8.088048026196107e-06,
      "loss": 1.3401,
      "step": 9000
    },
    {
      "epoch": 7.546329219828488,
      "grad_norm": 71.3056640625,
      "learning_rate": 8.019828997635074e-06,
      "loss": 1.4002,
      "step": 9025
    },
    {
      "epoch": 7.567245346161891,
      "grad_norm": 38.48550033569336,
      "learning_rate": 7.95160996907404e-06,
      "loss": 1.2843,
      "step": 9050
    },
    {
      "epoch": 7.588161472495294,
      "grad_norm": 54.14589309692383,
      "learning_rate": 7.883390940513007e-06,
      "loss": 1.1982,
      "step": 9075
    },
    {
      "epoch": 7.609077598828697,
      "grad_norm": 41.54970169067383,
      "learning_rate": 7.815171911951975e-06,
      "loss": 1.2221,
      "step": 9100
    },
    {
      "epoch": 7.6299937251621,
      "grad_norm": 51.51393127441406,
      "learning_rate": 7.74695288339094e-06,
      "loss": 1.1549,
      "step": 9125
    },
    {
      "epoch": 7.650909851495503,
      "grad_norm": 47.756961822509766,
      "learning_rate": 7.678733854829908e-06,
      "loss": 1.267,
      "step": 9150
    },
    {
      "epoch": 7.671825977828906,
      "grad_norm": 50.51896667480469,
      "learning_rate": 7.610514826268874e-06,
      "loss": 1.2565,
      "step": 9175
    },
    {
      "epoch": 7.692742104162309,
      "grad_norm": 45.28558349609375,
      "learning_rate": 7.542295797707841e-06,
      "loss": 1.2828,
      "step": 9200
    },
    {
      "epoch": 7.713658230495712,
      "grad_norm": 52.89387512207031,
      "learning_rate": 7.474076769146808e-06,
      "loss": 1.3383,
      "step": 9225
    },
    {
      "epoch": 7.734574356829115,
      "grad_norm": 58.25821304321289,
      "learning_rate": 7.405857740585774e-06,
      "loss": 1.2643,
      "step": 9250
    },
    {
      "epoch": 7.755490483162518,
      "grad_norm": 44.26544189453125,
      "learning_rate": 7.3376387120247415e-06,
      "loss": 1.231,
      "step": 9275
    },
    {
      "epoch": 7.776406609495921,
      "grad_norm": 38.64336013793945,
      "learning_rate": 7.269419683463708e-06,
      "loss": 1.2377,
      "step": 9300
    },
    {
      "epoch": 7.797322735829324,
      "grad_norm": 42.080711364746094,
      "learning_rate": 7.201200654902674e-06,
      "loss": 1.279,
      "step": 9325
    },
    {
      "epoch": 7.818238862162728,
      "grad_norm": 42.13421630859375,
      "learning_rate": 7.132981626341641e-06,
      "loss": 1.2965,
      "step": 9350
    },
    {
      "epoch": 7.839154988496131,
      "grad_norm": 50.24284744262695,
      "learning_rate": 7.064762597780608e-06,
      "loss": 1.3587,
      "step": 9375
    },
    {
      "epoch": 7.860071114829534,
      "grad_norm": 48.894893646240234,
      "learning_rate": 6.9965435692195745e-06,
      "loss": 1.3332,
      "step": 9400
    },
    {
      "epoch": 7.880987241162937,
      "grad_norm": 50.721256256103516,
      "learning_rate": 6.928324540658542e-06,
      "loss": 1.2737,
      "step": 9425
    },
    {
      "epoch": 7.90190336749634,
      "grad_norm": 41.86726379394531,
      "learning_rate": 6.860105512097508e-06,
      "loss": 1.1891,
      "step": 9450
    },
    {
      "epoch": 7.922819493829743,
      "grad_norm": 45.6505012512207,
      "learning_rate": 6.791886483536475e-06,
      "loss": 1.206,
      "step": 9475
    },
    {
      "epoch": 7.943735620163146,
      "grad_norm": 50.496665954589844,
      "learning_rate": 6.723667454975441e-06,
      "loss": 1.2518,
      "step": 9500
    },
    {
      "epoch": 7.964651746496549,
      "grad_norm": 62.3567008972168,
      "learning_rate": 6.6554484264144074e-06,
      "loss": 1.3168,
      "step": 9525
    },
    {
      "epoch": 7.985567872829952,
      "grad_norm": 46.650264739990234,
      "learning_rate": 6.587229397853375e-06,
      "loss": 1.1845,
      "step": 9550
    },
    {
      "epoch": 8.005856515373353,
      "grad_norm": 57.82951736450195,
      "learning_rate": 6.519010369292341e-06,
      "loss": 1.2343,
      "step": 9575
    },
    {
      "epoch": 8.026772641706756,
      "grad_norm": 40.18219757080078,
      "learning_rate": 6.450791340731308e-06,
      "loss": 1.1953,
      "step": 9600
    },
    {
      "epoch": 8.047688768040159,
      "grad_norm": 49.931915283203125,
      "learning_rate": 6.382572312170275e-06,
      "loss": 1.2073,
      "step": 9625
    },
    {
      "epoch": 8.068604894373562,
      "grad_norm": 44.14402770996094,
      "learning_rate": 6.314353283609242e-06,
      "loss": 1.1798,
      "step": 9650
    },
    {
      "epoch": 8.089521020706965,
      "grad_norm": 37.8303108215332,
      "learning_rate": 6.2461342550482085e-06,
      "loss": 1.1719,
      "step": 9675
    },
    {
      "epoch": 8.110437147040368,
      "grad_norm": 45.6505126953125,
      "learning_rate": 6.177915226487175e-06,
      "loss": 1.1753,
      "step": 9700
    },
    {
      "epoch": 8.13135327337377,
      "grad_norm": 50.847808837890625,
      "learning_rate": 6.109696197926141e-06,
      "loss": 1.2755,
      "step": 9725
    },
    {
      "epoch": 8.152269399707174,
      "grad_norm": 38.53950500488281,
      "learning_rate": 6.041477169365109e-06,
      "loss": 1.1583,
      "step": 9750
    },
    {
      "epoch": 8.173185526040577,
      "grad_norm": 43.97337341308594,
      "learning_rate": 5.973258140804075e-06,
      "loss": 1.1742,
      "step": 9775
    },
    {
      "epoch": 8.19410165237398,
      "grad_norm": 50.901634216308594,
      "learning_rate": 5.905039112243042e-06,
      "loss": 1.1908,
      "step": 9800
    },
    {
      "epoch": 8.215017778707383,
      "grad_norm": 41.54732131958008,
      "learning_rate": 5.836820083682009e-06,
      "loss": 1.1943,
      "step": 9825
    },
    {
      "epoch": 8.235933905040786,
      "grad_norm": 51.554832458496094,
      "learning_rate": 5.768601055120975e-06,
      "loss": 1.1577,
      "step": 9850
    },
    {
      "epoch": 8.256850031374189,
      "grad_norm": 46.97303771972656,
      "learning_rate": 5.700382026559942e-06,
      "loss": 1.2108,
      "step": 9875
    },
    {
      "epoch": 8.277766157707592,
      "grad_norm": 37.67015075683594,
      "learning_rate": 5.632162997998908e-06,
      "loss": 1.2176,
      "step": 9900
    },
    {
      "epoch": 8.298682284040996,
      "grad_norm": 45.29341506958008,
      "learning_rate": 5.563943969437875e-06,
      "loss": 1.19,
      "step": 9925
    },
    {
      "epoch": 8.319598410374399,
      "grad_norm": 38.14066696166992,
      "learning_rate": 5.495724940876842e-06,
      "loss": 1.1343,
      "step": 9950
    },
    {
      "epoch": 8.340514536707802,
      "grad_norm": 51.224098205566406,
      "learning_rate": 5.427505912315809e-06,
      "loss": 1.2175,
      "step": 9975
    },
    {
      "epoch": 8.361430663041205,
      "grad_norm": 45.31913757324219,
      "learning_rate": 5.3592868837547754e-06,
      "loss": 1.0908,
      "step": 10000
    },
    {
      "epoch": 8.382346789374608,
      "grad_norm": 54.03606033325195,
      "learning_rate": 5.291067855193743e-06,
      "loss": 1.202,
      "step": 10025
    },
    {
      "epoch": 8.40326291570801,
      "grad_norm": 55.19104766845703,
      "learning_rate": 5.222848826632709e-06,
      "loss": 1.3879,
      "step": 10050
    },
    {
      "epoch": 8.424179042041414,
      "grad_norm": 48.442344665527344,
      "learning_rate": 5.1546297980716756e-06,
      "loss": 1.147,
      "step": 10075
    },
    {
      "epoch": 8.445095168374817,
      "grad_norm": 45.054622650146484,
      "learning_rate": 5.086410769510642e-06,
      "loss": 1.1367,
      "step": 10100
    },
    {
      "epoch": 8.46601129470822,
      "grad_norm": 50.391357421875,
      "learning_rate": 5.018191740949609e-06,
      "loss": 1.1745,
      "step": 10125
    },
    {
      "epoch": 8.486927421041623,
      "grad_norm": 43.87202072143555,
      "learning_rate": 4.949972712388576e-06,
      "loss": 1.1737,
      "step": 10150
    },
    {
      "epoch": 8.507843547375026,
      "grad_norm": 52.810977935791016,
      "learning_rate": 4.881753683827543e-06,
      "loss": 1.1559,
      "step": 10175
    },
    {
      "epoch": 8.528759673708429,
      "grad_norm": 48.89564895629883,
      "learning_rate": 4.813534655266509e-06,
      "loss": 1.1333,
      "step": 10200
    },
    {
      "epoch": 8.549675800041832,
      "grad_norm": 50.87118911743164,
      "learning_rate": 4.745315626705476e-06,
      "loss": 1.1929,
      "step": 10225
    },
    {
      "epoch": 8.570591926375235,
      "grad_norm": 45.6518669128418,
      "learning_rate": 4.677096598144442e-06,
      "loss": 1.1663,
      "step": 10250
    },
    {
      "epoch": 8.591508052708638,
      "grad_norm": 51.34090042114258,
      "learning_rate": 4.608877569583409e-06,
      "loss": 1.1664,
      "step": 10275
    },
    {
      "epoch": 8.61242417904204,
      "grad_norm": 40.684852600097656,
      "learning_rate": 4.540658541022376e-06,
      "loss": 1.1578,
      "step": 10300
    },
    {
      "epoch": 8.633340305375445,
      "grad_norm": 48.28627014160156,
      "learning_rate": 4.472439512461342e-06,
      "loss": 1.2935,
      "step": 10325
    },
    {
      "epoch": 8.654256431708848,
      "grad_norm": 33.38114547729492,
      "learning_rate": 4.40422048390031e-06,
      "loss": 1.0937,
      "step": 10350
    },
    {
      "epoch": 8.67517255804225,
      "grad_norm": 49.42060470581055,
      "learning_rate": 4.336001455339276e-06,
      "loss": 1.1697,
      "step": 10375
    },
    {
      "epoch": 8.696088684375654,
      "grad_norm": 55.446434020996094,
      "learning_rate": 4.267782426778243e-06,
      "loss": 1.2246,
      "step": 10400
    },
    {
      "epoch": 8.717004810709057,
      "grad_norm": 43.731788635253906,
      "learning_rate": 4.199563398217209e-06,
      "loss": 1.0991,
      "step": 10425
    },
    {
      "epoch": 8.73792093704246,
      "grad_norm": 44.624996185302734,
      "learning_rate": 4.131344369656176e-06,
      "loss": 1.1856,
      "step": 10450
    },
    {
      "epoch": 8.758837063375863,
      "grad_norm": 36.826297760009766,
      "learning_rate": 4.063125341095143e-06,
      "loss": 1.0603,
      "step": 10475
    },
    {
      "epoch": 8.779753189709266,
      "grad_norm": 52.4532470703125,
      "learning_rate": 3.99490631253411e-06,
      "loss": 1.1805,
      "step": 10500
    },
    {
      "epoch": 8.800669316042669,
      "grad_norm": 56.85908508300781,
      "learning_rate": 3.926687283973076e-06,
      "loss": 1.228,
      "step": 10525
    },
    {
      "epoch": 8.821585442376072,
      "grad_norm": 41.96944046020508,
      "learning_rate": 3.8584682554120435e-06,
      "loss": 1.1443,
      "step": 10550
    },
    {
      "epoch": 8.842501568709475,
      "grad_norm": 50.0311393737793,
      "learning_rate": 3.79024922685101e-06,
      "loss": 1.2603,
      "step": 10575
    },
    {
      "epoch": 8.863417695042878,
      "grad_norm": 45.01953125,
      "learning_rate": 3.7220301982899764e-06,
      "loss": 1.2607,
      "step": 10600
    },
    {
      "epoch": 8.88433382137628,
      "grad_norm": 51.258094787597656,
      "learning_rate": 3.6538111697289432e-06,
      "loss": 1.1796,
      "step": 10625
    },
    {
      "epoch": 8.905249947709684,
      "grad_norm": 42.26016616821289,
      "learning_rate": 3.58559214116791e-06,
      "loss": 1.0707,
      "step": 10650
    },
    {
      "epoch": 8.926166074043087,
      "grad_norm": 34.383399963378906,
      "learning_rate": 3.5173731126068765e-06,
      "loss": 1.022,
      "step": 10675
    },
    {
      "epoch": 8.94708220037649,
      "grad_norm": 50.439735412597656,
      "learning_rate": 3.4491540840458434e-06,
      "loss": 1.219,
      "step": 10700
    },
    {
      "epoch": 8.967998326709893,
      "grad_norm": 43.12095642089844,
      "learning_rate": 3.3809350554848102e-06,
      "loss": 1.1911,
      "step": 10725
    },
    {
      "epoch": 8.988914453043297,
      "grad_norm": 37.343875885009766,
      "learning_rate": 3.312716026923777e-06,
      "loss": 1.1141,
      "step": 10750
    },
    {
      "epoch": 9.009203095586697,
      "grad_norm": 36.619937896728516,
      "learning_rate": 3.2444969983627435e-06,
      "loss": 1.1967,
      "step": 10775
    },
    {
      "epoch": 9.030119221920101,
      "grad_norm": 47.067901611328125,
      "learning_rate": 3.17627796980171e-06,
      "loss": 1.0752,
      "step": 10800
    },
    {
      "epoch": 9.051035348253503,
      "grad_norm": 42.284385681152344,
      "learning_rate": 3.1080589412406768e-06,
      "loss": 1.1054,
      "step": 10825
    },
    {
      "epoch": 9.071951474586907,
      "grad_norm": 44.91628646850586,
      "learning_rate": 3.039839912679643e-06,
      "loss": 1.1113,
      "step": 10850
    },
    {
      "epoch": 9.092867600920309,
      "grad_norm": 33.48893737792969,
      "learning_rate": 2.97162088411861e-06,
      "loss": 1.0617,
      "step": 10875
    },
    {
      "epoch": 9.113783727253713,
      "grad_norm": 43.41284942626953,
      "learning_rate": 2.903401855557577e-06,
      "loss": 1.0703,
      "step": 10900
    },
    {
      "epoch": 9.134699853587115,
      "grad_norm": 34.29306411743164,
      "learning_rate": 2.8351828269965437e-06,
      "loss": 1.2254,
      "step": 10925
    },
    {
      "epoch": 9.155615979920519,
      "grad_norm": 42.38772964477539,
      "learning_rate": 2.76696379843551e-06,
      "loss": 1.0755,
      "step": 10950
    },
    {
      "epoch": 9.176532106253921,
      "grad_norm": 44.363895416259766,
      "learning_rate": 2.698744769874477e-06,
      "loss": 1.1226,
      "step": 10975
    },
    {
      "epoch": 9.197448232587325,
      "grad_norm": 36.576683044433594,
      "learning_rate": 2.630525741313444e-06,
      "loss": 1.0661,
      "step": 11000
    },
    {
      "epoch": 9.218364358920727,
      "grad_norm": 42.33050537109375,
      "learning_rate": 2.5623067127524107e-06,
      "loss": 1.1592,
      "step": 11025
    },
    {
      "epoch": 9.239280485254131,
      "grad_norm": 41.08555221557617,
      "learning_rate": 2.494087684191377e-06,
      "loss": 1.2054,
      "step": 11050
    },
    {
      "epoch": 9.260196611587533,
      "grad_norm": 43.62483215332031,
      "learning_rate": 2.425868655630344e-06,
      "loss": 1.0501,
      "step": 11075
    },
    {
      "epoch": 9.281112737920937,
      "grad_norm": 40.097259521484375,
      "learning_rate": 2.357649627069311e-06,
      "loss": 1.0369,
      "step": 11100
    },
    {
      "epoch": 9.302028864254341,
      "grad_norm": 46.62436294555664,
      "learning_rate": 2.2894305985082772e-06,
      "loss": 1.1677,
      "step": 11125
    },
    {
      "epoch": 9.322944990587743,
      "grad_norm": 43.803993225097656,
      "learning_rate": 2.221211569947244e-06,
      "loss": 1.068,
      "step": 11150
    },
    {
      "epoch": 9.343861116921147,
      "grad_norm": 37.38458251953125,
      "learning_rate": 2.152992541386211e-06,
      "loss": 0.9578,
      "step": 11175
    },
    {
      "epoch": 9.364777243254549,
      "grad_norm": 52.64495849609375,
      "learning_rate": 2.0847735128251774e-06,
      "loss": 1.1794,
      "step": 11200
    },
    {
      "epoch": 9.385693369587953,
      "grad_norm": 41.22502136230469,
      "learning_rate": 2.0165544842641438e-06,
      "loss": 1.0591,
      "step": 11225
    },
    {
      "epoch": 9.406609495921355,
      "grad_norm": 49.40909194946289,
      "learning_rate": 1.9483354557031106e-06,
      "loss": 1.1041,
      "step": 11250
    },
    {
      "epoch": 9.427525622254759,
      "grad_norm": 46.606689453125,
      "learning_rate": 1.8801164271420777e-06,
      "loss": 1.1211,
      "step": 11275
    },
    {
      "epoch": 9.448441748588161,
      "grad_norm": 43.9527702331543,
      "learning_rate": 1.8118973985810441e-06,
      "loss": 1.0642,
      "step": 11300
    },
    {
      "epoch": 9.469357874921565,
      "grad_norm": 46.618831634521484,
      "learning_rate": 1.743678370020011e-06,
      "loss": 1.1179,
      "step": 11325
    },
    {
      "epoch": 9.490274001254967,
      "grad_norm": 48.50856018066406,
      "learning_rate": 1.6754593414589776e-06,
      "loss": 1.1357,
      "step": 11350
    },
    {
      "epoch": 9.511190127588371,
      "grad_norm": 53.206024169921875,
      "learning_rate": 1.6072403128979444e-06,
      "loss": 1.0903,
      "step": 11375
    },
    {
      "epoch": 9.532106253921773,
      "grad_norm": 43.582420349121094,
      "learning_rate": 1.539021284336911e-06,
      "loss": 1.1634,
      "step": 11400
    },
    {
      "epoch": 9.553022380255177,
      "grad_norm": 42.108211517333984,
      "learning_rate": 1.470802255775878e-06,
      "loss": 1.0832,
      "step": 11425
    },
    {
      "epoch": 9.57393850658858,
      "grad_norm": 42.2984619140625,
      "learning_rate": 1.4025832272148446e-06,
      "loss": 1.1346,
      "step": 11450
    },
    {
      "epoch": 9.594854632921983,
      "grad_norm": 52.80603790283203,
      "learning_rate": 1.3343641986538112e-06,
      "loss": 1.1134,
      "step": 11475
    },
    {
      "epoch": 9.615770759255385,
      "grad_norm": 45.932159423828125,
      "learning_rate": 1.2661451700927778e-06,
      "loss": 1.0217,
      "step": 11500
    },
    {
      "epoch": 9.636686885588789,
      "grad_norm": 35.81978988647461,
      "learning_rate": 1.1979261415317445e-06,
      "loss": 1.0595,
      "step": 11525
    },
    {
      "epoch": 9.657603011922191,
      "grad_norm": 42.35458755493164,
      "learning_rate": 1.1297071129707113e-06,
      "loss": 1.0886,
      "step": 11550
    },
    {
      "epoch": 9.678519138255595,
      "grad_norm": 50.413719177246094,
      "learning_rate": 1.061488084409678e-06,
      "loss": 1.0514,
      "step": 11575
    },
    {
      "epoch": 9.699435264588999,
      "grad_norm": 48.791221618652344,
      "learning_rate": 9.932690558486448e-07,
      "loss": 1.1276,
      "step": 11600
    },
    {
      "epoch": 9.720351390922401,
      "grad_norm": 48.47390365600586,
      "learning_rate": 9.250500272876114e-07,
      "loss": 1.1272,
      "step": 11625
    },
    {
      "epoch": 9.741267517255805,
      "grad_norm": 53.781864166259766,
      "learning_rate": 8.568309987265781e-07,
      "loss": 1.1155,
      "step": 11650
    },
    {
      "epoch": 9.762183643589207,
      "grad_norm": 49.06599426269531,
      "learning_rate": 7.886119701655448e-07,
      "loss": 1.0859,
      "step": 11675
    },
    {
      "epoch": 9.783099769922611,
      "grad_norm": 41.909645080566406,
      "learning_rate": 7.203929416045116e-07,
      "loss": 1.1246,
      "step": 11700
    },
    {
      "epoch": 9.804015896256013,
      "grad_norm": 40.743709564208984,
      "learning_rate": 6.521739130434783e-07,
      "loss": 1.133,
      "step": 11725
    },
    {
      "epoch": 9.824932022589417,
      "grad_norm": 53.179805755615234,
      "learning_rate": 5.839548844824449e-07,
      "loss": 1.0952,
      "step": 11750
    },
    {
      "epoch": 9.84584814892282,
      "grad_norm": 43.82381820678711,
      "learning_rate": 5.157358559214117e-07,
      "loss": 1.1181,
      "step": 11775
    },
    {
      "epoch": 9.866764275256223,
      "grad_norm": 46.067745208740234,
      "learning_rate": 4.4751682736037843e-07,
      "loss": 1.0787,
      "step": 11800
    },
    {
      "epoch": 9.887680401589625,
      "grad_norm": 42.819480895996094,
      "learning_rate": 3.792977987993451e-07,
      "loss": 1.0798,
      "step": 11825
    },
    {
      "epoch": 9.908596527923029,
      "grad_norm": 48.82379913330078,
      "learning_rate": 3.110787702383118e-07,
      "loss": 1.0218,
      "step": 11850
    },
    {
      "epoch": 9.929512654256431,
      "grad_norm": 46.20103073120117,
      "learning_rate": 2.428597416772785e-07,
      "loss": 1.1141,
      "step": 11875
    },
    {
      "epoch": 9.950428780589835,
      "grad_norm": 46.36656188964844,
      "learning_rate": 1.7464071311624524e-07,
      "loss": 1.1062,
      "step": 11900
    },
    {
      "epoch": 9.971344906923237,
      "grad_norm": 34.23735427856445,
      "learning_rate": 1.0642168455521194e-07,
      "loss": 1.0237,
      "step": 11925
    },
    {
      "epoch": 9.992261033256641,
      "grad_norm": 36.44773864746094,
      "learning_rate": 3.8202655994178645e-08,
      "loss": 1.1981,
      "step": 11950
    }
  ],
  "logging_steps": 25,
  "max_steps": 11950,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 0,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
